{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "055ee4cb",
   "metadata": {},
   "source": [
    "---\n",
    "<h1 align=\"center\"> PatchTST Architecture</h1>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cf48fb",
   "metadata": {},
   "source": [
    "PatchTST is a new Transformer architecture that incorporates two key concepts for time series forecasting: channel independence and patching.\n",
    "Channel independence involves decomposing multichannel sequences into single channels before input to the model, allowing for handling different types of data.\n",
    "Patching divides the input sequence into smaller parts, or patches \"window\", allowing the model to focus on similar group of data.\n",
    "PatchTST is evaluated the effectiveness of the model on multichannel time series forecasting tasks where both the input and output data are multichannel.* PatchTST is a new Transformer architecture that incorporates two key concepts for time series forecasting: channel independence and patching.\n",
    "* Channel independence involves decomposing multichannel sequences into single channels before input to the model, allowing for handling different types of data. \n",
    "* Patching divides the input sequence into smaller parts, or patches \"window\", allowing the model to focus on similar group of data.\n",
    "* PatchTST is evaluated the effectiveness of the model on multichannel time series forecasting tasks where both the input and output data are multichannel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74979f57",
   "metadata": {},
   "source": [
    "<img width=\"1000\" src=\"Images/PatchTST.png\" alt=\"PatchTST Archticture\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e37e199",
   "metadata": {},
   "source": [
    "---\n",
    "## Inputs and Outputs\n",
    "---\n",
    "##### **Input sequence:**\n",
    "* The input to the PatchTST algorithm is a time series data that can have multiple channels (features). The multichannel input is decomposed into single channels using the channel independence concept, then PatchTST model applies patching to divide the input sequence into smaller parts or patches, allowing the model to focus on local patterns while avoiding memory constraints and facilitating quicker inference.\n",
    "\n",
    "* The length of the input sequence is defined by the `seq_len` hyperparameter.\n",
    "\n",
    "\n",
    "##### **Output sequence:**\n",
    "* The output of the PatchTST model is a sequence of multichannel time series of predicted values, where each value corresponds to a prediction made at a specific time step in the output sequence.\n",
    "\n",
    "* The length of the output sequence is defined by the `pred_len` hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904765d8",
   "metadata": {},
   "source": [
    "---\n",
    "## Methodology\n",
    "---\n",
    "\n",
    "- This notebook provide a step-by-step guide for replicating the **PatchTST** model and training it on the ETDataset (ETTh1, ETTh2, ETTm1, and ETTm2), ensuring accurate reproduction of the models by comparing the notebook results with the official paper results. The main foucs here is to study the *impact of a number of prediction length and the number of input patches on the performance*. The workflow from the beginning to forcasting is as follows:\n",
    "\n",
    "\n",
    "### 1. Data Preparation: \n",
    "* The author preprocessed the ETT dataset by normalizing the input features and splitting the data into training, validation, and test sets.\n",
    "\n",
    "### 2. Replicat Model Architecture: \n",
    "* Using the PatchTST architecture with channel independence and patching. Then reconstructing the PatchTST model to output a single-channel sequence instead of a multichannel sequence.\n",
    "\n",
    "### 3. Training: \n",
    "* Training the PatchTST model on the training set and validated it on the validation set. We used the Mean Absolute Percentage Error (MAPE) as the evaluation metric.\n",
    "\n",
    "### 4. Hyperparameter Tuning: \n",
    "* Tune the hyperparameters of the PatchTST model using a grid search approach. We experimented with different values for the number of layers, the number of heads, the patch length, and the stride.\n",
    "\n",
    "### 5. Testing: \n",
    "* Evaluating the performance of the PatchTST model on the test set and compared it to the performance of simple DNN models.\n",
    "\n",
    "### 6. Results: \n",
    "* We will apply the PatchTST model on ETT datasets with different number of patches and prediction lenght according to the trials In the original PatchTST paper and compare the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262cac24",
   "metadata": {},
   "source": [
    "---\n",
    "## Hyperparameters\n",
    "---\n",
    "\n",
    "**Here are the hyperparameters that control the input and output of PatchTST:**\n",
    "\n",
    "| Parameter | Description | Value |\n",
    "|---|---|---|\n",
    "| `args.data` | The name of the dataset to use. | 'ETTh1', 'ETTh2', 'ETTm1', and 'ETTm2'|\n",
    "| `args.root_path` | The root path of the data file. | `./Datasets/` |\n",
    "| `args.features` | The type of forecasting task to perform. The options are 'M' (multivariate predict multivariate), 'S' (univariate predict univariate), and 'MS' (multivariate predict univariate). | 'M'|\n",
    "| `args.target` | The target feature to predict in a univariate or multivariate task. ('HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT')| 'OT' |\n",
    "| `args.seq_len` | The length of the input sequence to the Informer encoder. | 336|\n",
    "| `args.pred_len` | The length of the future sequence to be predicted. | [96, 192, 336, 720]|\n",
    "| `args.patch_len` | The size of each patch | 24 |\n",
    "| `args.num_patch` | The number of input patches| [42, 64]  |\n",
    "| `args.stride` | The step size used to slide the patch window across the input time series data during the patching process | 8|\n",
    "| `args.batch_size` |  The size of one batch in training | 128 |\n",
    "| `args.learning_rate` | Learning rate  | 0.0001 |\n",
    "| `args.patience` | The number of epochs to wait before early stopping | 10 |\n",
    "| `args.train_epochs` | Number of epochs in train | 20 |\n",
    "| `args.padding` | The amount of padding to add to the input sequence, if any. | 0|\n",
    "| `args.freq` |  freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h | 'h' |\n",
    "| `args.embed` | The type of time feature encoding to use. The options are 'timeF' (time features encoding), 'fixed' (fixed positional encoding), and 'learned' (learned positional encoding). | 'timeF' |\n",
    "\n",
    "---------\n",
    "- **ProbSparse Attention:** \n",
    "\n",
    "| Parameter | Description | Value |\n",
    "|---|---|---|\n",
    "| `args.attn` | The type of attention used in the encoder. The options are 'prob' (probabilistic sparse attention) and 'full' (full attention). | 'prob' |\n",
    "| `args.n_heads` | The number of attention heads in the encoder. | 16 |\n",
    "| `args.factor` | The ProbSparse attention factor. A higher value of factor results in a sparser attention matrix. | 5 |\n",
    "| `args.dropout` | The dropout probability applied to the attention weights. | 0.1 |\n",
    "| `args.d_model` | The dimension of the model. | 512 |\n",
    "\n",
    "---\n",
    "- **Feedforward Network:** \n",
    "\n",
    "| Parameter | Description | Value |\n",
    "|---|---|---|\n",
    "| `args.d_ff` | The dimension of the feedforward network. | 2048 |\n",
    "| `args.activation` | The activation function used in the feedforward network. | 'gelu' |\n",
    "| `args.dropout` | The dropout probability applied to the attention weights. | 0.1 |\n",
    "\n",
    "---\n",
    "- **Mixing Layer:**\n",
    "\n",
    "The model includes a mixing layer that linearly combines the outputs of the attention heads in the encoder and decoder, which helps to improve the model's performance. Here are the hyperparameters that control the mixing layer:\n",
    "\n",
    "| Parameter | Description | Value |\n",
    "|---|---|---|\n",
    "| `args.mix` | Whether to apply a linear projection to the concatenated outputs of the attention heads. | True |\n",
    "| `args.d_model` | The dimension of the model. | 512 |\n",
    "\n",
    "---\n",
    "**The following are the experiment Hyperparameters.**\n",
    "\n",
    "\n",
    "| Parameter | Description | Value |\n",
    "|---|---|---|\n",
    "| `args.output_attention` | Whether to output attention in ecoder | False |\n",
    "| `args.use_amp` | Whether to use automatic mixed precision training | False |\n",
    "| `args.train_only` | Whether to train the model or fine-tune | True |\n",
    "| `args.train_epochs` | The number of epochs to train for. | 8 |\n",
    "| `args.batch_size` | The batch size of training input data. | 32 |\n",
    "| `args.learning_rate` | Learning rate starts from 1e−4, decaying two times smaller every epoch. | 0.0001 |\n",
    "| `args.lradj` | Learning rate decayed two times smaller every epoch. | 'type1' |\n",
    "| `args.loss` | Evaluating criteria | `'mse'` |\n",
    "| `args.patience` | The number of epochs to wait before early stopping. | 3 |\n",
    "| `args.des` | The description of the experiment. | 'test' |\n",
    "| `args.itr` | The iteration of the experiment. | 1 |\n",
    "| `args.model` | The model name | 'informer' |\n",
    "| `args.checkpoints` | Location of model checkpoints | `'./Checkpoints/Informer_checkpoints'` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58663df",
   "metadata": {},
   "source": [
    "---\n",
    "# Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486c6973",
   "metadata": {},
   "source": [
    "**Add project_files to system path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5c8430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('./Transformer-based-solutions-for-the-long-term-time-series-forecasting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789d7379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if not 'Transformer-based-solutions-for-the-long-term-time-series-forecasting' in sys.path:\n",
    "    sys.path += ['Transformer-based-solutions-for-the-long-term-time-series-forecasting']\n",
    "    \n",
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e043bdae",
   "metadata": {},
   "source": [
    "**Important library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c49e788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np \n",
    "import os\n",
    "from exp.exp_PatchTST import Exp_Main #, Dataset_Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ce8d8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5afddf",
   "metadata": {},
   "source": [
    "---\n",
    "# Working on ETT Dataset\n",
    "\n",
    "\n",
    "* The Electricity Transformer Temperature (ETT) is a crucial indicator in the electric power long-term deployment. This dataset consists of 2 years data from two separated counties in .There are different subsets, {ETTh1, ETTh2} for 1-hour-level and ETTm1 for 15-minutes-level. Each data point consists of the target value ”oil temperature” and 6 power load features. The train/val/test is 12/4/4 months."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74a0b7c",
   "metadata": {},
   "source": [
    "---\n",
    "# Working on ETTh1 Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d97e70",
   "metadata": {},
   "source": [
    "## Trail 1: PatchTST/42, Dataset:ETTh1,  Metric: 96\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5ef66b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    **dotdict function**\n",
    "    This function is used to convert a dictionary into\n",
    "    an object whose keys can be accessed as attributes\n",
    "\"\"\"\n",
    "\n",
    "args = dotdict()\n",
    "\n",
    "args.model = 'PatchTST'   # Model Name\n",
    "args.random_seed = 2021\n",
    "args.is_training = 1\n",
    "args.model_id = f\"{args.data}_{args.seq_len}_{args.pred_len}\"\n",
    "args.fc_dropout = 0.3\n",
    "args.head_dropout = 0\n",
    "args.patch_len = 24 # The size of each patch\n",
    "args.num_patch = 42  # The number of input patches (42 64)  \n",
    "args.stride = 8 # The step size used to slide the patch window across the input time series data during the patching process.\n",
    "args.batch_size = 32 # The size of one batch in training\n",
    "args.learning_rate = 0.0001 # Learning rate\n",
    "args.pred_len = 96 # prediction sequence length   (96, 192, 336, 720)  \n",
    "args.patience = 10  # The number of epochs to wait before early stopping.\n",
    "args.train_epochs = 20   # Number of epochs in train\n",
    "\n",
    "args.use_multi_gpu = False \n",
    "args.use_gpu = True if torch.cuda.is_available() else False # Using GPU if cuda is available \n",
    "args.learning_rate = 0.005 \n",
    "args.label_len = 48 # start token length of PatchTST decoder\n",
    "args.use_amp = False # whether to use automatic mixed precision training\n",
    "args.output_attention = False # whether to output attention in ecoder\n",
    "args.features = 'M' # forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate\n",
    "args.train_only=True\n",
    "args.checkpoints = './PatchTST_checkpoints' # location of model checkpoints\n",
    "\n",
    "\n",
    "args.data = 'ETTh1'  # data\n",
    "args.root_path = './ETDataset/' # root path of data file\n",
    "args.data_path = 'ETTh1.csv' # data file\n",
    "args.target = 'OT' # target feature in S or MS task\n",
    "args.freq = 'h' # freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\n",
    "args.seq_len = 336 # input sequence length of PatchTST encoder\n",
    "\n",
    "# PatchTST decoder input: concat[start token series(label_len), zero padding series(pred_len)]\n",
    "args.enc_in = 7 # encoder input size\n",
    "args.dec_in = 7 # decoder input size\n",
    "args.c_out = 7 # output size\n",
    "args.factor = 5 # probsparse attn factor\n",
    "args.d_model = 16 # dimension of model\n",
    "args.n_heads = 4 # num of heads\n",
    "args.e_layers = 3 # num of encoder layers\n",
    "args.d_layers = 1 # num of decoder layers\n",
    "args.d_ff = 128   # dimension of fcn in model\n",
    "args.dropout =0.3# 0.05 # dropout\n",
    "args.attn = 'prob' # attention used in encoder, options:[prob, full]\n",
    "args.embed = 'timeF' # time features encoding, options:[timeF, fixed, learned]\n",
    "args.activation = 'gelu' # activation\n",
    "args.distil = True # whether to use distilling in encoder\n",
    "args.mix = True\n",
    "args.padding = 0\n",
    "#args.freq = 'h'   # # freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\n",
    "args.loss = 'mse'   # evaluating criteria\n",
    "args.lradj = 'type1'  # learning rate decayed two times smaller every epoch.\n",
    "args.num_workers = 0\n",
    "args.itr = 1\n",
    "args.des = \"Exp\"   # The description of the experiment.\n",
    "args.gpu = 0\n",
    "args.devices = '0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cfb595d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter Combination of Trail 1: \n",
      "{'model': 'PatchTST', 'random_seed': 2021, 'is_training': 1, 'model_id': 'None_None_None', 'fc_dropout': 0.3, 'head_dropout': 0, 'patch_len': 24, 'num_patch': 42, 'stride': 8, 'batch_size': 32, 'learning_rate': 0.005, 'pred_len': 96, 'patience': 10, 'train_epochs': 20, 'use_multi_gpu': False, 'use_gpu': True, 'label_len': 48, 'use_amp': False, 'output_attention': False, 'features': 'M', 'train_only': True, 'checkpoints': './PatchTST_checkpoints', 'data': 'ETTh1', 'root_path': './ETDataset/', 'data_path': 'ETTh1.csv', 'target': 'OT', 'freq': 'h', 'seq_len': 336, 'enc_in': 7, 'dec_in': 7, 'c_out': 7, 'factor': 5, 'd_model': 16, 'n_heads': 4, 'e_layers': 3, 'd_layers': 1, 'd_ff': 128, 'dropout': 0.3, 'attn': 'prob', 'embed': 'timeF', 'activation': 'gelu', 'distil': True, 'mix': True, 'padding': 0, 'loss': 'mse', 'lradj': 'type1', 'num_workers': 0, 'itr': 1, 'des': 'Exp', 'gpu': 0, 'devices': '0,1,2,3'}\n"
     ]
    }
   ],
   "source": [
    "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "if args.use_gpu and args.use_multi_gpu:\n",
    "    args.devices = args.devices.replace(' ','')\n",
    "    device_ids = args.devices.split(',')\n",
    "    args.device_ids = [int(id_) for id_ in device_ids]\n",
    "    args.gpu = args.device_ids[0]\n",
    "    \n",
    "print(\"Hyperparameter Combination of Trail 1: \") \n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256217e2",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e1cecbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "train 8209\n",
      "val 2785\n",
      "test 2785\n",
      "\titers: 100, epoch: 1 | loss: 0.5021333\n",
      "\tspeed: 0.1957s/iter; left time: 982.6902s\n",
      "\titers: 200, epoch: 1 | loss: 0.4459157\n",
      "\tspeed: 0.1831s/iter; left time: 900.9876s\n",
      "Epoch: 1 cost time: 49.07178854942322\n",
      "Epoch: 1, Steps: 256 | Train Loss: 0.5326953 Vali Loss: 0.7148266 Test Loss: 0.4131814\n",
      "Validation loss decreased (inf --> 0.714827).  Saving model ...\n",
      "Updating learning rate to 0.005\n",
      "\titers: 100, epoch: 2 | loss: 0.3932944\n",
      "\tspeed: 0.7549s/iter; left time: 3597.2838s\n",
      "\titers: 200, epoch: 2 | loss: 0.3659179\n",
      "\tspeed: 0.2130s/iter; left time: 993.4122s\n",
      "Epoch: 2 cost time: 51.905097007751465\n",
      "Epoch: 2, Steps: 256 | Train Loss: 0.4019561 Vali Loss: 0.7034194 Test Loss: 0.3983363\n",
      "Validation loss decreased (0.714827 --> 0.703419).  Saving model ...\n",
      "Updating learning rate to 0.0025\n",
      "\titers: 100, epoch: 3 | loss: 0.3327482\n",
      "\tspeed: 0.7451s/iter; left time: 3359.5105s\n",
      "\titers: 200, epoch: 3 | loss: 0.3363656\n",
      "\tspeed: 0.2110s/iter; left time: 930.1154s\n",
      "Epoch: 3 cost time: 52.19716739654541\n",
      "Epoch: 3, Steps: 256 | Train Loss: 0.3528377 Vali Loss: 0.6809579 Test Loss: 0.4014288\n",
      "Validation loss decreased (0.703419 --> 0.680958).  Saving model ...\n",
      "Updating learning rate to 0.00125\n",
      "\titers: 100, epoch: 4 | loss: 0.3627482\n",
      "\tspeed: 0.7472s/iter; left time: 3177.6945s\n",
      "\titers: 200, epoch: 4 | loss: 0.3262992\n",
      "\tspeed: 0.1960s/iter; left time: 813.9744s\n",
      "Epoch: 4 cost time: 48.80375695228577\n",
      "Epoch: 4, Steps: 256 | Train Loss: 0.3435474 Vali Loss: 0.6801991 Test Loss: 0.3920157\n",
      "Validation loss decreased (0.680958 --> 0.680199).  Saving model ...\n",
      "Updating learning rate to 0.000625\n",
      "\titers: 100, epoch: 5 | loss: 0.4424896\n",
      "\tspeed: 0.7539s/iter; left time: 3013.4617s\n",
      "\titers: 200, epoch: 5 | loss: 0.3063396\n",
      "\tspeed: 0.1969s/iter; left time: 767.4350s\n",
      "Epoch: 5 cost time: 53.499327182769775\n",
      "Epoch: 5, Steps: 256 | Train Loss: 0.3376715 Vali Loss: 0.6735350 Test Loss: 0.3821295\n",
      "Validation loss decreased (0.680199 --> 0.673535).  Saving model ...\n",
      "Updating learning rate to 0.0003125\n",
      "\titers: 100, epoch: 6 | loss: 0.2990322\n",
      "\tspeed: 0.7930s/iter; left time: 2966.7929s\n",
      "\titers: 200, epoch: 6 | loss: 0.3904168\n",
      "\tspeed: 0.2190s/iter; left time: 797.2363s\n",
      "Epoch: 6 cost time: 54.705069065093994\n",
      "Epoch: 6, Steps: 256 | Train Loss: 0.3342070 Vali Loss: 0.6719543 Test Loss: 0.3848481\n",
      "Validation loss decreased (0.673535 --> 0.671954).  Saving model ...\n",
      "Updating learning rate to 0.00015625\n",
      "\titers: 100, epoch: 7 | loss: 0.3712859\n",
      "\tspeed: 0.7612s/iter; left time: 2652.6672s\n",
      "\titers: 200, epoch: 7 | loss: 0.2953336\n",
      "\tspeed: 0.2059s/iter; left time: 696.9965s\n",
      "Epoch: 7 cost time: 53.09979796409607\n",
      "Epoch: 7, Steps: 256 | Train Loss: 0.3336387 Vali Loss: 0.6683494 Test Loss: 0.3831507\n",
      "Validation loss decreased (0.671954 --> 0.668349).  Saving model ...\n",
      "Updating learning rate to 7.8125e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.3440401\n",
      "\tspeed: 0.7719s/iter; left time: 2492.4348s\n",
      "\titers: 200, epoch: 8 | loss: 0.2763094\n",
      "\tspeed: 0.2091s/iter; left time: 654.3078s\n",
      "Epoch: 8 cost time: 52.89399552345276\n",
      "Epoch: 8, Steps: 256 | Train Loss: 0.3307133 Vali Loss: 0.6664891 Test Loss: 0.3858272\n",
      "Validation loss decreased (0.668349 --> 0.666489).  Saving model ...\n",
      "Updating learning rate to 3.90625e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.3246619\n",
      "\tspeed: 0.7669s/iter; left time: 2279.9967s\n",
      "\titers: 200, epoch: 9 | loss: 0.3540023\n",
      "\tspeed: 0.2000s/iter; left time: 574.6472s\n",
      "Epoch: 9 cost time: 50.496145248413086\n",
      "Epoch: 9, Steps: 256 | Train Loss: 0.3310142 Vali Loss: 0.6630997 Test Loss: 0.3890249\n",
      "Validation loss decreased (0.666489 --> 0.663100).  Saving model ...\n",
      "Updating learning rate to 1.953125e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.3381975\n",
      "\tspeed: 0.7480s/iter; left time: 2032.3511s\n",
      "\titers: 200, epoch: 10 | loss: 0.3518775\n",
      "\tspeed: 0.1890s/iter; left time: 494.5700s\n",
      "Epoch: 10 cost time: 50.89687657356262\n",
      "Epoch: 10, Steps: 256 | Train Loss: 0.3313858 Vali Loss: 0.6713496 Test Loss: 0.3825421\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 9.765625e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.3165390\n",
      "\tspeed: 0.7680s/iter; left time: 1890.0188s\n",
      "\titers: 200, epoch: 11 | loss: 0.3441446\n",
      "\tspeed: 0.1950s/iter; left time: 460.3500s\n",
      "Epoch: 11 cost time: 51.40492582321167\n",
      "Epoch: 11, Steps: 256 | Train Loss: 0.3307604 Vali Loss: 0.6727996 Test Loss: 0.3905813\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 4.8828125e-06\n",
      "\titers: 100, epoch: 12 | loss: 0.2837804\n",
      "\tspeed: 0.7540s/iter; left time: 1662.5873s\n",
      "\titers: 200, epoch: 12 | loss: 0.3006190\n",
      "\tspeed: 0.1971s/iter; left time: 414.9058s\n",
      "Epoch: 12 cost time: 52.20729088783264\n",
      "Epoch: 12, Steps: 256 | Train Loss: 0.3308744 Vali Loss: 0.6696619 Test Loss: 0.3817389\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 2.44140625e-06\n",
      "\titers: 100, epoch: 13 | loss: 0.3129875\n",
      "\tspeed: 0.7680s/iter; left time: 1496.9252s\n",
      "\titers: 200, epoch: 13 | loss: 0.2896408\n",
      "\tspeed: 0.2220s/iter; left time: 410.4916s\n",
      "Epoch: 13 cost time: 56.10852384567261\n",
      "Epoch: 13, Steps: 256 | Train Loss: 0.3319831 Vali Loss: 0.6701956 Test Loss: 0.3912364\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.220703125e-06\n",
      "\titers: 100, epoch: 14 | loss: 0.3105388\n",
      "\tspeed: 0.7819s/iter; left time: 1323.7272s\n",
      "\titers: 200, epoch: 14 | loss: 0.3253219\n",
      "\tspeed: 0.2390s/iter; left time: 380.6730s\n",
      "Epoch: 14 cost time: 56.10387921333313\n",
      "Epoch: 14, Steps: 256 | Train Loss: 0.3312258 Vali Loss: 0.6696893 Test Loss: 0.3823275\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 6.103515625e-07\n",
      "\titers: 100, epoch: 15 | loss: 0.3740281\n",
      "\tspeed: 0.7830s/iter; left time: 1125.1916s\n",
      "\titers: 200, epoch: 15 | loss: 0.3735144\n",
      "\tspeed: 0.2241s/iter; left time: 299.6480s\n",
      "Epoch: 15 cost time: 56.905693769454956\n",
      "Epoch: 15, Steps: 256 | Train Loss: 0.3311956 Vali Loss: 0.6697605 Test Loss: 0.3858412\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 3.0517578125e-07\n",
      "\titers: 100, epoch: 16 | loss: 0.3311987\n",
      "\tspeed: 0.7528s/iter; left time: 889.1119s\n",
      "\titers: 200, epoch: 16 | loss: 0.3590097\n",
      "\tspeed: 0.2290s/iter; left time: 247.5279s\n",
      "Epoch: 16 cost time: 56.205952882766724\n",
      "Epoch: 16, Steps: 256 | Train Loss: 0.3300043 Vali Loss: 0.6723825 Test Loss: 0.3817332\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.52587890625e-07\n",
      "\titers: 100, epoch: 17 | loss: 0.3642025\n",
      "\tspeed: 0.7750s/iter; left time: 716.8940s\n",
      "\titers: 200, epoch: 17 | loss: 0.3671850\n",
      "\tspeed: 0.2120s/iter; left time: 174.9190s\n",
      "Epoch: 17 cost time: 55.10394096374512\n",
      "Epoch: 17, Steps: 256 | Train Loss: 0.3296109 Vali Loss: 0.6734975 Test Loss: 0.3853711\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 7.62939453125e-08\n",
      "\titers: 100, epoch: 18 | loss: 0.2686638\n",
      "\tspeed: 0.7990s/iter; left time: 534.5184s\n",
      "\titers: 200, epoch: 18 | loss: 0.3478645\n",
      "\tspeed: 0.1971s/iter; left time: 112.1317s\n",
      "Epoch: 18 cost time: 54.01132297515869\n",
      "Epoch: 18, Steps: 256 | Train Loss: 0.3300439 Vali Loss: 0.6677446 Test Loss: 0.3854293\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.814697265625e-08\n",
      "\titers: 100, epoch: 19 | loss: 0.3432914\n",
      "\tspeed: 0.7481s/iter; left time: 308.9601s\n",
      "\titers: 200, epoch: 19 | loss: 0.3386852\n",
      "\tspeed: 0.2008s/iter; left time: 62.8644s\n",
      "Epoch: 19 cost time: 53.89544105529785\n",
      "Epoch: 19, Steps: 256 | Train Loss: 0.3291515 Vali Loss: 0.6730978 Test Loss: 0.3817826\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): PatchTST_backbone(\n",
       "    (backbone): TSTiEncoder(\n",
       "      (W_P): Linear(in_features=24, out_features=16, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (encoder): TSTEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-2): 3 x TSTEncoderLayer(\n",
       "            (self_attn): _MultiheadAttention(\n",
       "              (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (sdp_attn): _ScaledDotProductAttention(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (1): Dropout(p=0.3, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout_attn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_attn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.3, inplace=False)\n",
       "              (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "            )\n",
       "            (dropout_ffn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_ffn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Flatten_Head(\n",
       "      (flatten): Flatten(start_dim=-2, end_dim=-1)\n",
       "      (linear): Linear(in_features=640, out_features=96, bias=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02310d36",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "747d79af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 2785\n",
      "mse:0.3890248239040375, mae:0.40697768330574036, rse:0.5924373865127563\n"
     ]
    }
   ],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf7747f",
   "metadata": {},
   "source": [
    "---\n",
    "## Trail 2: PatchTST/64, Dataset:ETTh1 , Metric: 96\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56cd7bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ETTh1,  Prediction Length : 96\n"
     ]
    }
   ],
   "source": [
    "args.pred_len = 96 # prediction sequence length\n",
    "args.num_patch = 64  # The number of input patches\n",
    "print(f\"Dataset: {args.data}, Prediction Length: {args.pred_len}, Number of Patches: {args.num_patch}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d072999d",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fe62726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "train 8209\n",
      "val 2785\n",
      "test 2785\n",
      "\titers: 100, epoch: 1 | loss: 0.5467896\n",
      "\tspeed: 0.4824s/iter; left time: 1833.4442s\n",
      "Epoch: 1 cost time: 91.73658847808838\n",
      "Epoch: 1, Steps: 195 | Train Loss: 0.5800090 Vali Loss: 0.8040251 Test Loss: 0.4371468\n",
      "Validation loss decreased (inf --> 0.804025).  Saving model ...\n",
      "Updating learning rate to 0.005\n",
      "\titers: 100, epoch: 2 | loss: 0.3973847\n",
      "\tspeed: 1.6531s/iter; left time: 5961.1725s\n",
      "Epoch: 2 cost time: 103.09685468673706\n",
      "Epoch: 2, Steps: 195 | Train Loss: 0.4063360 Vali Loss: 0.6899766 Test Loss: 0.4076010\n",
      "Validation loss decreased (0.804025 --> 0.689977).  Saving model ...\n",
      "Updating learning rate to 0.0025\n",
      "\titers: 100, epoch: 3 | loss: 0.3191050\n",
      "\tspeed: 1.7549s/iter; left time: 5985.8728s\n",
      "Epoch: 3 cost time: 106.60452842712402\n",
      "Epoch: 3, Steps: 195 | Train Loss: 0.3574004 Vali Loss: 0.7909207 Test Loss: 0.4054973\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.00125\n",
      "\titers: 100, epoch: 4 | loss: 0.3109620\n",
      "\tspeed: 1.7951s/iter; left time: 5773.1764s\n",
      "Epoch: 4 cost time: 106.21425485610962\n",
      "Epoch: 4, Steps: 195 | Train Loss: 0.3478560 Vali Loss: 0.6895709 Test Loss: 0.3902769\n",
      "Validation loss decreased (0.689977 --> 0.689571).  Saving model ...\n",
      "Updating learning rate to 0.000625\n",
      "\titers: 100, epoch: 5 | loss: 0.3650165\n",
      "\tspeed: 1.7788s/iter; left time: 5373.8936s\n",
      "Epoch: 5 cost time: 103.77920961380005\n",
      "Epoch: 5, Steps: 195 | Train Loss: 0.3416657 Vali Loss: 0.6582509 Test Loss: 0.4029776\n",
      "Validation loss decreased (0.689571 --> 0.658251).  Saving model ...\n",
      "Updating learning rate to 0.0003125\n",
      "\titers: 100, epoch: 6 | loss: 0.3290862\n",
      "\tspeed: 1.7541s/iter; left time: 4956.9547s\n",
      "Epoch: 6 cost time: 103.40585565567017\n",
      "Epoch: 6, Steps: 195 | Train Loss: 0.3377856 Vali Loss: 0.6936858 Test Loss: 0.3871210\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.00015625\n",
      "\titers: 100, epoch: 7 | loss: 0.3398261\n",
      "\tspeed: 1.7850s/iter; left time: 4696.4437s\n",
      "Epoch: 7 cost time: 105.70370411872864\n",
      "Epoch: 7, Steps: 195 | Train Loss: 0.3348519 Vali Loss: 0.6947209 Test Loss: 0.3919687\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 7.8125e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.2868885\n",
      "\tspeed: 1.7899s/iter; left time: 4360.2956s\n",
      "Epoch: 8 cost time: 105.19539451599121\n",
      "Epoch: 8, Steps: 195 | Train Loss: 0.3341228 Vali Loss: 0.6990926 Test Loss: 0.3894733\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.90625e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.3172925\n",
      "\tspeed: 1.7750s/iter; left time: 3977.8486s\n",
      "Epoch: 9 cost time: 104.6929566860199\n",
      "Epoch: 9, Steps: 195 | Train Loss: 0.3345398 Vali Loss: 0.6926588 Test Loss: 0.3874538\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 1.953125e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.3055095\n",
      "\tspeed: 1.7630s/iter; left time: 3607.0788s\n",
      "Epoch: 10 cost time: 104.58520889282227\n",
      "Epoch: 10, Steps: 195 | Train Loss: 0.3347121 Vali Loss: 0.6909971 Test Loss: 0.3902257\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 9.765625e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.3189489\n",
      "\tspeed: 1.7759s/iter; left time: 3287.2744s\n",
      "Epoch: 11 cost time: 104.49932146072388\n",
      "Epoch: 11, Steps: 195 | Train Loss: 0.3327148 Vali Loss: 0.6900715 Test Loss: 0.3919112\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 4.8828125e-06\n",
      "\titers: 100, epoch: 12 | loss: 0.3172328\n",
      "\tspeed: 1.7521s/iter; left time: 2901.4288s\n",
      "Epoch: 12 cost time: 103.49612545967102\n",
      "Epoch: 12, Steps: 195 | Train Loss: 0.3328925 Vali Loss: 0.7011456 Test Loss: 0.3880154\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 2.44140625e-06\n",
      "\titers: 100, epoch: 13 | loss: 0.3136107\n",
      "\tspeed: 1.7899s/iter; left time: 2615.0366s\n",
      "Epoch: 13 cost time: 103.8983428478241\n",
      "Epoch: 13, Steps: 195 | Train Loss: 0.3323249 Vali Loss: 0.6942977 Test Loss: 0.3868678\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.220703125e-06\n",
      "\titers: 100, epoch: 14 | loss: 0.2865804\n",
      "\tspeed: 1.7510s/iter; left time: 2216.7707s\n",
      "Epoch: 14 cost time: 102.80798053741455\n",
      "Epoch: 14, Steps: 195 | Train Loss: 0.3326001 Vali Loss: 0.6967915 Test Loss: 0.3899937\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 6.103515625e-07\n",
      "\titers: 100, epoch: 15 | loss: 0.3509160\n",
      "\tspeed: 1.7690s/iter; left time: 1894.6459s\n",
      "Epoch: 15 cost time: 104.89965796470642\n",
      "Epoch: 15, Steps: 195 | Train Loss: 0.3340858 Vali Loss: 0.6917649 Test Loss: 0.3888194\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): PatchTST_backbone(\n",
       "    (backbone): TSTiEncoder(\n",
       "      (W_P): Linear(in_features=48, out_features=16, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (encoder): TSTEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-2): 3 x TSTEncoderLayer(\n",
       "            (self_attn): _MultiheadAttention(\n",
       "              (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (sdp_attn): _ScaledDotProductAttention(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (1): Dropout(p=0.3, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout_attn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_attn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.3, inplace=False)\n",
       "              (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "            )\n",
       "            (dropout_ffn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_ffn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Flatten_Head(\n",
       "      (flatten): Flatten(start_dim=-2, end_dim=-1)\n",
       "      (linear): Linear(in_features=592, out_features=96, bias=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e8ff17",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "182670f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 2785\n",
      "mse:0.40297746658325195, mae:0.41574206948280334, rse:0.6028541922569275\n"
     ]
    }
   ],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e46022",
   "metadata": {},
   "source": [
    "---\n",
    "## Trail 3: PatchTST/42, Dataset:ETTh1 , Metric: 192\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3e95162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ETTh1,  Prediction Length : 192\n"
     ]
    }
   ],
   "source": [
    "args.pred_len = 192 # prediction sequence length\n",
    "args.num_patch = 42  # The number of input patches\n",
    "print(f\"Dataset: {args.data}, Prediction Length: {args.pred_len}, Number of Patches: {args.num_patch}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080f63c7",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "452dcead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "train 8113\n",
      "val 2689\n",
      "test 2689\n",
      "\titers: 100, epoch: 1 | loss: 0.4995359\n",
      "\tspeed: 0.7013s/iter; left time: 2637.6064s\n",
      "Epoch: 1 cost time: 132.8545961380005\n",
      "Epoch: 1, Steps: 193 | Train Loss: 0.6192046 Vali Loss: 0.9940767 Test Loss: 0.4705402\n",
      "Validation loss decreased (inf --> 0.994077).  Saving model ...\n",
      "Updating learning rate to 0.005\n",
      "\titers: 100, epoch: 2 | loss: 0.4622335\n",
      "\tspeed: 3.4180s/iter; left time: 12195.3505s\n",
      "Epoch: 2 cost time: 132.79999923706055\n",
      "Epoch: 2, Steps: 193 | Train Loss: 0.4497860 Vali Loss: 0.9738395 Test Loss: 0.4425929\n",
      "Validation loss decreased (0.994077 --> 0.973839).  Saving model ...\n",
      "Updating learning rate to 0.0025\n",
      "\titers: 100, epoch: 3 | loss: 0.3778394\n",
      "\tspeed: 3.4640s/iter; left time: 11690.9593s\n",
      "Epoch: 3 cost time: 134.50500321388245\n",
      "Epoch: 3, Steps: 193 | Train Loss: 0.3972456 Vali Loss: 1.0681245 Test Loss: 0.4415466\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.00125\n",
      "\titers: 100, epoch: 4 | loss: 0.3657199\n",
      "\tspeed: 3.4542s/iter; left time: 10991.1515s\n",
      "Epoch: 4 cost time: 137.10132598876953\n",
      "Epoch: 4, Steps: 193 | Train Loss: 0.3845396 Vali Loss: 0.9977632 Test Loss: 0.4282127\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.000625\n",
      "\titers: 100, epoch: 5 | loss: 0.3726870\n",
      "\tspeed: 3.4919s/iter; left time: 10437.4128s\n",
      "Epoch: 5 cost time: 137.50391578674316\n",
      "Epoch: 5, Steps: 193 | Train Loss: 0.3760355 Vali Loss: 1.0298538 Test Loss: 0.4373082\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 0.0003125\n",
      "\titers: 100, epoch: 6 | loss: 0.3748501\n",
      "\tspeed: 3.4289s/iter; left time: 9587.2352s\n",
      "Epoch: 6 cost time: 131.6052474975586\n",
      "Epoch: 6, Steps: 193 | Train Loss: 0.3737838 Vali Loss: 1.0652004 Test Loss: 0.4270855\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 0.00015625\n",
      "\titers: 100, epoch: 7 | loss: 0.3432400\n",
      "\tspeed: 3.4341s/iter; left time: 8938.8407s\n",
      "Epoch: 7 cost time: 136.10925889015198\n",
      "Epoch: 7, Steps: 193 | Train Loss: 0.3702897 Vali Loss: 1.0318326 Test Loss: 0.4281594\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 7.8125e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.3629672\n",
      "\tspeed: 3.4890s/iter; left time: 8408.4479s\n",
      "Epoch: 8 cost time: 139.60064244270325\n",
      "Epoch: 8, Steps: 193 | Train Loss: 0.3685897 Vali Loss: 1.0472976 Test Loss: 0.4274574\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 3.90625e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.3681879\n",
      "\tspeed: 3.4549s/iter; left time: 7659.6118s\n",
      "Epoch: 9 cost time: 134.61568188667297\n",
      "Epoch: 9, Steps: 193 | Train Loss: 0.3695828 Vali Loss: 1.0404140 Test Loss: 0.4323559\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.953125e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.3536998\n",
      "\tspeed: 3.4640s/iter; left time: 7011.1153s\n",
      "Epoch: 10 cost time: 135.60850358009338\n",
      "Epoch: 10, Steps: 193 | Train Loss: 0.3681631 Vali Loss: 1.0385222 Test Loss: 0.4247015\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 9.765625e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.4061374\n",
      "\tspeed: 3.3941s/iter; left time: 6214.6803s\n",
      "Epoch: 11 cost time: 135.99207067489624\n",
      "Epoch: 11, Steps: 193 | Train Loss: 0.3684749 Vali Loss: 1.0408494 Test Loss: 0.4243160\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 4.8828125e-06\n",
      "\titers: 100, epoch: 12 | loss: 0.3677338\n",
      "\tspeed: 3.4979s/iter; left time: 5729.4908s\n",
      "Epoch: 12 cost time: 141.69359731674194\n",
      "Epoch: 12, Steps: 193 | Train Loss: 0.3689454 Vali Loss: 1.0350531 Test Loss: 0.4258559\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): PatchTST_backbone(\n",
       "    (backbone): TSTiEncoder(\n",
       "      (W_P): Linear(in_features=48, out_features=16, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (encoder): TSTEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-2): 3 x TSTEncoderLayer(\n",
       "            (self_attn): _MultiheadAttention(\n",
       "              (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (sdp_attn): _ScaledDotProductAttention(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (1): Dropout(p=0.3, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout_attn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_attn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.3, inplace=False)\n",
       "              (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "            )\n",
       "            (dropout_ffn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_ffn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Flatten_Head(\n",
       "      (flatten): Flatten(start_dim=-2, end_dim=-1)\n",
       "      (linear): Linear(in_features=592, out_features=192, bias=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b463f4f6",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dcf4762d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 2689\n",
      "mse:0.4425927400588989, mae:0.44522473216056824, rse:0.6317625045776367\n"
     ]
    }
   ],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53387ea",
   "metadata": {},
   "source": [
    "## Trail 4: PatchTST/64, Dataset:ETTh1, Metric: 192\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f9e9f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ETTh1,  Prediction Length : 192\n"
     ]
    }
   ],
   "source": [
    "args.pred_len = 192 # prediction sequence length\n",
    "args.num_patch = 64  # The number of input patches\n",
    "print(f\"Dataset: {args.data}, Prediction Length: {args.pred_len}, Number of Patches: {args.num_patch}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ce55a3",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a303366a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "train 8113\n",
      "val 2689\n",
      "test 2689\n",
      "\titers: 100, epoch: 1 | loss: 0.5080305\n",
      "\tspeed: 0.7014s/iter; left time: 2637.9586s\n",
      "Epoch: 1 cost time: 138.65479850769043\n",
      "Epoch: 1, Steps: 193 | Train Loss: 0.5953040 Vali Loss: 0.9777954 Test Loss: 0.4816293\n",
      "Validation loss decreased (inf --> 0.977795).  Saving model ...\n",
      "Updating learning rate to 0.005\n",
      "\titers: 100, epoch: 2 | loss: 0.4542234\n",
      "\tspeed: 3.5243s/iter; left time: 12574.5809s\n",
      "Epoch: 2 cost time: 138.20450687408447\n",
      "Epoch: 2, Steps: 193 | Train Loss: 0.4473222 Vali Loss: 0.9944211 Test Loss: 0.4389979\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0025\n",
      "\titers: 100, epoch: 3 | loss: 0.4092036\n",
      "\tspeed: 3.5358s/iter; left time: 11933.3309s\n",
      "Epoch: 3 cost time: 138.68963861465454\n",
      "Epoch: 3, Steps: 193 | Train Loss: 0.3981723 Vali Loss: 1.0615439 Test Loss: 0.4434692\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.00125\n",
      "\titers: 100, epoch: 4 | loss: 0.3955112\n",
      "\tspeed: 3.5299s/iter; left time: 11232.2908s\n",
      "Epoch: 4 cost time: 140.5100541114807\n",
      "Epoch: 4, Steps: 193 | Train Loss: 0.3844619 Vali Loss: 1.0052850 Test Loss: 0.4262930\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 0.000625\n",
      "\titers: 100, epoch: 5 | loss: 0.3548492\n",
      "\tspeed: 3.4922s/iter; left time: 10438.2038s\n",
      "Epoch: 5 cost time: 133.29297828674316\n",
      "Epoch: 5, Steps: 193 | Train Loss: 0.3783261 Vali Loss: 1.0061589 Test Loss: 0.4352410\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 0.0003125\n",
      "\titers: 100, epoch: 6 | loss: 0.3574375\n",
      "\tspeed: 3.1598s/iter; left time: 8834.7205s\n",
      "Epoch: 6 cost time: 116.70295286178589\n",
      "Epoch: 6, Steps: 193 | Train Loss: 0.3734456 Vali Loss: 1.0509400 Test Loss: 0.4364486\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 0.00015625\n",
      "\titers: 100, epoch: 7 | loss: 0.3337778\n",
      "\tspeed: 3.0641s/iter; left time: 7975.8411s\n",
      "Epoch: 7 cost time: 125.50337266921997\n",
      "Epoch: 7, Steps: 193 | Train Loss: 0.3731549 Vali Loss: 1.0211833 Test Loss: 0.4356325\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 7.8125e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.3657375\n",
      "\tspeed: 3.0570s/iter; left time: 7367.2592s\n",
      "Epoch: 8 cost time: 126.38996291160583\n",
      "Epoch: 8, Steps: 193 | Train Loss: 0.3705240 Vali Loss: 1.0209922 Test Loss: 0.4307008\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.90625e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.3727251\n",
      "\tspeed: 2.8060s/iter; left time: 6220.8134s\n",
      "Epoch: 9 cost time: 122.2148084640503\n",
      "Epoch: 9, Steps: 193 | Train Loss: 0.3701927 Vali Loss: 1.0535198 Test Loss: 0.4333249\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.953125e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.4045749\n",
      "\tspeed: 2.9860s/iter; left time: 6043.6937s\n",
      "Epoch: 10 cost time: 105.40415716171265\n",
      "Epoch: 10, Steps: 193 | Train Loss: 0.3705127 Vali Loss: 1.0461564 Test Loss: 0.4339640\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 9.765625e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.3384940\n",
      "\tspeed: 2.7850s/iter; left time: 5099.2832s\n",
      "Epoch: 11 cost time: 108.90395903587341\n",
      "Epoch: 11, Steps: 193 | Train Loss: 0.3692882 Vali Loss: 1.0490936 Test Loss: 0.4343427\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): PatchTST_backbone(\n",
       "    (backbone): TSTiEncoder(\n",
       "      (W_P): Linear(in_features=48, out_features=16, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (encoder): TSTEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-2): 3 x TSTEncoderLayer(\n",
       "            (self_attn): _MultiheadAttention(\n",
       "              (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (sdp_attn): _ScaledDotProductAttention(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (1): Dropout(p=0.3, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout_attn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_attn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.3, inplace=False)\n",
       "              (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "            )\n",
       "            (dropout_ffn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_ffn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Flatten_Head(\n",
       "      (flatten): Flatten(start_dim=-2, end_dim=-1)\n",
       "      (linear): Linear(in_features=592, out_features=192, bias=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc749e1e",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a8039c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 2689\n",
      "mse:0.4816294014453888, mae:0.4722605049610138, rse:0.6590345501899719\n"
     ]
    }
   ],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db261cc",
   "metadata": {},
   "source": [
    "---\n",
    "## Trail 5: PatchTST/42, Dataset:ETTh1,  Metric: 336\n",
    "\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54c35531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ETTh1,  Prediction Length : 336\n"
     ]
    }
   ],
   "source": [
    "args.pred_len = 336 # prediction sequence length\n",
    "args.num_patch = 42  # The number of input patches\n",
    "print(f\"Dataset: {args.data}, Prediction Length: {args.pred_len}, Number of Patches: {args.num_patch}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d04ed3",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f566bd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "train 7969\n",
      "val 2545\n",
      "test 2545\n",
      "\titers: 100, epoch: 1 | loss: 0.5583267\n",
      "\tspeed: 0.5613s/iter; left time: 2066.2315s\n",
      "Epoch: 1 cost time: 108.53734970092773\n",
      "Epoch: 1, Steps: 189 | Train Loss: 0.6319610 Vali Loss: 1.1278813 Test Loss: 0.5054743\n",
      "Validation loss decreased (inf --> 1.127881).  Saving model ...\n",
      "Updating learning rate to 0.005\n",
      "\titers: 100, epoch: 2 | loss: 0.4190464\n",
      "\tspeed: 2.9080s/iter; left time: 10154.8707s\n",
      "Epoch: 2 cost time: 116.59747886657715\n",
      "Epoch: 2, Steps: 189 | Train Loss: 0.4895814 Vali Loss: 1.1570295 Test Loss: 0.4693138\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0025\n",
      "\titers: 100, epoch: 3 | loss: 0.4272126\n",
      "\tspeed: 2.9900s/iter; left time: 9875.9737s\n",
      "Epoch: 3 cost time: 114.30476760864258\n",
      "Epoch: 3, Steps: 189 | Train Loss: 0.4388608 Vali Loss: 1.3474638 Test Loss: 0.4732141\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.00125\n",
      "\titers: 100, epoch: 4 | loss: 0.4151227\n",
      "\tspeed: 2.7710s/iter; left time: 8628.8553s\n",
      "Epoch: 4 cost time: 116.90344500541687\n",
      "Epoch: 4, Steps: 189 | Train Loss: 0.4247697 Vali Loss: 1.3955486 Test Loss: 0.4825970\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 0.000625\n",
      "\titers: 100, epoch: 5 | loss: 0.4331438\n",
      "\tspeed: 2.7530s/iter; left time: 8052.6484s\n",
      "Epoch: 5 cost time: 107.29851722717285\n",
      "Epoch: 5, Steps: 189 | Train Loss: 0.4157020 Vali Loss: 1.3054463 Test Loss: 0.4946689\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 0.0003125\n",
      "\titers: 100, epoch: 6 | loss: 0.3904489\n",
      "\tspeed: 2.7000s/iter; left time: 7387.1079s\n",
      "Epoch: 6 cost time: 105.50635862350464\n",
      "Epoch: 6, Steps: 189 | Train Loss: 0.4109057 Vali Loss: 1.3248111 Test Loss: 0.5040290\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 0.00015625\n",
      "\titers: 100, epoch: 7 | loss: 0.4390235\n",
      "\tspeed: 2.6900s/iter; left time: 6851.3264s\n",
      "Epoch: 7 cost time: 109.30441045761108\n",
      "Epoch: 7, Steps: 189 | Train Loss: 0.4096856 Vali Loss: 1.3334240 Test Loss: 0.4927324\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 7.8125e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.3866361\n",
      "\tspeed: 2.6720s/iter; left time: 6300.6524s\n",
      "Epoch: 8 cost time: 110.90742659568787\n",
      "Epoch: 8, Steps: 189 | Train Loss: 0.4085676 Vali Loss: 1.3437924 Test Loss: 0.4968493\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.90625e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.4216042\n",
      "\tspeed: 2.4111s/iter; left time: 5229.7001s\n",
      "Epoch: 9 cost time: 81.90386295318604\n",
      "Epoch: 9, Steps: 189 | Train Loss: 0.4068441 Vali Loss: 1.3344988 Test Loss: 0.5091564\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.953125e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.3911578\n",
      "\tspeed: 2.1129s/iter; left time: 4183.4564s\n",
      "Epoch: 10 cost time: 81.50329065322876\n",
      "Epoch: 10, Steps: 189 | Train Loss: 0.4067088 Vali Loss: 1.3510957 Test Loss: 0.4930185\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 9.765625e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.3894469\n",
      "\tspeed: 2.1320s/iter; left time: 3818.4139s\n",
      "Epoch: 11 cost time: 76.2073106765747\n",
      "Epoch: 11, Steps: 189 | Train Loss: 0.4082737 Vali Loss: 1.3398411 Test Loss: 0.5054348\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): PatchTST_backbone(\n",
       "    (backbone): TSTiEncoder(\n",
       "      (W_P): Linear(in_features=48, out_features=16, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (encoder): TSTEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-2): 3 x TSTEncoderLayer(\n",
       "            (self_attn): _MultiheadAttention(\n",
       "              (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (sdp_attn): _ScaledDotProductAttention(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (1): Dropout(p=0.3, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout_attn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_attn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.3, inplace=False)\n",
       "              (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "            )\n",
       "            (dropout_ffn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_ffn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Flatten_Head(\n",
       "      (flatten): Flatten(start_dim=-2, end_dim=-1)\n",
       "      (linear): Linear(in_features=592, out_features=336, bias=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16553948",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cfe7d853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 2545\n",
      "mse:0.5054745078086853, mae:0.4913105070590973, rse:0.6770716905593872\n"
     ]
    }
   ],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af001411",
   "metadata": {},
   "source": [
    "---\n",
    "## Trail 6: PatchTST/64, Dataset:ETTh1,  Metric: 336\n",
    "\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91d0a519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ETTh1,  Prediction Length : 336\n"
     ]
    }
   ],
   "source": [
    "args.pred_len = 336 # prediction sequence length\n",
    "args.num_patch = 64  # The number of input patches\n",
    "print(f\"Dataset: {args.data}, Prediction Length: {args.pred_len}, Number of Patches: {args.num_patch}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f28b21b",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84cecd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "train 7969\n",
      "val 2545\n",
      "test 2545\n",
      "\titers: 100, epoch: 1 | loss: 0.5901074\n",
      "\tspeed: 0.4463s/iter; left time: 1642.9432s\n",
      "Epoch: 1 cost time: 79.73448538780212\n",
      "Epoch: 1, Steps: 189 | Train Loss: 0.6334951 Vali Loss: 1.1467382 Test Loss: 0.5077454\n",
      "Validation loss decreased (inf --> 1.146738).  Saving model ...\n",
      "Updating learning rate to 0.005\n",
      "\titers: 100, epoch: 2 | loss: 0.4579918\n",
      "\tspeed: 1.9870s/iter; left time: 6938.4719s\n",
      "Epoch: 2 cost time: 88.40225148200989\n",
      "Epoch: 2, Steps: 189 | Train Loss: 0.4895072 Vali Loss: 1.1918480 Test Loss: 0.4653360\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0025\n",
      "\titers: 100, epoch: 3 | loss: 0.4123957\n",
      "\tspeed: 2.0161s/iter; left time: 6659.2653s\n",
      "Epoch: 3 cost time: 75.4123055934906\n",
      "Epoch: 3, Steps: 189 | Train Loss: 0.4425045 Vali Loss: 1.1529659 Test Loss: 0.4635212\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.00125\n",
      "\titers: 100, epoch: 4 | loss: 0.5498210\n",
      "\tspeed: 1.8259s/iter; left time: 5685.7970s\n",
      "Epoch: 4 cost time: 79.69808793067932\n",
      "Epoch: 4, Steps: 189 | Train Loss: 0.4248822 Vali Loss: 1.2404255 Test Loss: 0.4753364\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 0.000625\n",
      "\titers: 100, epoch: 5 | loss: 0.4383032\n",
      "\tspeed: 2.1280s/iter; left time: 6224.5369s\n",
      "Epoch: 5 cost time: 82.00828218460083\n",
      "Epoch: 5, Steps: 189 | Train Loss: 0.4160439 Vali Loss: 1.2235295 Test Loss: 0.4828525\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 0.0003125\n",
      "\titers: 100, epoch: 6 | loss: 0.4006024\n",
      "\tspeed: 2.3940s/iter; left time: 6550.1121s\n",
      "Epoch: 6 cost time: 102.7039623260498\n",
      "Epoch: 6, Steps: 189 | Train Loss: 0.4115851 Vali Loss: 1.2632675 Test Loss: 0.4763305\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 0.00015625\n",
      "\titers: 100, epoch: 7 | loss: 0.4279702\n",
      "\tspeed: 2.3730s/iter; left time: 6043.9053s\n",
      "Epoch: 7 cost time: 106.90877199172974\n",
      "Epoch: 7, Steps: 189 | Train Loss: 0.4104849 Vali Loss: 1.2311493 Test Loss: 0.4825793\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 7.8125e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.3738531\n",
      "\tspeed: 2.5240s/iter; left time: 5951.5712s\n",
      "Epoch: 8 cost time: 101.59983563423157\n",
      "Epoch: 8, Steps: 189 | Train Loss: 0.4077742 Vali Loss: 1.2270771 Test Loss: 0.4817298\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.90625e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.3914217\n",
      "\tspeed: 2.3560s/iter; left time: 5110.2181s\n",
      "Epoch: 9 cost time: 96.400719165802\n",
      "Epoch: 9, Steps: 189 | Train Loss: 0.4071129 Vali Loss: 1.2336680 Test Loss: 0.4863552\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.953125e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.4401097\n",
      "\tspeed: 2.2810s/iter; left time: 4516.3424s\n",
      "Epoch: 10 cost time: 100.30541491508484\n",
      "Epoch: 10, Steps: 189 | Train Loss: 0.4078483 Vali Loss: 1.2516218 Test Loss: 0.4848926\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 9.765625e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.4702659\n",
      "\tspeed: 2.6300s/iter; left time: 4710.2944s\n",
      "Epoch: 11 cost time: 97.9989447593689\n",
      "Epoch: 11, Steps: 189 | Train Loss: 0.4084035 Vali Loss: 1.2405561 Test Loss: 0.4894927\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): PatchTST_backbone(\n",
       "    (backbone): TSTiEncoder(\n",
       "      (W_P): Linear(in_features=48, out_features=16, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (encoder): TSTEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-2): 3 x TSTEncoderLayer(\n",
       "            (self_attn): _MultiheadAttention(\n",
       "              (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (sdp_attn): _ScaledDotProductAttention(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (1): Dropout(p=0.3, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout_attn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_attn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.3, inplace=False)\n",
       "              (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "            )\n",
       "            (dropout_ffn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_ffn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Flatten_Head(\n",
       "      (flatten): Flatten(start_dim=-2, end_dim=-1)\n",
       "      (linear): Linear(in_features=592, out_features=336, bias=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a0f4d",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e7e616f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 2545\n",
      "mse:0.507745623588562, mae:0.4910207688808441, rse:0.6785910725593567\n"
     ]
    }
   ],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32c4e45",
   "metadata": {},
   "source": [
    "---\n",
    "## Trail 7: PatchTST/42, Dataset:ETTh1, Metric: 720\n",
    "\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21bea2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ETTh1,  Prediction Length : 720\n"
     ]
    }
   ],
   "source": [
    "args.pred_len = 720 # prediction sequence length\n",
    "args.num_patch = 42  # The number of input patches\n",
    "print(f\"Dataset: {args.data}, Prediction Length: {args.pred_len}, Number of Patches: {args.num_patch}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97742cbc",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "59e3eb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "train 7585\n",
      "val 2161\n",
      "test 2161\n",
      "\titers: 100, epoch: 1 | loss: 0.6874392\n",
      "\tspeed: 0.5746s/iter; left time: 2011.7808s\n",
      "Epoch: 1 cost time: 102.16214656829834\n",
      "Epoch: 1, Steps: 180 | Train Loss: 0.6947509 Vali Loss: 1.2598931 Test Loss: 0.5196058\n",
      "Validation loss decreased (inf --> 1.259893).  Saving model ...\n",
      "Updating learning rate to 0.005\n",
      "\titers: 100, epoch: 2 | loss: 0.5145122\n",
      "\tspeed: 2.2390s/iter; left time: 7435.7632s\n",
      "Epoch: 2 cost time: 97.10556840896606\n",
      "Epoch: 2, Steps: 180 | Train Loss: 0.5511002 Vali Loss: 1.5157858 Test Loss: 0.6210873\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0025\n",
      "\titers: 100, epoch: 3 | loss: 0.4539023\n",
      "\tspeed: 2.1410s/iter; left time: 6724.7364s\n",
      "Epoch: 3 cost time: 97.00365948677063\n",
      "Epoch: 3, Steps: 180 | Train Loss: 0.4978741 Vali Loss: 1.3462009 Test Loss: 0.5808331\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.00125\n",
      "\titers: 100, epoch: 4 | loss: 0.4298618\n",
      "\tspeed: 2.3790s/iter; left time: 7044.2243s\n",
      "Epoch: 4 cost time: 96.80270600318909\n",
      "Epoch: 4, Steps: 180 | Train Loss: 0.4804995 Vali Loss: 1.3413665 Test Loss: 0.5914875\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 0.000625\n",
      "\titers: 100, epoch: 5 | loss: 0.4581125\n",
      "\tspeed: 2.3330s/iter; left time: 6488.0746s\n",
      "Epoch: 5 cost time: 96.60789036750793\n",
      "Epoch: 5, Steps: 180 | Train Loss: 0.4691798 Vali Loss: 1.4174939 Test Loss: 0.6130513\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 0.0003125\n",
      "\titers: 100, epoch: 6 | loss: 0.4321881\n",
      "\tspeed: 2.2271s/iter; left time: 5792.5650s\n",
      "Epoch: 6 cost time: 96.80820655822754\n",
      "Epoch: 6, Steps: 180 | Train Loss: 0.4624490 Vali Loss: 1.3708009 Test Loss: 0.6289533\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 0.00015625\n",
      "\titers: 100, epoch: 7 | loss: 0.4095309\n",
      "\tspeed: 2.2499s/iter; left time: 5447.0810s\n",
      "Epoch: 7 cost time: 101.10256814956665\n",
      "Epoch: 7, Steps: 180 | Train Loss: 0.4587055 Vali Loss: 1.3561554 Test Loss: 0.6245556\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 7.8125e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.5211796\n",
      "\tspeed: 2.2681s/iter; left time: 5082.8499s\n",
      "Epoch: 8 cost time: 96.60070180892944\n",
      "Epoch: 8, Steps: 180 | Train Loss: 0.4587728 Vali Loss: 1.3586209 Test Loss: 0.6024084\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.90625e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.4070580\n",
      "\tspeed: 2.2159s/iter; left time: 4567.0146s\n",
      "Epoch: 9 cost time: 97.4023027420044\n",
      "Epoch: 9, Steps: 180 | Train Loss: 0.4555330 Vali Loss: 1.3617899 Test Loss: 0.6178809\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.953125e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.4005677\n",
      "\tspeed: 2.2740s/iter; left time: 4277.4308s\n",
      "Epoch: 10 cost time: 97.10535931587219\n",
      "Epoch: 10, Steps: 180 | Train Loss: 0.4568012 Vali Loss: 1.3684957 Test Loss: 0.6219080\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 9.765625e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.4893120\n",
      "\tspeed: 2.2330s/iter; left time: 3798.2915s\n",
      "Epoch: 11 cost time: 92.8970398902893\n",
      "Epoch: 11, Steps: 180 | Train Loss: 0.4557191 Vali Loss: 1.3752801 Test Loss: 0.6036213\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): PatchTST_backbone(\n",
       "    (backbone): TSTiEncoder(\n",
       "      (W_P): Linear(in_features=48, out_features=16, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (encoder): TSTEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-2): 3 x TSTEncoderLayer(\n",
       "            (self_attn): _MultiheadAttention(\n",
       "              (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (sdp_attn): _ScaledDotProductAttention(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (1): Dropout(p=0.3, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout_attn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_attn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.3, inplace=False)\n",
       "              (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "            )\n",
       "            (dropout_ffn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_ffn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Flatten_Head(\n",
       "      (flatten): Flatten(start_dim=-2, end_dim=-1)\n",
       "      (linear): Linear(in_features=592, out_features=720, bias=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2c6a63",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4f88e113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 2161\n",
      "mse:0.5196058750152588, mae:0.5248337984085083, rse:0.6904626488685608\n"
     ]
    }
   ],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e12a37",
   "metadata": {},
   "source": [
    "---\n",
    "## Trail 8: PatchTST/64, Dataset:ETTh1, Metric: 336\n",
    "\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd7ffaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ETTh1, Prediction Length: 336, Number of Patches: 64\n"
     ]
    }
   ],
   "source": [
    "args.pred_len = 336 # prediction sequence length\n",
    "args.num_patch = 64  # The number of input patches\n",
    "print(f\"Dataset: {args.data}, Prediction Length: {args.pred_len}, Number of Patches: {args.num_patch}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef9c072",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab133bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "train 7969\n",
      "val 2545\n",
      "test 2545\n",
      "\titers: 100, epoch: 1 | loss: 0.6037611\n",
      "\tspeed: 0.5880s/iter; left time: 2869.9431s\n",
      "\titers: 200, epoch: 1 | loss: 0.5447749\n",
      "\tspeed: 0.5710s/iter; left time: 2730.1475s\n",
      "Epoch: 1 cost time: 143.60614132881165\n",
      "Epoch: 1, Steps: 249 | Train Loss: 0.6057979 Vali Loss: 1.1328398 Test Loss: 0.4814026\n",
      "Validation loss decreased (inf --> 1.132840).  Saving model ...\n",
      "Updating learning rate to 0.005\n",
      "\titers: 100, epoch: 2 | loss: 0.5242595\n",
      "\tspeed: 2.8460s/iter; left time: 13182.7123s\n",
      "\titers: 200, epoch: 2 | loss: 0.4272539\n",
      "\tspeed: 0.5520s/iter; left time: 2501.6329s\n",
      "Epoch: 2 cost time: 137.59608268737793\n",
      "Epoch: 2, Steps: 249 | Train Loss: 0.4929329 Vali Loss: 1.2551457 Test Loss: 0.4989186\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0025\n",
      "\titers: 100, epoch: 3 | loss: 0.3851923\n",
      "\tspeed: 2.7419s/iter; left time: 12017.9617s\n",
      "\titers: 200, epoch: 3 | loss: 0.3962872\n",
      "\tspeed: 0.5690s/iter; left time: 2437.0923s\n",
      "Epoch: 3 cost time: 141.91222023963928\n",
      "Epoch: 3, Steps: 249 | Train Loss: 0.4360663 Vali Loss: 1.1973267 Test Loss: 0.5068502\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.00125\n",
      "\titers: 100, epoch: 4 | loss: 0.3835747\n",
      "\tspeed: 2.6091s/iter; left time: 10785.8666s\n",
      "\titers: 200, epoch: 4 | loss: 0.4795230\n",
      "\tspeed: 0.5492s/iter; left time: 2215.4353s\n",
      "Epoch: 4 cost time: 133.3936276435852\n",
      "Epoch: 4, Steps: 249 | Train Loss: 0.4221473 Vali Loss: 1.3083817 Test Loss: 0.4918895\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 0.000625\n",
      "\titers: 100, epoch: 5 | loss: 0.4678582\n",
      "\tspeed: 2.7708s/iter; left time: 10764.4997s\n",
      "\titers: 200, epoch: 5 | loss: 0.4288051\n",
      "\tspeed: 0.4990s/iter; left time: 1888.7445s\n",
      "Epoch: 5 cost time: 130.20862913131714\n",
      "Epoch: 5, Steps: 249 | Train Loss: 0.4150668 Vali Loss: 1.2709043 Test Loss: 0.5020078\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 0.0003125\n",
      "\titers: 100, epoch: 6 | loss: 0.3699443\n",
      "\tspeed: 2.4761s/iter; left time: 9003.0409s\n",
      "\titers: 200, epoch: 6 | loss: 0.4120097\n",
      "\tspeed: 0.4999s/iter; left time: 1767.7092s\n",
      "Epoch: 6 cost time: 126.69848704338074\n",
      "Epoch: 6, Steps: 249 | Train Loss: 0.4128592 Vali Loss: 1.3596120 Test Loss: 0.5044760\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 0.00015625\n",
      "\titers: 100, epoch: 7 | loss: 0.3936920\n",
      "\tspeed: 2.5660s/iter; left time: 8691.0706s\n",
      "\titers: 200, epoch: 7 | loss: 0.3954923\n",
      "\tspeed: 0.5689s/iter; left time: 1870.1022s\n",
      "Epoch: 7 cost time: 141.11161518096924\n",
      "Epoch: 7, Steps: 249 | Train Loss: 0.4099764 Vali Loss: 1.3206706 Test Loss: 0.5022225\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 7.8125e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.4134100\n",
      "\tspeed: 2.5470s/iter; left time: 7992.6278s\n",
      "\titers: 200, epoch: 8 | loss: 0.3687480\n",
      "\tspeed: 0.5571s/iter; left time: 1692.3745s\n",
      "Epoch: 8 cost time: 139.30558586120605\n",
      "Epoch: 8, Steps: 249 | Train Loss: 0.4086291 Vali Loss: 1.3268822 Test Loss: 0.5241141\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.90625e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.3778930\n",
      "\tspeed: 2.6269s/iter; left time: 7589.1439s\n",
      "\titers: 200, epoch: 9 | loss: 0.3976888\n",
      "\tspeed: 0.5591s/iter; left time: 1559.1937s\n",
      "Epoch: 9 cost time: 134.39663863182068\n",
      "Epoch: 9, Steps: 249 | Train Loss: 0.4084744 Vali Loss: 1.3110678 Test Loss: 0.5072247\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.953125e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.3775881\n",
      "\tspeed: 2.7679s/iter; left time: 7307.3083s\n",
      "\titers: 200, epoch: 10 | loss: 0.4176428\n",
      "\tspeed: 0.5021s/iter; left time: 1275.2223s\n",
      "Epoch: 10 cost time: 127.11922574043274\n",
      "Epoch: 10, Steps: 249 | Train Loss: 0.4063716 Vali Loss: 1.3153251 Test Loss: 0.5149010\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 9.765625e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.4463665\n",
      "\tspeed: 2.7439s/iter; left time: 6560.7791s\n",
      "\titers: 200, epoch: 11 | loss: 0.3806969\n",
      "\tspeed: 0.4841s/iter; left time: 1108.9818s\n",
      "Epoch: 11 cost time: 127.11022877693176\n",
      "Epoch: 11, Steps: 249 | Train Loss: 0.4085485 Vali Loss: 1.3308434 Test Loss: 0.5202548\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): PatchTST_backbone(\n",
       "    (backbone): TSTiEncoder(\n",
       "      (W_P): Linear(in_features=24, out_features=16, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (encoder): TSTEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-2): 3 x TSTEncoderLayer(\n",
       "            (self_attn): _MultiheadAttention(\n",
       "              (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (sdp_attn): _ScaledDotProductAttention(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (1): Dropout(p=0.3, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout_attn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_attn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.3, inplace=False)\n",
       "              (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "            )\n",
       "            (dropout_ffn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_ffn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Flatten_Head(\n",
       "      (flatten): Flatten(start_dim=-2, end_dim=-1)\n",
       "      (linear): Linear(in_features=640, out_features=336, bias=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fafd97f",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4861f5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 2545\n",
      "mse:0.4814022183418274, mae:0.4737595319747925, rse:0.6606630086898804\n"
     ]
    }
   ],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea7d54a",
   "metadata": {},
   "source": [
    "### Compare our results with paper results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83190ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=r\"./Images/ETT1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cce209c",
   "metadata": {},
   "source": [
    "#### **Experiment Results**:\n",
    "Comaparing my results with the paper resulted highlited in the image above.\n",
    "\n",
    "| PatchTST/42 | Dataset | Seq_len | MSE | MAE |\n",
    "|---|---|---|---|---|\n",
    "|  | ETTh1 | 96 |  0.37019482254981995| 0.3915349543094635  |\n",
    "|  | ETTh1 | 192 |  0.40499380230903625 | 0.4138428270816803 |\n",
    "|  | ETTh1 | 336 | 0.4337225556373596 | 0.434622198343277  |\n",
    "|  | ETTh1 | 720 | 0.470274418592453| 0.4867483675479889 |\n",
    "\n",
    "\n",
    "| PatchTST/64 | Dataset | Seq_len | MSE | MAE |\n",
    "|---|---|---|---|---|\n",
    "|  | ETTh1 | 96 |  0.37019482254981995| 0.3915349543094635  |\n",
    "|  | ETTh1 | 192 |  0.40499380230903625 | 0.4138428270816803 |\n",
    "|  | ETTh1 | 336 | 0.4337225556373596 | 0.434622198343277  |\n",
    "|  | ETTh1 | 720 | 0.470274418592453| 0.4867483675479889 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b38822a",
   "metadata": {},
   "source": [
    "---\n",
    "# Working on ETTh2 Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ebe658",
   "metadata": {},
   "source": [
    "## Trail 1: PatchTST/42, Dataset:ETTh2,  Metric: 96\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dc14e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ETTh2, Prediction Length: 96, Number of Patches: 42\n"
     ]
    }
   ],
   "source": [
    "args.data_path = 'ETTh2.csv' # data file\n",
    "args.data = 'ETTh2'  # data\n",
    "args.pred_len = 96 # prediction sequence length\n",
    "args.num_patch = 42  # The number of input patches\n",
    "\n",
    "print(f\"Dataset: {args.data}, Prediction Length: {args.pred_len}, Number of Patches: {args.num_patch}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb74570",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99038d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "train 8209\n",
      "val 2785\n",
      "test 2785\n",
      "\titers: 100, epoch: 1 | loss: 0.3296215\n",
      "\tspeed: 0.2841s/iter; left time: 1426.3843s\n",
      "\titers: 200, epoch: 1 | loss: 0.4071481\n",
      "\tspeed: 0.2830s/iter; left time: 1392.6717s\n",
      "Epoch: 1 cost time: 72.61269783973694\n",
      "Epoch: 1, Steps: 256 | Train Loss: 0.6070960 Vali Loss: 0.3094437 Test Loss: 0.4443583\n",
      "Validation loss decreased (inf --> 0.309444).  Saving model ...\n",
      "Updating learning rate to 0.005\n",
      "\titers: 100, epoch: 2 | loss: 0.4269828\n",
      "\tspeed: 0.9280s/iter; left time: 4422.0935s\n",
      "\titers: 200, epoch: 2 | loss: 0.2938041\n",
      "\tspeed: 0.2840s/iter; left time: 1325.0090s\n",
      "Epoch: 2 cost time: 72.60372042655945\n",
      "Epoch: 2, Steps: 256 | Train Loss: 0.4624461 Vali Loss: 0.2535830 Test Loss: 0.3750189\n",
      "Validation loss decreased (0.309444 --> 0.253583).  Saving model ...\n",
      "Updating learning rate to 0.0025\n",
      "\titers: 100, epoch: 3 | loss: 0.3597878\n",
      "\tspeed: 0.9290s/iter; left time: 4188.6407s\n",
      "\titers: 200, epoch: 3 | loss: 0.2856484\n",
      "\tspeed: 0.2851s/iter; left time: 1256.9543s\n",
      "Epoch: 3 cost time: 73.49036169052124\n",
      "Epoch: 3, Steps: 256 | Train Loss: 0.3854767 Vali Loss: 0.2261989 Test Loss: 0.2988657\n",
      "Validation loss decreased (0.253583 --> 0.226199).  Saving model ...\n",
      "Updating learning rate to 0.00125\n",
      "\titers: 100, epoch: 4 | loss: 0.4565513\n",
      "\tspeed: 0.9269s/iter; left time: 3942.1907s\n",
      "\titers: 200, epoch: 4 | loss: 0.4310124\n",
      "\tspeed: 0.2870s/iter; left time: 1191.9246s\n",
      "Epoch: 4 cost time: 73.20009803771973\n",
      "Epoch: 4, Steps: 256 | Train Loss: 0.3589030 Vali Loss: 0.2187188 Test Loss: 0.2877921\n",
      "Validation loss decreased (0.226199 --> 0.218719).  Saving model ...\n",
      "Updating learning rate to 0.000625\n",
      "\titers: 100, epoch: 5 | loss: 0.2479632\n",
      "\tspeed: 0.9350s/iter; left time: 3737.0410s\n",
      "\titers: 200, epoch: 5 | loss: 0.4012131\n",
      "\tspeed: 0.2870s/iter; left time: 1118.4267s\n",
      "Epoch: 5 cost time: 73.41111969947815\n",
      "Epoch: 5, Steps: 256 | Train Loss: 0.3513556 Vali Loss: 0.2222571 Test Loss: 0.3045112\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0003125\n",
      "\titers: 100, epoch: 6 | loss: 0.2392181\n",
      "\tspeed: 0.9250s/iter; left time: 3460.6082s\n",
      "\titers: 200, epoch: 6 | loss: 0.4205960\n",
      "\tspeed: 0.2830s/iter; left time: 1030.3331s\n",
      "Epoch: 6 cost time: 72.70526480674744\n",
      "Epoch: 6, Steps: 256 | Train Loss: 0.3400513 Vali Loss: 0.2361991 Test Loss: 0.3386444\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.00015625\n",
      "\titers: 100, epoch: 7 | loss: 0.3110152\n",
      "\tspeed: 0.9281s/iter; left time: 3234.3860s\n",
      "\titers: 200, epoch: 7 | loss: 0.3018520\n",
      "\tspeed: 0.2860s/iter; left time: 968.1828s\n",
      "Epoch: 7 cost time: 72.59821343421936\n",
      "Epoch: 7, Steps: 256 | Train Loss: 0.3405048 Vali Loss: 0.2271028 Test Loss: 0.3135292\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 7.8125e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.2954697\n",
      "\tspeed: 0.9259s/iter; left time: 2989.8883s\n",
      "\titers: 200, epoch: 8 | loss: 0.2913046\n",
      "\tspeed: 0.2869s/iter; left time: 897.8394s\n",
      "Epoch: 8 cost time: 73.00780272483826\n",
      "Epoch: 8, Steps: 256 | Train Loss: 0.3379889 Vali Loss: 0.2301517 Test Loss: 0.3195598\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 3.90625e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.3711262\n",
      "\tspeed: 0.9251s/iter; left time: 2750.3365s\n",
      "\titers: 200, epoch: 9 | loss: 0.3845320\n",
      "\tspeed: 0.2790s/iter; left time: 801.6416s\n",
      "Epoch: 9 cost time: 72.60168862342834\n",
      "Epoch: 9, Steps: 256 | Train Loss: 0.3368811 Vali Loss: 0.2239497 Test Loss: 0.3053619\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.953125e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.2768954\n",
      "\tspeed: 0.9319s/iter; left time: 2531.9346s\n",
      "\titers: 200, epoch: 10 | loss: 0.2984840\n",
      "\tspeed: 0.2870s/iter; left time: 751.0810s\n",
      "Epoch: 10 cost time: 73.19835638999939\n",
      "Epoch: 10, Steps: 256 | Train Loss: 0.3363129 Vali Loss: 0.2230695 Test Loss: 0.3001829\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 9.765625e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.3846941\n",
      "\tspeed: 0.9280s/iter; left time: 2283.8097s\n",
      "\titers: 200, epoch: 11 | loss: 0.3837563\n",
      "\tspeed: 0.2901s/iter; left time: 684.8536s\n",
      "Epoch: 11 cost time: 73.70133090019226\n",
      "Epoch: 11, Steps: 256 | Train Loss: 0.3353703 Vali Loss: 0.2296498 Test Loss: 0.3212109\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 4.8828125e-06\n",
      "\titers: 100, epoch: 12 | loss: 0.2146339\n",
      "\tspeed: 0.9619s/iter; left time: 2120.9925s\n",
      "\titers: 200, epoch: 12 | loss: 0.4751050\n",
      "\tspeed: 0.2941s/iter; left time: 619.0167s\n",
      "Epoch: 12 cost time: 75.22175598144531\n",
      "Epoch: 12, Steps: 256 | Train Loss: 0.3368542 Vali Loss: 0.2369003 Test Loss: 0.3365136\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 2.44140625e-06\n",
      "\titers: 100, epoch: 13 | loss: 0.1629312\n",
      "\tspeed: 0.9661s/iter; left time: 1882.8614s\n",
      "\titers: 200, epoch: 13 | loss: 0.2573507\n",
      "\tspeed: 0.2969s/iter; left time: 548.9775s\n",
      "Epoch: 13 cost time: 75.90508198738098\n",
      "Epoch: 13, Steps: 256 | Train Loss: 0.3366917 Vali Loss: 0.2291918 Test Loss: 0.3183606\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 1.220703125e-06\n",
      "\titers: 100, epoch: 14 | loss: 0.3900023\n",
      "\tspeed: 0.9640s/iter; left time: 1632.0580s\n",
      "\titers: 200, epoch: 14 | loss: 0.3462144\n",
      "\tspeed: 0.2910s/iter; left time: 463.5529s\n",
      "Epoch: 14 cost time: 75.09565901756287\n",
      "Epoch: 14, Steps: 256 | Train Loss: 0.3374322 Vali Loss: 0.2249370 Test Loss: 0.3099304\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): PatchTST_backbone(\n",
       "    (backbone): TSTiEncoder(\n",
       "      (W_P): Linear(in_features=24, out_features=16, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (encoder): TSTEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-2): 3 x TSTEncoderLayer(\n",
       "            (self_attn): _MultiheadAttention(\n",
       "              (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (sdp_attn): _ScaledDotProductAttention(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (1): Dropout(p=0.3, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout_attn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_attn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.3, inplace=False)\n",
       "              (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "            )\n",
       "            (dropout_ffn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_ffn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Flatten_Head(\n",
       "      (flatten): Flatten(start_dim=-2, end_dim=-1)\n",
       "      (linear): Linear(in_features=640, out_features=96, bias=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf25b5c1",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c204cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 2785\n",
      "mse:0.2877921462059021, mae:0.35264715552330017, rse:0.432295024394989\n"
     ]
    }
   ],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c22dcbd",
   "metadata": {},
   "source": [
    "---\n",
    "## Trail 2: PatchTST/64, Dataset:ETTh2, Metric: 96\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fdc6ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ETTh2, Prediction Length: 96, Number of Patches: 64\n"
     ]
    }
   ],
   "source": [
    "args.pred_len = 96 # prediction sequence length\n",
    "args.num_patch = 64  # The number of input patches\n",
    "print(f\"Dataset: {args.data}, Prediction Length: {args.pred_len}, Number of Patches: {args.num_patch}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f232359b",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ce6cb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "train 8209\n",
      "val 2785\n",
      "test 2785\n",
      "\titers: 100, epoch: 1 | loss: 0.7571195\n",
      "\tspeed: 0.2093s/iter; left time: 1051.0720s\n",
      "\titers: 200, epoch: 1 | loss: 0.7486441\n",
      "\tspeed: 0.1912s/iter; left time: 940.7164s\n",
      "Epoch: 1 cost time: 50.73831534385681\n",
      "Epoch: 1, Steps: 256 | Train Loss: 0.5849259 Vali Loss: 0.2624639 Test Loss: 0.3637852\n",
      "Validation loss decreased (inf --> 0.262464).  Saving model ...\n",
      "Updating learning rate to 0.005\n",
      "\titers: 100, epoch: 2 | loss: 0.3266740\n",
      "\tspeed: 0.7429s/iter; left time: 3539.7215s\n",
      "\titers: 200, epoch: 2 | loss: 0.3405324\n",
      "\tspeed: 0.2360s/iter; left time: 1100.7197s\n",
      "Epoch: 2 cost time: 60.89007520675659\n",
      "Epoch: 2, Steps: 256 | Train Loss: 0.4569192 Vali Loss: 0.2672715 Test Loss: 0.3213788\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0025\n",
      "\titers: 100, epoch: 3 | loss: 0.5169606\n",
      "\tspeed: 0.7460s/iter; left time: 3363.7368s\n",
      "\titers: 200, epoch: 3 | loss: 0.3054450\n",
      "\tspeed: 0.2370s/iter; left time: 1045.0616s\n",
      "Epoch: 3 cost time: 61.69784593582153\n",
      "Epoch: 3, Steps: 256 | Train Loss: 0.3867771 Vali Loss: 0.2491033 Test Loss: 0.3672395\n",
      "Validation loss decreased (0.262464 --> 0.249103).  Saving model ...\n",
      "Updating learning rate to 0.00125\n",
      "\titers: 100, epoch: 4 | loss: 0.5693638\n",
      "\tspeed: 0.7270s/iter; left time: 3091.8876s\n",
      "\titers: 200, epoch: 4 | loss: 0.3643412\n",
      "\tspeed: 0.2240s/iter; left time: 930.2995s\n",
      "Epoch: 4 cost time: 57.59317231178284\n",
      "Epoch: 4, Steps: 256 | Train Loss: 0.3581145 Vali Loss: 0.2276782 Test Loss: 0.3064647\n",
      "Validation loss decreased (0.249103 --> 0.227678).  Saving model ...\n",
      "Updating learning rate to 0.000625\n",
      "\titers: 100, epoch: 5 | loss: 0.2612330\n",
      "\tspeed: 0.6880s/iter; left time: 2749.9086s\n",
      "\titers: 200, epoch: 5 | loss: 0.3542531\n",
      "\tspeed: 0.2431s/iter; left time: 947.4124s\n",
      "Epoch: 5 cost time: 61.21262502670288\n",
      "Epoch: 5, Steps: 256 | Train Loss: 0.3511000 Vali Loss: 0.2274787 Test Loss: 0.3199478\n",
      "Validation loss decreased (0.227678 --> 0.227479).  Saving model ...\n",
      "Updating learning rate to 0.0003125\n",
      "\titers: 100, epoch: 6 | loss: 0.4344776\n",
      "\tspeed: 0.8509s/iter; left time: 3183.1132s\n",
      "\titers: 200, epoch: 6 | loss: 0.3101533\n",
      "\tspeed: 0.2401s/iter; left time: 874.3400s\n",
      "Epoch: 6 cost time: 61.80933952331543\n",
      "Epoch: 6, Steps: 256 | Train Loss: 0.3415456 Vali Loss: 0.2438921 Test Loss: 0.3661481\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.00015625\n",
      "\titers: 100, epoch: 7 | loss: 0.2562142\n",
      "\tspeed: 0.8330s/iter; left time: 2902.9316s\n",
      "\titers: 200, epoch: 7 | loss: 0.3861973\n",
      "\tspeed: 0.2499s/iter; left time: 845.9536s\n",
      "Epoch: 7 cost time: 60.700159788131714\n",
      "Epoch: 7, Steps: 256 | Train Loss: 0.3384918 Vali Loss: 0.2366452 Test Loss: 0.3466850\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 7.8125e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.4072859\n",
      "\tspeed: 0.8260s/iter; left time: 2667.0713s\n",
      "\titers: 200, epoch: 8 | loss: 0.2034687\n",
      "\tspeed: 0.2450s/iter; left time: 766.5054s\n",
      "Epoch: 8 cost time: 63.11103892326355\n",
      "Epoch: 8, Steps: 256 | Train Loss: 0.3377641 Vali Loss: 0.2257073 Test Loss: 0.3202282\n",
      "Validation loss decreased (0.227479 --> 0.225707).  Saving model ...\n",
      "Updating learning rate to 3.90625e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.3779919\n",
      "\tspeed: 0.8471s/iter; left time: 2518.4139s\n",
      "\titers: 200, epoch: 9 | loss: 0.2315873\n",
      "\tspeed: 0.2429s/iter; left time: 697.9303s\n",
      "Epoch: 9 cost time: 63.31280517578125\n",
      "Epoch: 9, Steps: 256 | Train Loss: 0.3377173 Vali Loss: 0.2330324 Test Loss: 0.3361412\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.953125e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.2523192\n",
      "\tspeed: 0.8550s/iter; left time: 2323.0838s\n",
      "\titers: 200, epoch: 10 | loss: 0.3869156\n",
      "\tspeed: 0.2391s/iter; left time: 625.6216s\n",
      "Epoch: 10 cost time: 63.40383768081665\n",
      "Epoch: 10, Steps: 256 | Train Loss: 0.3377868 Vali Loss: 0.2234759 Test Loss: 0.3130279\n",
      "Validation loss decreased (0.225707 --> 0.223476).  Saving model ...\n",
      "Updating learning rate to 9.765625e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.3157076\n",
      "\tspeed: 0.8560s/iter; left time: 2106.5028s\n",
      "\titers: 200, epoch: 11 | loss: 0.3347899\n",
      "\tspeed: 0.2440s/iter; left time: 576.0911s\n",
      "Epoch: 11 cost time: 63.01055097579956\n",
      "Epoch: 11, Steps: 256 | Train Loss: 0.3375696 Vali Loss: 0.2326295 Test Loss: 0.3345037\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 4.8828125e-06\n",
      "\titers: 100, epoch: 12 | loss: 0.2090381\n",
      "\tspeed: 0.8670s/iter; left time: 1911.7654s\n",
      "\titers: 200, epoch: 12 | loss: 0.4183213\n",
      "\tspeed: 0.2420s/iter; left time: 509.3628s\n",
      "Epoch: 12 cost time: 64.10875296592712\n",
      "Epoch: 12, Steps: 256 | Train Loss: 0.3351943 Vali Loss: 0.2295717 Test Loss: 0.3286391\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 2.44140625e-06\n",
      "\titers: 100, epoch: 13 | loss: 0.2349595\n",
      "\tspeed: 0.8520s/iter; left time: 1660.4528s\n",
      "\titers: 200, epoch: 13 | loss: 0.4237216\n",
      "\tspeed: 0.2451s/iter; left time: 453.1521s\n",
      "Epoch: 13 cost time: 64.69668436050415\n",
      "Epoch: 13, Steps: 256 | Train Loss: 0.3386248 Vali Loss: 0.2354203 Test Loss: 0.3418957\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.220703125e-06\n",
      "\titers: 100, epoch: 14 | loss: 0.4802138\n",
      "\tspeed: 0.8530s/iter; left time: 1444.0826s\n",
      "\titers: 200, epoch: 14 | loss: 0.3116265\n",
      "\tspeed: 0.2500s/iter; left time: 398.2391s\n",
      "Epoch: 14 cost time: 62.80541396141052\n",
      "Epoch: 14, Steps: 256 | Train Loss: 0.3362714 Vali Loss: 0.2257186 Test Loss: 0.3183160\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 6.103515625e-07\n",
      "\titers: 100, epoch: 15 | loss: 0.2747948\n",
      "\tspeed: 0.8170s/iter; left time: 1174.0602s\n",
      "\titers: 200, epoch: 15 | loss: 0.4428446\n",
      "\tspeed: 0.2481s/iter; left time: 331.6489s\n",
      "Epoch: 15 cost time: 62.107155561447144\n",
      "Epoch: 15, Steps: 256 | Train Loss: 0.3353510 Vali Loss: 0.2327242 Test Loss: 0.3366572\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 3.0517578125e-07\n",
      "\titers: 100, epoch: 16 | loss: 0.4994203\n",
      "\tspeed: 0.8539s/iter; left time: 1008.5019s\n",
      "\titers: 200, epoch: 16 | loss: 0.2470417\n",
      "\tspeed: 0.2350s/iter; left time: 253.9910s\n",
      "Epoch: 16 cost time: 62.401429891586304\n",
      "Epoch: 16, Steps: 256 | Train Loss: 0.3350195 Vali Loss: 0.2356613 Test Loss: 0.3422912\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.52587890625e-07\n",
      "\titers: 100, epoch: 17 | loss: 0.4647145\n",
      "\tspeed: 0.8450s/iter; left time: 781.6281s\n",
      "\titers: 200, epoch: 17 | loss: 0.4632012\n",
      "\tspeed: 0.2060s/iter; left time: 169.9471s\n",
      "Epoch: 17 cost time: 58.79651641845703\n",
      "Epoch: 17, Steps: 256 | Train Loss: 0.3367128 Vali Loss: 0.2361401 Test Loss: 0.3442912\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 7.62939453125e-08\n",
      "\titers: 100, epoch: 18 | loss: 0.3244449\n",
      "\tspeed: 0.7041s/iter; left time: 471.0654s\n",
      "\titers: 200, epoch: 18 | loss: 0.2733691\n",
      "\tspeed: 0.2019s/iter; left time: 114.8940s\n",
      "Epoch: 18 cost time: 54.198493242263794\n",
      "Epoch: 18, Steps: 256 | Train Loss: 0.3368798 Vali Loss: 0.2385438 Test Loss: 0.3502435\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 3.814697265625e-08\n",
      "\titers: 100, epoch: 19 | loss: 0.2970147\n",
      "\tspeed: 0.7560s/iter; left time: 312.2094s\n",
      "\titers: 200, epoch: 19 | loss: 0.3279084\n",
      "\tspeed: 0.2071s/iter; left time: 64.8138s\n",
      "Epoch: 19 cost time: 56.61064600944519\n",
      "Epoch: 19, Steps: 256 | Train Loss: 0.3368781 Vali Loss: 0.2287241 Test Loss: 0.3263761\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 1.9073486328125e-08\n",
      "\titers: 100, epoch: 20 | loss: 0.4251346\n",
      "\tspeed: 0.7109s/iter; left time: 111.6156s\n",
      "\titers: 200, epoch: 20 | loss: 0.3473080\n",
      "\tspeed: 0.2151s/iter; left time: 12.2595s\n",
      "Epoch: 20 cost time: 55.495365858078\n",
      "Epoch: 20, Steps: 256 | Train Loss: 0.3368229 Vali Loss: 0.2393297 Test Loss: 0.3510568\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): PatchTST_backbone(\n",
       "    (backbone): TSTiEncoder(\n",
       "      (W_P): Linear(in_features=24, out_features=16, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (encoder): TSTEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-2): 3 x TSTEncoderLayer(\n",
       "            (self_attn): _MultiheadAttention(\n",
       "              (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (sdp_attn): _ScaledDotProductAttention(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (1): Dropout(p=0.3, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout_attn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_attn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.3, inplace=False)\n",
       "              (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "            )\n",
       "            (dropout_ffn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_ffn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Flatten_Head(\n",
       "      (flatten): Flatten(start_dim=-2, end_dim=-1)\n",
       "      (linear): Linear(in_features=640, out_features=96, bias=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd62296",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f64e7d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 2785\n",
      "mse:0.31302785873413086, mae:0.3729207217693329, rse:0.4508501887321472\n"
     ]
    }
   ],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfdf02e",
   "metadata": {},
   "source": [
    "---\n",
    "## Trail 3: PatchTST/42, Dataset:ETTh2,  Metric: 192\n",
    "\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0fdd3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ETTh2, Prediction Length: 192, Number of Patches: 42\n"
     ]
    }
   ],
   "source": [
    "args.pred_len = 192 # prediction sequence length\n",
    "args.num_patch = 42  # The number of input patches\n",
    "print(f\"Dataset: {args.data}, Prediction Length: {args.pred_len}, Number of Patches: {args.num_patch}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6ec6de",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fc54040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "train 8113\n",
      "val 2689\n",
      "test 2689\n",
      "\titers: 100, epoch: 1 | loss: 0.4758472\n",
      "\tspeed: 0.2090s/iter; left time: 1036.7490s\n",
      "\titers: 200, epoch: 1 | loss: 0.3944894\n",
      "\tspeed: 0.2820s/iter; left time: 1370.8789s\n",
      "Epoch: 1 cost time: 62.80650758743286\n",
      "Epoch: 1, Steps: 253 | Train Loss: 0.6672120 Vali Loss: 0.3772666 Test Loss: 0.5099685\n",
      "Validation loss decreased (inf --> 0.377267).  Saving model ...\n",
      "Updating learning rate to 0.005\n",
      "\titers: 100, epoch: 2 | loss: 0.7683532\n",
      "\tspeed: 1.1720s/iter; left time: 5517.8984s\n",
      "\titers: 200, epoch: 2 | loss: 0.4299730\n",
      "\tspeed: 0.2690s/iter; left time: 1239.7452s\n",
      "Epoch: 2 cost time: 69.69929361343384\n",
      "Epoch: 2, Steps: 253 | Train Loss: 0.5342177 Vali Loss: 0.3972457 Test Loss: 0.7384973\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0025\n",
      "\titers: 100, epoch: 3 | loss: 0.6663211\n",
      "\tspeed: 1.1249s/iter; left time: 5011.5550s\n",
      "\titers: 200, epoch: 3 | loss: 0.2616723\n",
      "\tspeed: 0.2300s/iter; left time: 1001.7356s\n",
      "Epoch: 3 cost time: 63.9035964012146\n",
      "Epoch: 3, Steps: 253 | Train Loss: 0.4664725 Vali Loss: 0.3603818 Test Loss: 0.5399617\n",
      "Validation loss decreased (0.377267 --> 0.360382).  Saving model ...\n",
      "Updating learning rate to 0.00125\n",
      "\titers: 100, epoch: 4 | loss: 0.4736502\n",
      "\tspeed: 1.1390s/iter; left time: 4785.9941s\n",
      "\titers: 200, epoch: 4 | loss: 0.7561409\n",
      "\tspeed: 0.2770s/iter; left time: 1136.2875s\n",
      "Epoch: 4 cost time: 72.49077081680298\n",
      "Epoch: 4, Steps: 253 | Train Loss: 0.4407813 Vali Loss: 0.3475325 Test Loss: 0.5483270\n",
      "Validation loss decreased (0.360382 --> 0.347532).  Saving model ...\n",
      "Updating learning rate to 0.000625\n",
      "\titers: 100, epoch: 5 | loss: 0.2614631\n",
      "\tspeed: 1.1522s/iter; left time: 4549.8452s\n",
      "\titers: 200, epoch: 5 | loss: 0.3918169\n",
      "\tspeed: 0.2329s/iter; left time: 896.3832s\n",
      "Epoch: 5 cost time: 65.40125250816345\n",
      "Epoch: 5, Steps: 253 | Train Loss: 0.4339074 Vali Loss: 0.3204657 Test Loss: 0.4466334\n",
      "Validation loss decreased (0.347532 --> 0.320466).  Saving model ...\n",
      "Updating learning rate to 0.0003125\n",
      "\titers: 100, epoch: 6 | loss: 0.4796343\n",
      "\tspeed: 1.2500s/iter; left time: 4620.1575s\n",
      "\titers: 200, epoch: 6 | loss: 0.3674616\n",
      "\tspeed: 0.2949s/iter; left time: 1060.5825s\n",
      "Epoch: 6 cost time: 71.91654944419861\n",
      "Epoch: 6, Steps: 253 | Train Loss: 0.4253696 Vali Loss: 0.3100159 Test Loss: 0.3900582\n",
      "Validation loss decreased (0.320466 --> 0.310016).  Saving model ...\n",
      "Updating learning rate to 0.00015625\n",
      "\titers: 100, epoch: 7 | loss: 0.5355472\n",
      "\tspeed: 1.2540s/iter; left time: 4317.5120s\n",
      "\titers: 200, epoch: 7 | loss: 0.4893795\n",
      "\tspeed: 0.2530s/iter; left time: 845.7493s\n",
      "Epoch: 7 cost time: 65.80068469047546\n",
      "Epoch: 7, Steps: 253 | Train Loss: 0.4207208 Vali Loss: 0.3415943 Test Loss: 0.4674753\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 7.8125e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.3830463\n",
      "\tspeed: 1.3370s/iter; left time: 4264.9045s\n",
      "\titers: 200, epoch: 8 | loss: 0.3161009\n",
      "\tspeed: 0.2230s/iter; left time: 689.0442s\n",
      "Epoch: 8 cost time: 67.40623807907104\n",
      "Epoch: 8, Steps: 253 | Train Loss: 0.4206922 Vali Loss: 0.3303315 Test Loss: 0.4352520\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.90625e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.4971078\n",
      "\tspeed: 1.2111s/iter; left time: 3557.0963s\n",
      "\titers: 200, epoch: 9 | loss: 0.4508185\n",
      "\tspeed: 0.2780s/iter; left time: 788.7261s\n",
      "Epoch: 9 cost time: 70.30171227455139\n",
      "Epoch: 9, Steps: 253 | Train Loss: 0.4175345 Vali Loss: 0.3319381 Test Loss: 0.4330526\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.953125e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.2283934\n",
      "\tspeed: 1.1410s/iter; left time: 3062.3404s\n",
      "\titers: 200, epoch: 10 | loss: 0.2820452\n",
      "\tspeed: 0.2789s/iter; left time: 720.7345s\n",
      "Epoch: 10 cost time: 63.809914112091064\n",
      "Epoch: 10, Steps: 253 | Train Loss: 0.4174546 Vali Loss: 0.3365390 Test Loss: 0.4487405\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 9.765625e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.5266115\n",
      "\tspeed: 1.2270s/iter; left time: 2982.7763s\n",
      "\titers: 200, epoch: 11 | loss: 0.4030631\n",
      "\tspeed: 0.2590s/iter; left time: 603.8220s\n",
      "Epoch: 11 cost time: 67.6043210029602\n",
      "Epoch: 11, Steps: 253 | Train Loss: 0.4153310 Vali Loss: 0.3361948 Test Loss: 0.4453853\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 4.8828125e-06\n",
      "\titers: 100, epoch: 12 | loss: 0.5016001\n",
      "\tspeed: 1.1810s/iter; left time: 2572.2525s\n",
      "\titers: 200, epoch: 12 | loss: 0.2218429\n",
      "\tspeed: 0.2690s/iter; left time: 558.9021s\n",
      "Epoch: 12 cost time: 69.5955798625946\n",
      "Epoch: 12, Steps: 253 | Train Loss: 0.4171907 Vali Loss: 0.3301324 Test Loss: 0.4317210\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 2.44140625e-06\n",
      "\titers: 100, epoch: 13 | loss: 0.3538169\n",
      "\tspeed: 1.2810s/iter; left time: 2465.8589s\n",
      "\titers: 200, epoch: 13 | loss: 0.6313733\n",
      "\tspeed: 0.2221s/iter; left time: 405.3742s\n",
      "Epoch: 13 cost time: 66.31193470954895\n",
      "Epoch: 13, Steps: 253 | Train Loss: 0.4184666 Vali Loss: 0.3296620 Test Loss: 0.4268593\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 1.220703125e-06\n",
      "\titers: 100, epoch: 14 | loss: 0.3292794\n",
      "\tspeed: 1.1820s/iter; left time: 1976.2957s\n",
      "\titers: 200, epoch: 14 | loss: 0.4091874\n",
      "\tspeed: 0.2849s/iter; left time: 447.9364s\n",
      "Epoch: 14 cost time: 69.59554505348206\n",
      "Epoch: 14, Steps: 253 | Train Loss: 0.4172707 Vali Loss: 0.3324212 Test Loss: 0.4294572\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 6.103515625e-07\n",
      "\titers: 100, epoch: 15 | loss: 0.2740059\n",
      "\tspeed: 0.8001s/iter; left time: 1135.3335s\n",
      "\titers: 200, epoch: 15 | loss: 0.5392953\n",
      "\tspeed: 0.1960s/iter; left time: 258.5077s\n",
      "Epoch: 15 cost time: 51.504960775375366\n",
      "Epoch: 15, Steps: 253 | Train Loss: 0.4162151 Vali Loss: 0.3267477 Test Loss: 0.4209029\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 3.0517578125e-07\n",
      "\titers: 100, epoch: 16 | loss: 0.4366739\n",
      "\tspeed: 1.1899s/iter; left time: 1387.4723s\n",
      "\titers: 200, epoch: 16 | loss: 0.9405074\n",
      "\tspeed: 0.2660s/iter; left time: 283.5484s\n",
      "Epoch: 16 cost time: 63.40230584144592\n",
      "Epoch: 16, Steps: 253 | Train Loss: 0.4174727 Vali Loss: 0.3278734 Test Loss: 0.4229115\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): PatchTST_backbone(\n",
       "    (backbone): TSTiEncoder(\n",
       "      (W_P): Linear(in_features=24, out_features=16, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (encoder): TSTEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-2): 3 x TSTEncoderLayer(\n",
       "            (self_attn): _MultiheadAttention(\n",
       "              (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (sdp_attn): _ScaledDotProductAttention(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (1): Dropout(p=0.3, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout_attn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_attn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.3, inplace=False)\n",
       "              (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "            )\n",
       "            (dropout_ffn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_ffn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Flatten_Head(\n",
       "      (flatten): Flatten(start_dim=-2, end_dim=-1)\n",
       "      (linear): Linear(in_features=640, out_features=192, bias=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ef693e",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0da45afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 2689\n",
      "mse:0.39005810022354126, mae:0.41103923320770264, rse:0.5008021593093872\n"
     ]
    }
   ],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72397ab6",
   "metadata": {},
   "source": [
    "---\n",
    "## Trail 4: PatchTST/64, Dataset:ETTh2,  Metric: 192\n",
    "\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93d96f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ETTh2, Prediction Length: 192, Number of Patches: 64\n"
     ]
    }
   ],
   "source": [
    "args.pred_len = 192 # prediction sequence length\n",
    "args.num_patch = 64  # The number of input patches\n",
    "print(f\"Dataset: {args.data}, Prediction Length: {args.pred_len}, Number of Patches: {args.num_patch}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a9ce1d",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fdcfe852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "train 8113\n",
      "val 2689\n",
      "test 2689\n",
      "\titers: 100, epoch: 1 | loss: 0.4370615\n",
      "\tspeed: 0.2612s/iter; left time: 1295.8034s\n",
      "\titers: 200, epoch: 1 | loss: 0.7420160\n",
      "\tspeed: 0.2620s/iter; left time: 1273.3663s\n",
      "Epoch: 1 cost time: 65.11931347846985\n",
      "Epoch: 1, Steps: 253 | Train Loss: 0.6650037 Vali Loss: 0.3667887 Test Loss: 0.4895372\n",
      "Validation loss decreased (inf --> 0.366789).  Saving model ...\n",
      "Updating learning rate to 0.005\n",
      "\titers: 100, epoch: 2 | loss: 0.3104156\n",
      "\tspeed: 1.4250s/iter; left time: 6709.0957s\n",
      "\titers: 200, epoch: 2 | loss: 0.5599427\n",
      "\tspeed: 0.1880s/iter; left time: 866.3137s\n",
      "Epoch: 2 cost time: 62.50291919708252\n",
      "Epoch: 2, Steps: 253 | Train Loss: 0.5431473 Vali Loss: 0.4427473 Test Loss: 0.8358685\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0025\n",
      "\titers: 100, epoch: 3 | loss: 0.3144198\n",
      "\tspeed: 1.3159s/iter; left time: 5862.2936s\n",
      "\titers: 200, epoch: 3 | loss: 0.2996835\n",
      "\tspeed: 0.1971s/iter; left time: 858.5016s\n",
      "Epoch: 3 cost time: 63.59732675552368\n",
      "Epoch: 3, Steps: 253 | Train Loss: 0.4629736 Vali Loss: 0.3195098 Test Loss: 0.3915819\n",
      "Validation loss decreased (0.366789 --> 0.319510).  Saving model ...\n",
      "Updating learning rate to 0.00125\n",
      "\titers: 100, epoch: 4 | loss: 0.3029438\n",
      "\tspeed: 1.3229s/iter; left time: 5558.8935s\n",
      "\titers: 200, epoch: 4 | loss: 0.5468466\n",
      "\tspeed: 0.2400s/iter; left time: 984.4415s\n",
      "Epoch: 4 cost time: 65.40526413917542\n",
      "Epoch: 4, Steps: 253 | Train Loss: 0.4361708 Vali Loss: 0.3217237 Test Loss: 0.3881734\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.000625\n",
      "\titers: 100, epoch: 5 | loss: 0.3535157\n",
      "\tspeed: 1.2000s/iter; left time: 4738.8466s\n",
      "\titers: 200, epoch: 5 | loss: 0.3457555\n",
      "\tspeed: 0.2750s/iter; left time: 1058.5352s\n",
      "Epoch: 5 cost time: 62.589592695236206\n",
      "Epoch: 5, Steps: 253 | Train Loss: 0.4249138 Vali Loss: 0.3295589 Test Loss: 0.3827205\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.0003125\n",
      "\titers: 100, epoch: 6 | loss: 0.4979486\n",
      "\tspeed: 1.1769s/iter; left time: 4349.9381s\n",
      "\titers: 200, epoch: 6 | loss: 0.5524042\n",
      "\tspeed: 0.3101s/iter; left time: 1115.1641s\n",
      "Epoch: 6 cost time: 70.31847357749939\n",
      "Epoch: 6, Steps: 253 | Train Loss: 0.4198412 Vali Loss: 0.3278906 Test Loss: 0.3793493\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 0.00015625\n",
      "\titers: 100, epoch: 7 | loss: 0.3515004\n",
      "\tspeed: 1.3800s/iter; left time: 4751.4074s\n",
      "\titers: 200, epoch: 7 | loss: 0.3989501\n",
      "\tspeed: 0.2730s/iter; left time: 912.5587s\n",
      "Epoch: 7 cost time: 68.20360136032104\n",
      "Epoch: 7, Steps: 253 | Train Loss: 0.4158463 Vali Loss: 0.3322614 Test Loss: 0.3890579\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 7.8125e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.2263457\n",
      "\tspeed: 1.3670s/iter; left time: 4360.6278s\n",
      "\titers: 200, epoch: 8 | loss: 0.6209995\n",
      "\tspeed: 0.2711s/iter; left time: 837.7093s\n",
      "Epoch: 8 cost time: 69.70366358757019\n",
      "Epoch: 8, Steps: 253 | Train Loss: 0.4127727 Vali Loss: 0.3314770 Test Loss: 0.3810773\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 3.90625e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.4471143\n",
      "\tspeed: 1.2199s/iter; left time: 3582.7126s\n",
      "\titers: 200, epoch: 9 | loss: 0.5128415\n",
      "\tspeed: 0.3040s/iter; left time: 862.4543s\n",
      "Epoch: 9 cost time: 70.90207815170288\n",
      "Epoch: 9, Steps: 253 | Train Loss: 0.4113858 Vali Loss: 0.3335677 Test Loss: 0.3868470\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.953125e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.4248308\n",
      "\tspeed: 1.1621s/iter; left time: 3119.1527s\n",
      "\titers: 200, epoch: 10 | loss: 0.2864674\n",
      "\tspeed: 0.2690s/iter; left time: 695.0049s\n",
      "Epoch: 10 cost time: 61.199177742004395\n",
      "Epoch: 10, Steps: 253 | Train Loss: 0.4112414 Vali Loss: 0.3328442 Test Loss: 0.3823808\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 9.765625e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.4383334\n",
      "\tspeed: 1.1869s/iter; left time: 2885.2830s\n",
      "\titers: 200, epoch: 11 | loss: 0.3380078\n",
      "\tspeed: 0.2611s/iter; left time: 608.5303s\n",
      "Epoch: 11 cost time: 66.09980511665344\n",
      "Epoch: 11, Steps: 253 | Train Loss: 0.4103210 Vali Loss: 0.3309666 Test Loss: 0.3791504\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.8828125e-06\n",
      "\titers: 100, epoch: 12 | loss: 0.5113398\n",
      "\tspeed: 1.2760s/iter; left time: 2779.0915s\n",
      "\titers: 200, epoch: 12 | loss: 0.4441948\n",
      "\tspeed: 0.2340s/iter; left time: 486.3492s\n",
      "Epoch: 12 cost time: 62.90712285041809\n",
      "Epoch: 12, Steps: 253 | Train Loss: 0.4108644 Vali Loss: 0.3317015 Test Loss: 0.3812371\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 2.44140625e-06\n",
      "\titers: 100, epoch: 13 | loss: 0.5142270\n",
      "\tspeed: 1.2841s/iter; left time: 2471.9096s\n",
      "\titers: 200, epoch: 13 | loss: 0.3688856\n",
      "\tspeed: 0.2868s/iter; left time: 523.4430s\n",
      "Epoch: 13 cost time: 69.90432405471802\n",
      "Epoch: 13, Steps: 253 | Train Loss: 0.4124194 Vali Loss: 0.3357237 Test Loss: 0.3901086\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): PatchTST_backbone(\n",
       "    (backbone): TSTiEncoder(\n",
       "      (W_P): Linear(in_features=24, out_features=16, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (encoder): TSTEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-2): 3 x TSTEncoderLayer(\n",
       "            (self_attn): _MultiheadAttention(\n",
       "              (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (sdp_attn): _ScaledDotProductAttention(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (1): Dropout(p=0.3, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout_attn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_attn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.3, inplace=False)\n",
       "              (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "            )\n",
       "            (dropout_ffn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_ffn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Flatten_Head(\n",
       "      (flatten): Flatten(start_dim=-2, end_dim=-1)\n",
       "      (linear): Linear(in_features=640, out_features=192, bias=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1993b853",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fcdc24b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 2689\n",
      "mse:0.391582190990448, mae:0.43561968207359314, rse:0.5017796158790588\n"
     ]
    }
   ],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4970d210",
   "metadata": {},
   "source": [
    "---\n",
    "## Trail 5: PatchTST/42, Dataset:ETTh2,  Metric: 336\n",
    "\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b342da7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ETTh2, Prediction Length: 336, Number of Patches: 42\n"
     ]
    }
   ],
   "source": [
    "args.pred_len = 336 # prediction sequence length\n",
    "args.num_patch = 42  # The number of input patches\n",
    "print(f\"Dataset: {args.data}, Prediction Length: {args.pred_len}, Number of Patches: {args.num_patch}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6523266b",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a624b017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "train 7969\n",
      "val 2545\n",
      "test 2545\n",
      "\titers: 100, epoch: 1 | loss: 0.6213180\n",
      "\tspeed: 0.2780s/iter; left time: 1356.8023s\n",
      "\titers: 200, epoch: 1 | loss: 0.6660712\n",
      "\tspeed: 0.2880s/iter; left time: 1376.8979s\n",
      "Epoch: 1 cost time: 76.89646935462952\n",
      "Epoch: 1, Steps: 249 | Train Loss: 0.7311775 Vali Loss: 0.4277453 Test Loss: 0.4471804\n",
      "Validation loss decreased (inf --> 0.427745).  Saving model ...\n",
      "Updating learning rate to 0.005\n",
      "\titers: 100, epoch: 2 | loss: 0.6087614\n",
      "\tspeed: 1.7781s/iter; left time: 8236.1108s\n",
      "\titers: 200, epoch: 2 | loss: 0.4876281\n",
      "\tspeed: 0.3339s/iter; left time: 1513.3912s\n",
      "Epoch: 2 cost time: 87.2998058795929\n",
      "Epoch: 2, Steps: 249 | Train Loss: 0.6134263 Vali Loss: 0.4421162 Test Loss: 0.4548156\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0025\n",
      "\titers: 100, epoch: 3 | loss: 0.3928181\n",
      "\tspeed: 1.8880s/iter; left time: 8275.1433s\n",
      "\titers: 200, epoch: 3 | loss: 0.5879908\n",
      "\tspeed: 0.3840s/iter; left time: 1644.5920s\n",
      "Epoch: 3 cost time: 92.79592943191528\n",
      "Epoch: 3, Steps: 249 | Train Loss: 0.5321927 Vali Loss: 0.4767314 Test Loss: 0.5865673\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.00125\n",
      "\titers: 100, epoch: 4 | loss: 0.5860187\n",
      "\tspeed: 1.5659s/iter; left time: 6473.5874s\n",
      "\titers: 200, epoch: 4 | loss: 0.3513868\n",
      "\tspeed: 0.3251s/iter; left time: 1311.5442s\n",
      "Epoch: 4 cost time: 83.40680456161499\n",
      "Epoch: 4, Steps: 249 | Train Loss: 0.5093255 Vali Loss: 0.4440480 Test Loss: 0.4234385\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 0.000625\n",
      "\titers: 100, epoch: 5 | loss: 0.7205834\n",
      "\tspeed: 1.7589s/iter; left time: 6833.2942s\n",
      "\titers: 200, epoch: 5 | loss: 0.3316282\n",
      "\tspeed: 0.3551s/iter; left time: 1344.1760s\n",
      "Epoch: 5 cost time: 84.41330552101135\n",
      "Epoch: 5, Steps: 249 | Train Loss: 0.4983996 Vali Loss: 0.4641594 Test Loss: 0.4189151\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 0.0003125\n",
      "\titers: 100, epoch: 6 | loss: 0.6595396\n",
      "\tspeed: 1.8680s/iter; left time: 6792.0830s\n",
      "\titers: 200, epoch: 6 | loss: 0.4222219\n",
      "\tspeed: 0.3819s/iter; left time: 1350.4069s\n",
      "Epoch: 6 cost time: 87.19577074050903\n",
      "Epoch: 6, Steps: 249 | Train Loss: 0.4928570 Vali Loss: 0.4453150 Test Loss: 0.4059893\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 0.00015625\n",
      "\titers: 100, epoch: 7 | loss: 0.4258448\n",
      "\tspeed: 1.7661s/iter; left time: 5981.6131s\n",
      "\titers: 200, epoch: 7 | loss: 0.5424690\n",
      "\tspeed: 0.3689s/iter; left time: 1212.6595s\n",
      "Epoch: 7 cost time: 89.80209064483643\n",
      "Epoch: 7, Steps: 249 | Train Loss: 0.4899547 Vali Loss: 0.4642012 Test Loss: 0.4215350\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 7.8125e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.5584967\n",
      "\tspeed: 1.8160s/iter; left time: 5698.7012s\n",
      "\titers: 200, epoch: 8 | loss: 0.4494835\n",
      "\tspeed: 0.3381s/iter; left time: 1027.0017s\n",
      "Epoch: 8 cost time: 84.11063766479492\n",
      "Epoch: 8, Steps: 249 | Train Loss: 0.4876213 Vali Loss: 0.4580752 Test Loss: 0.4208180\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.90625e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.3927862\n",
      "\tspeed: 1.5349s/iter; left time: 4434.2910s\n",
      "\titers: 200, epoch: 9 | loss: 0.6543953\n",
      "\tspeed: 0.4531s/iter; left time: 1263.7983s\n",
      "Epoch: 9 cost time: 109.80623316764832\n",
      "Epoch: 9, Steps: 249 | Train Loss: 0.4863201 Vali Loss: 0.4537485 Test Loss: 0.4102834\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.953125e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.5181259\n",
      "\tspeed: 2.4480s/iter; left time: 6462.7663s\n",
      "\titers: 200, epoch: 10 | loss: 0.5636878\n",
      "\tspeed: 0.4501s/iter; left time: 1143.1344s\n",
      "Epoch: 10 cost time: 104.696617603302\n",
      "Epoch: 10, Steps: 249 | Train Loss: 0.4878165 Vali Loss: 0.4545038 Test Loss: 0.4157952\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 9.765625e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.5345508\n",
      "\tspeed: 2.4239s/iter; left time: 5795.6240s\n",
      "\titers: 200, epoch: 11 | loss: 0.5895880\n",
      "\tspeed: 0.4329s/iter; left time: 991.7713s\n",
      "Epoch: 11 cost time: 110.79758524894714\n",
      "Epoch: 11, Steps: 249 | Train Loss: 0.4856338 Vali Loss: 0.4578351 Test Loss: 0.4187922\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): PatchTST_backbone(\n",
       "    (backbone): TSTiEncoder(\n",
       "      (W_P): Linear(in_features=24, out_features=16, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (encoder): TSTEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-2): 3 x TSTEncoderLayer(\n",
       "            (self_attn): _MultiheadAttention(\n",
       "              (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (sdp_attn): _ScaledDotProductAttention(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (1): Dropout(p=0.3, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout_attn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_attn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.3, inplace=False)\n",
       "              (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "            )\n",
       "            (dropout_ffn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_ffn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Flatten_Head(\n",
       "      (flatten): Flatten(start_dim=-2, end_dim=-1)\n",
       "      (linear): Linear(in_features=640, out_features=336, bias=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d19012",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b1c2450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 2545\n",
      "mse:0.4471801817417145, mae:0.46865761280059814, rse:0.534436821937561\n"
     ]
    }
   ],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4121b7d",
   "metadata": {},
   "source": [
    "---\n",
    "## Trail 6: PatchTST/64, Dataset:ETTh2,  Metric: 336\n",
    "\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd3e54e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ETTh2, Prediction Length: 336, Number of Patches: 64\n"
     ]
    }
   ],
   "source": [
    "args.pred_len = 336 # prediction sequence length\n",
    "args.num_patch = 64  # The number of input patches\n",
    "print(f\"Dataset: {args.data}, Prediction Length: {args.pred_len}, Number of Patches: {args.num_patch}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ddb549",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "77f4e805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "train 7969\n",
      "val 2545\n",
      "test 2545\n",
      "\titers: 100, epoch: 1 | loss: 0.8477201\n",
      "\tspeed: 0.2082s/iter; left time: 1016.4142s\n",
      "\titers: 200, epoch: 1 | loss: 0.7340878\n",
      "\tspeed: 0.2189s/iter; left time: 1046.6976s\n",
      "Epoch: 1 cost time: 53.93207311630249\n",
      "Epoch: 1, Steps: 249 | Train Loss: 0.7225486 Vali Loss: 0.4625877 Test Loss: 0.5699974\n",
      "Validation loss decreased (inf --> 0.462588).  Saving model ...\n",
      "Updating learning rate to 0.005\n",
      "\titers: 100, epoch: 2 | loss: 0.4026355\n",
      "\tspeed: 0.8470s/iter; left time: 3923.3568s\n",
      "\titers: 200, epoch: 2 | loss: 0.5932161\n",
      "\tspeed: 0.2441s/iter; left time: 1106.1355s\n",
      "Epoch: 2 cost time: 58.89114546775818\n",
      "Epoch: 2, Steps: 249 | Train Loss: 0.5968887 Vali Loss: 0.4708426 Test Loss: 0.6477016\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0025\n",
      "\titers: 100, epoch: 3 | loss: 0.5625603\n",
      "\tspeed: 0.7970s/iter; left time: 3493.3154s\n",
      "\titers: 200, epoch: 3 | loss: 0.4461977\n",
      "\tspeed: 0.2389s/iter; left time: 1023.2267s\n",
      "Epoch: 3 cost time: 57.300293922424316\n",
      "Epoch: 3, Steps: 249 | Train Loss: 0.5310333 Vali Loss: 0.4434173 Test Loss: 0.5775725\n",
      "Validation loss decreased (0.462588 --> 0.443417).  Saving model ...\n",
      "Updating learning rate to 0.00125\n",
      "\titers: 100, epoch: 4 | loss: 0.2822664\n",
      "\tspeed: 0.8930s/iter; left time: 3691.7815s\n",
      "\titers: 200, epoch: 4 | loss: 0.6494681\n",
      "\tspeed: 0.2160s/iter; left time: 871.4430s\n",
      "Epoch: 4 cost time: 55.5959849357605\n",
      "Epoch: 4, Steps: 249 | Train Loss: 0.5069595 Vali Loss: 0.4589537 Test Loss: 0.4792098\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.000625\n",
      "\titers: 100, epoch: 5 | loss: 0.3546396\n",
      "\tspeed: 0.7520s/iter; left time: 2921.5229s\n",
      "\titers: 200, epoch: 5 | loss: 0.4212351\n",
      "\tspeed: 0.2089s/iter; left time: 790.8336s\n",
      "Epoch: 5 cost time: 56.50099039077759\n",
      "Epoch: 5, Steps: 249 | Train Loss: 0.4949580 Vali Loss: 0.5241935 Test Loss: 0.7447089\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.0003125\n",
      "\titers: 100, epoch: 6 | loss: 0.5247225\n",
      "\tspeed: 0.7510s/iter; left time: 2730.7537s\n",
      "\titers: 200, epoch: 6 | loss: 0.5372933\n",
      "\tspeed: 0.2540s/iter; left time: 898.1359s\n",
      "Epoch: 6 cost time: 63.60086536407471\n",
      "Epoch: 6, Steps: 249 | Train Loss: 0.4878134 Vali Loss: 0.4972466 Test Loss: 0.6303326\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 0.00015625\n",
      "\titers: 100, epoch: 7 | loss: 0.6436097\n",
      "\tspeed: 0.8490s/iter; left time: 2875.4613s\n",
      "\titers: 200, epoch: 7 | loss: 0.3242206\n",
      "\tspeed: 0.2180s/iter; left time: 716.5569s\n",
      "Epoch: 7 cost time: 53.80381894111633\n",
      "Epoch: 7, Steps: 249 | Train Loss: 0.4834907 Vali Loss: 0.4923919 Test Loss: 0.5640727\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 7.8125e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.3818543\n",
      "\tspeed: 0.7261s/iter; left time: 2278.4852s\n",
      "\titers: 200, epoch: 8 | loss: 0.6107951\n",
      "\tspeed: 0.2400s/iter; left time: 729.2002s\n",
      "Epoch: 8 cost time: 54.80855846405029\n",
      "Epoch: 8, Steps: 249 | Train Loss: 0.4815683 Vali Loss: 0.4873075 Test Loss: 0.5475685\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 3.90625e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.5088013\n",
      "\tspeed: 0.7100s/iter; left time: 2051.3315s\n",
      "\titers: 200, epoch: 9 | loss: 0.4266061\n",
      "\tspeed: 0.2330s/iter; left time: 649.7047s\n",
      "Epoch: 9 cost time: 54.805455684661865\n",
      "Epoch: 9, Steps: 249 | Train Loss: 0.4803424 Vali Loss: 0.4842651 Test Loss: 0.5477665\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.953125e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.4234610\n",
      "\tspeed: 0.7530s/iter; left time: 1987.8585s\n",
      "\titers: 200, epoch: 10 | loss: 0.3700374\n",
      "\tspeed: 0.2209s/iter; left time: 561.1946s\n",
      "Epoch: 10 cost time: 52.606205463409424\n",
      "Epoch: 10, Steps: 249 | Train Loss: 0.4811767 Vali Loss: 0.4977595 Test Loss: 0.5656150\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 9.765625e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.5290050\n",
      "\tspeed: 0.6510s/iter; left time: 1556.5248s\n",
      "\titers: 200, epoch: 11 | loss: 0.5986846\n",
      "\tspeed: 0.2050s/iter; left time: 469.6858s\n",
      "Epoch: 11 cost time: 52.501198053359985\n",
      "Epoch: 11, Steps: 249 | Train Loss: 0.4798545 Vali Loss: 0.4795593 Test Loss: 0.5492391\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.8828125e-06\n",
      "\titers: 100, epoch: 12 | loss: 0.3068410\n",
      "\tspeed: 0.6960s/iter; left time: 1490.8758s\n",
      "\titers: 200, epoch: 12 | loss: 0.5148948\n",
      "\tspeed: 0.1850s/iter; left time: 377.8492s\n",
      "Epoch: 12 cost time: 50.499244928359985\n",
      "Epoch: 12, Steps: 249 | Train Loss: 0.4803868 Vali Loss: 0.4890951 Test Loss: 0.5724857\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 2.44140625e-06\n",
      "\titers: 100, epoch: 13 | loss: 0.3476858\n",
      "\tspeed: 0.7069s/iter; left time: 1338.2521s\n",
      "\titers: 200, epoch: 13 | loss: 0.4682856\n",
      "\tspeed: 0.2040s/iter; left time: 365.7381s\n",
      "Epoch: 13 cost time: 54.49892735481262\n",
      "Epoch: 13, Steps: 249 | Train Loss: 0.4785207 Vali Loss: 0.4881713 Test Loss: 0.5603089\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): PatchTST_backbone(\n",
       "    (backbone): TSTiEncoder(\n",
       "      (W_P): Linear(in_features=24, out_features=16, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (encoder): TSTEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-2): 3 x TSTEncoderLayer(\n",
       "            (self_attn): _MultiheadAttention(\n",
       "              (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (sdp_attn): _ScaledDotProductAttention(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (1): Dropout(p=0.3, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout_attn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_attn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.3, inplace=False)\n",
       "              (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "            )\n",
       "            (dropout_ffn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_ffn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Flatten_Head(\n",
       "      (flatten): Flatten(start_dim=-2, end_dim=-1)\n",
       "      (linear): Linear(in_features=640, out_features=336, bias=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e06168b",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52cf7cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 2545\n",
      "mse:0.5775724649429321, mae:0.5237013101577759, rse:0.6073770523071289\n"
     ]
    }
   ],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f14843",
   "metadata": {},
   "source": [
    "---\n",
    "## Trail 7: PatchTST/42, Dataset:ETTh2,  Metric: 720\n",
    "\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3bd55aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ETTh2, Prediction Length: 720, Number of Patches: 42\n"
     ]
    }
   ],
   "source": [
    "args.pred_len = 720 # prediction sequence length\n",
    "args.num_patch = 42  # The number of input patches\n",
    "print(f\"Dataset: {args.data}, Prediction Length: {args.pred_len}, Number of Patches: {args.num_patch}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5662cfc0",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49e4773f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "train 7585\n",
      "val 2161\n",
      "test 2161\n",
      "\titers: 100, epoch: 1 | loss: 0.4212859\n",
      "\tspeed: 0.2342s/iter; left time: 1087.0385s\n",
      "\titers: 200, epoch: 1 | loss: 0.7312227\n",
      "\tspeed: 0.2759s/iter; left time: 1253.0478s\n",
      "Epoch: 1 cost time: 59.01574182510376\n",
      "Epoch: 1, Steps: 237 | Train Loss: 0.8387095 Vali Loss: 0.6917935 Test Loss: 0.7220803\n",
      "Validation loss decreased (inf --> 0.691794).  Saving model ...\n",
      "Updating learning rate to 0.005\n",
      "\titers: 100, epoch: 2 | loss: 0.4868017\n",
      "\tspeed: 1.2760s/iter; left time: 5619.3527s\n",
      "\titers: 200, epoch: 2 | loss: 0.7825486\n",
      "\tspeed: 0.2471s/iter; left time: 1063.4961s\n",
      "Epoch: 2 cost time: 61.913822650909424\n",
      "Epoch: 2, Steps: 237 | Train Loss: 0.7230844 Vali Loss: 0.8397724 Test Loss: 0.7207495\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0025\n",
      "\titers: 100, epoch: 3 | loss: 0.5912184\n",
      "\tspeed: 1.3189s/iter; left time: 5495.8989s\n",
      "\titers: 200, epoch: 3 | loss: 0.6378720\n",
      "\tspeed: 0.2770s/iter; left time: 1126.4691s\n",
      "Epoch: 3 cost time: 67.50114393234253\n",
      "Epoch: 3, Steps: 237 | Train Loss: 0.6446894 Vali Loss: 0.8496073 Test Loss: 0.6561267\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.00125\n",
      "\titers: 100, epoch: 4 | loss: 0.7276422\n",
      "\tspeed: 1.3720s/iter; left time: 5391.9098s\n",
      "\titers: 200, epoch: 4 | loss: 0.5485831\n",
      "\tspeed: 0.2551s/iter; left time: 976.8996s\n",
      "Epoch: 4 cost time: 67.79609251022339\n",
      "Epoch: 4, Steps: 237 | Train Loss: 0.6196515 Vali Loss: 0.7629464 Test Loss: 0.4894736\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 0.000625\n",
      "\titers: 100, epoch: 5 | loss: 1.0335665\n",
      "\tspeed: 1.4099s/iter; left time: 5206.8874s\n",
      "\titers: 200, epoch: 5 | loss: 0.5569574\n",
      "\tspeed: 0.2990s/iter; left time: 1074.3914s\n",
      "Epoch: 5 cost time: 68.19961547851562\n",
      "Epoch: 5, Steps: 237 | Train Loss: 0.6080406 Vali Loss: 0.8102023 Test Loss: 0.4943808\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 0.0003125\n",
      "\titers: 100, epoch: 6 | loss: 0.4452570\n",
      "\tspeed: 1.3550s/iter; left time: 4682.9299s\n",
      "\titers: 200, epoch: 6 | loss: 0.7171767\n",
      "\tspeed: 0.3040s/iter; left time: 1020.0857s\n",
      "Epoch: 6 cost time: 68.20058703422546\n",
      "Epoch: 6, Steps: 237 | Train Loss: 0.5999824 Vali Loss: 0.8299327 Test Loss: 0.5053463\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 0.00015625\n",
      "\titers: 100, epoch: 7 | loss: 0.5437686\n",
      "\tspeed: 1.3710s/iter; left time: 4413.3213s\n",
      "\titers: 200, epoch: 7 | loss: 0.8238463\n",
      "\tspeed: 0.3500s/iter; left time: 1091.7333s\n",
      "Epoch: 7 cost time: 81.71875882148743\n",
      "Epoch: 7, Steps: 237 | Train Loss: 0.5980552 Vali Loss: 0.8138361 Test Loss: 0.4918016\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 7.8125e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.4969432\n",
      "\tspeed: 1.2811s/iter; left time: 3820.1822s\n",
      "\titers: 200, epoch: 8 | loss: 0.7006136\n",
      "\tspeed: 0.2409s/iter; left time: 694.2393s\n",
      "Epoch: 8 cost time: 61.50828313827515\n",
      "Epoch: 8, Steps: 237 | Train Loss: 0.5966539 Vali Loss: 0.8427523 Test Loss: 0.5217299\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.90625e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.5272736\n",
      "\tspeed: 1.4510s/iter; left time: 3983.0532s\n",
      "\titers: 200, epoch: 9 | loss: 0.4719782\n",
      "\tspeed: 0.2610s/iter; left time: 690.2171s\n",
      "Epoch: 9 cost time: 62.20385003089905\n",
      "Epoch: 9, Steps: 237 | Train Loss: 0.5956047 Vali Loss: 0.8292798 Test Loss: 0.4957591\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.953125e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.4313636\n",
      "\tspeed: 1.3170s/iter; left time: 3303.1612s\n",
      "\titers: 200, epoch: 10 | loss: 0.7136452\n",
      "\tspeed: 0.3030s/iter; left time: 729.6107s\n",
      "Epoch: 10 cost time: 70.40342998504639\n",
      "Epoch: 10, Steps: 237 | Train Loss: 0.5948141 Vali Loss: 0.8335617 Test Loss: 0.4964277\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 9.765625e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.6349176\n",
      "\tspeed: 1.2661s/iter; left time: 2875.3146s\n",
      "\titers: 200, epoch: 11 | loss: 0.6217961\n",
      "\tspeed: 0.3019s/iter; left time: 655.4793s\n",
      "Epoch: 11 cost time: 67.10391736030579\n",
      "Epoch: 11, Steps: 237 | Train Loss: 0.5945548 Vali Loss: 0.8412584 Test Loss: 0.4999952\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): PatchTST_backbone(\n",
       "    (backbone): TSTiEncoder(\n",
       "      (W_P): Linear(in_features=24, out_features=16, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (encoder): TSTEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-2): 3 x TSTEncoderLayer(\n",
       "            (self_attn): _MultiheadAttention(\n",
       "              (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (sdp_attn): _ScaledDotProductAttention(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (1): Dropout(p=0.3, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout_attn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_attn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.3, inplace=False)\n",
       "              (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "            )\n",
       "            (dropout_ffn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_ffn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Flatten_Head(\n",
       "      (flatten): Flatten(start_dim=-2, end_dim=-1)\n",
       "      (linear): Linear(in_features=640, out_features=720, bias=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff73ef",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a8a63c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 2161\n",
      "mse:0.7220802307128906, mae:0.6122045516967773, rse:0.6793708205223083\n"
     ]
    }
   ],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5422ceca",
   "metadata": {},
   "source": [
    "---\n",
    "## Trail 8: PatchTST/64, Dataset:ETTh2,  Metric: 720\n",
    "\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e643d669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ETTh1, Prediction Length: 720, Number of Patches: 64\n"
     ]
    }
   ],
   "source": [
    "args.pred_len = 720 # prediction sequence length\n",
    "args.num_patch = 64  # The number of input patches\n",
    "print(f\"Dataset: {args.data}, Prediction Length: {args.pred_len}, Number of Patches: {args.num_patch}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33122e4",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ecc593f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "train 7585\n",
      "val 2161\n",
      "test 2161\n",
      "\titers: 100, epoch: 1 | loss: 0.6905423\n",
      "\tspeed: 0.2913s/iter; left time: 1351.8219s\n",
      "\titers: 200, epoch: 1 | loss: 0.5533327\n",
      "\tspeed: 0.2929s/iter; left time: 1330.0202s\n",
      "Epoch: 1 cost time: 68.22403383255005\n",
      "Epoch: 1, Steps: 237 | Train Loss: 0.6730704 Vali Loss: 1.2304925 Test Loss: 0.5032592\n",
      "Validation loss decreased (inf --> 1.230492).  Saving model ...\n",
      "Updating learning rate to 0.005\n",
      "\titers: 100, epoch: 2 | loss: 0.5409788\n",
      "\tspeed: 1.3981s/iter; left time: 6157.3664s\n",
      "\titers: 200, epoch: 2 | loss: 0.5539945\n",
      "\tspeed: 0.2920s/iter; left time: 1256.7084s\n",
      "Epoch: 2 cost time: 71.20020580291748\n",
      "Epoch: 2, Steps: 237 | Train Loss: 0.5450662 Vali Loss: 1.4113623 Test Loss: 0.5585139\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0025\n",
      "\titers: 100, epoch: 3 | loss: 0.4570258\n",
      "\tspeed: 1.4370s/iter; left time: 5987.9974s\n",
      "\titers: 200, epoch: 3 | loss: 0.5009385\n",
      "\tspeed: 0.2660s/iter; left time: 1081.8061s\n",
      "Epoch: 3 cost time: 64.59979104995728\n",
      "Epoch: 3, Steps: 237 | Train Loss: 0.4937776 Vali Loss: 1.4467335 Test Loss: 0.6097014\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.00125\n",
      "\titers: 100, epoch: 4 | loss: 0.4509003\n",
      "\tspeed: 1.2679s/iter; left time: 4982.8925s\n",
      "\titers: 200, epoch: 4 | loss: 0.4700914\n",
      "\tspeed: 0.2711s/iter; left time: 1038.1986s\n",
      "Epoch: 4 cost time: 66.90499353408813\n",
      "Epoch: 4, Steps: 237 | Train Loss: 0.4785014 Vali Loss: 1.5223914 Test Loss: 0.6064851\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 0.000625\n",
      "\titers: 100, epoch: 5 | loss: 0.4365411\n",
      "\tspeed: 1.3730s/iter; left time: 5070.5484s\n",
      "\titers: 200, epoch: 5 | loss: 0.5130857\n",
      "\tspeed: 0.2609s/iter; left time: 937.3947s\n",
      "Epoch: 5 cost time: 66.70561408996582\n",
      "Epoch: 5, Steps: 237 | Train Loss: 0.4697364 Vali Loss: 1.4275156 Test Loss: 0.6460082\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 0.0003125\n",
      "\titers: 100, epoch: 6 | loss: 0.4670827\n",
      "\tspeed: 1.4210s/iter; left time: 4911.0371s\n",
      "\titers: 200, epoch: 6 | loss: 0.4836484\n",
      "\tspeed: 0.2810s/iter; left time: 943.0098s\n",
      "Epoch: 6 cost time: 68.71061897277832\n",
      "Epoch: 6, Steps: 237 | Train Loss: 0.4644142 Vali Loss: 1.4177852 Test Loss: 0.6208947\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 0.00015625\n",
      "\titers: 100, epoch: 7 | loss: 0.4336584\n",
      "\tspeed: 1.3521s/iter; left time: 4352.3635s\n",
      "\titers: 200, epoch: 7 | loss: 0.4530790\n",
      "\tspeed: 0.2909s/iter; left time: 907.2370s\n",
      "Epoch: 7 cost time: 68.8023669719696\n",
      "Epoch: 7, Steps: 237 | Train Loss: 0.4613989 Vali Loss: 1.3991312 Test Loss: 0.6283020\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 7.8125e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.4532300\n",
      "\tspeed: 1.4391s/iter; left time: 4291.4611s\n",
      "\titers: 200, epoch: 8 | loss: 0.4427257\n",
      "\tspeed: 0.2580s/iter; left time: 743.5884s\n",
      "Epoch: 8 cost time: 62.60773181915283\n",
      "Epoch: 8, Steps: 237 | Train Loss: 0.4601259 Vali Loss: 1.3764558 Test Loss: 0.6347129\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.90625e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.4142733\n",
      "\tspeed: 1.3440s/iter; left time: 3689.3934s\n",
      "\titers: 200, epoch: 9 | loss: 0.4334662\n",
      "\tspeed: 0.2730s/iter; left time: 721.9836s\n",
      "Epoch: 9 cost time: 71.20225763320923\n",
      "Epoch: 9, Steps: 237 | Train Loss: 0.4596192 Vali Loss: 1.4091986 Test Loss: 0.6379790\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.953125e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.4845583\n",
      "\tspeed: 1.3940s/iter; left time: 3496.1457s\n",
      "\titers: 200, epoch: 10 | loss: 0.4292827\n",
      "\tspeed: 0.2750s/iter; left time: 662.1408s\n",
      "Epoch: 10 cost time: 66.00228714942932\n",
      "Epoch: 10, Steps: 237 | Train Loss: 0.4584634 Vali Loss: 1.3883356 Test Loss: 0.6312962\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 9.765625e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.4658892\n",
      "\tspeed: 1.4129s/iter; left time: 3208.7287s\n",
      "\titers: 200, epoch: 11 | loss: 0.5584514\n",
      "\tspeed: 0.2530s/iter; left time: 549.3506s\n",
      "Epoch: 11 cost time: 64.89876532554626\n",
      "Epoch: 11, Steps: 237 | Train Loss: 0.4579611 Vali Loss: 1.3934808 Test Loss: 0.6301140\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): PatchTST_backbone(\n",
       "    (backbone): TSTiEncoder(\n",
       "      (W_P): Linear(in_features=24, out_features=16, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (encoder): TSTEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-2): 3 x TSTEncoderLayer(\n",
       "            (self_attn): _MultiheadAttention(\n",
       "              (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "              (sdp_attn): _ScaledDotProductAttention(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "                (1): Dropout(p=0.3, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout_attn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_attn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.3, inplace=False)\n",
       "              (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "            )\n",
       "            (dropout_ffn): Dropout(p=0.3, inplace=False)\n",
       "            (norm_ffn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Flatten_Head(\n",
       "      (flatten): Flatten(start_dim=-2, end_dim=-1)\n",
       "      (linear): Linear(in_features=640, out_features=720, bias=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea7b517",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b6125dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 2161\n",
      "mse:0.5032591223716736, mae:0.5134444236755371, rse:0.6794742345809937\n"
     ]
    }
   ],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5701ab18",
   "metadata": {},
   "source": [
    "### Compare our results with paper results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7240f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=r\"./Images/ETT1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686fbf37",
   "metadata": {},
   "source": [
    "#### **Experiment Results**:\n",
    "Comaparing my results with the paper resulted highlited in the image above.\n",
    "\n",
    "| PatchTST/42 | Dataset | Seq_len | MSE | MAE |\n",
    "|---|---|---|---|---|\n",
    "|  | ETTh2 | 96 |  0.37019482254981995| 0.3915349543094635  |\n",
    "|  | ETTh2 | 192 |  0.40499380230903625 | 0.4138428270816803 |\n",
    "|  | ETTh2 | 336 | 0.4337225556373596 | 0.434622198343277  |\n",
    "|  | ETTh2 | 720 | 0.470274418592453| 0.4867483675479889 |\n",
    "\n",
    "\n",
    "| PatchTST/64 | Dataset | Seq_len | MSE | MAE |\n",
    "|---|---|---|---|---|\n",
    "|  | ETTh2 | 96 |  0.37019482254981995| 0.3915349543094635  |\n",
    "|  | ETTh2 | 192 |  0.40499380230903625 | 0.4138428270816803 |\n",
    "|  | ETTh2 | 336 | 0.4337225556373596 | 0.434622198343277  |\n",
    "|  | ETTh2 | 720 | 0.470274418592453| 0.4867483675479889 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66217f72",
   "metadata": {},
   "source": [
    "---\n",
    "# Working on ETTm1 Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d2dcdf",
   "metadata": {},
   "source": [
    "## Trail 1: PatchTST/42, Dataset:ETTm1,  Metric: 96\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "60870406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ETTm1, Prediction Length : 96\n"
     ]
    }
   ],
   "source": [
    "args.data_path = 'ETTm1.csv' # data file\n",
    "args.data = 'ETTm1'  # data\n",
    "args.pred_len = 96 # prediction sequence length\n",
    "args.n_heads = 16 \n",
    "args.d_model = 128 \n",
    "args.d_ff = 256 \n",
    "args.dropout = 0.2\n",
    "args.fc_dropout = 0.2\n",
    "args.patience = 10#20\n",
    "args.pct_start = 0.4\n",
    "args.patch_len = 16#60\n",
    "args.num_patch = 42  # The number of input patches\n",
    "args.lradj = 'TST'\n",
    "\n",
    "print(f\"Dataset: {args.data}, Prediction Length : {args.pred_len}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd52b279",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5cca472b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "train 34129\n",
      "val 11425\n",
      "test 11425\n",
      "\titers: 100, epoch: 1 | loss: 0.3383717\n",
      "\tspeed: 0.1908s/iter; left time: 4049.1192s\n",
      "\titers: 200, epoch: 1 | loss: 0.3019485\n",
      "\tspeed: 0.1881s/iter; left time: 3973.4323s\n",
      "\titers: 300, epoch: 1 | loss: 0.4252478\n",
      "\tspeed: 0.1949s/iter; left time: 4096.2786s\n",
      "\titers: 400, epoch: 1 | loss: 0.2874214\n",
      "\tspeed: 0.1759s/iter; left time: 3680.8659s\n",
      "\titers: 500, epoch: 1 | loss: 0.3519585\n",
      "\tspeed: 0.1970s/iter; left time: 4102.5502s\n",
      "\titers: 600, epoch: 1 | loss: 0.2871589\n",
      "\tspeed: 0.2020s/iter; left time: 4186.2210s\n",
      "\titers: 700, epoch: 1 | loss: 0.3036229\n",
      "\tspeed: 0.2010s/iter; left time: 4144.4639s\n",
      "\titers: 800, epoch: 1 | loss: 0.2620423\n",
      "\tspeed: 0.1839s/iter; left time: 3774.6934s\n",
      "\titers: 900, epoch: 1 | loss: 0.3427806\n",
      "\tspeed: 0.1971s/iter; left time: 4025.0566s\n",
      "\titers: 1000, epoch: 1 | loss: 0.3516700\n",
      "\tspeed: 0.1899s/iter; left time: 3859.4710s\n",
      "Epoch: 1 cost time: 205.17846369743347\n",
      "Epoch: 1, Steps: 1066 | Train Loss: 0.3263496 Vali Loss: 0.4278865 Test Loss: 0.3456663\n",
      "Validation loss decreased (inf --> 0.427886).  Saving model ...\n",
      "Updating learning rate to 0.000521637289413384\n",
      "\titers: 100, epoch: 2 | loss: 0.2507444\n",
      "\tspeed: 1.8460s/iter; left time: 37206.6744s\n",
      "\titers: 200, epoch: 2 | loss: 0.2943071\n",
      "\tspeed: 0.2111s/iter; left time: 4232.7054s\n",
      "\titers: 300, epoch: 2 | loss: 0.2381828\n",
      "\tspeed: 0.1969s/iter; left time: 3929.6214s\n",
      "\titers: 400, epoch: 2 | loss: 0.2902126\n",
      "\tspeed: 0.1980s/iter; left time: 3930.6837s\n",
      "\titers: 500, epoch: 2 | loss: 0.2599333\n",
      "\tspeed: 0.2190s/iter; left time: 4326.4575s\n",
      "\titers: 600, epoch: 2 | loss: 0.2532069\n",
      "\tspeed: 0.2060s/iter; left time: 4049.9093s\n",
      "\titers: 700, epoch: 2 | loss: 0.2515688\n",
      "\tspeed: 0.2051s/iter; left time: 4010.5417s\n",
      "\titers: 800, epoch: 2 | loss: 0.2356909\n",
      "\tspeed: 0.2210s/iter; left time: 4299.2535s\n",
      "\titers: 900, epoch: 2 | loss: 0.2636993\n",
      "\tspeed: 0.2040s/iter; left time: 3948.8351s\n",
      "\titers: 1000, epoch: 2 | loss: 0.2302471\n",
      "\tspeed: 0.2139s/iter; left time: 4118.7491s\n",
      "Epoch: 2 cost time: 221.88331246376038\n",
      "Epoch: 2, Steps: 1066 | Train Loss: 0.2911107 Vali Loss: 0.4467405 Test Loss: 0.3747282\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Updating learning rate to 0.0014003403693692118\n",
      "\titers: 100, epoch: 3 | loss: 0.2494737\n",
      "\tspeed: 1.9200s/iter; left time: 36650.8278s\n",
      "\titers: 200, epoch: 3 | loss: 0.3386373\n",
      "\tspeed: 0.2060s/iter; left time: 3911.6967s\n",
      "\titers: 300, epoch: 3 | loss: 0.2976762\n",
      "\tspeed: 0.2130s/iter; left time: 4023.3492s\n",
      "\titers: 400, epoch: 3 | loss: 0.2366522\n",
      "\tspeed: 0.2179s/iter; left time: 4095.0353s\n",
      "\titers: 500, epoch: 3 | loss: 0.2847727\n",
      "\tspeed: 0.2000s/iter; left time: 3738.6167s\n",
      "\titers: 600, epoch: 3 | loss: 0.3485212\n",
      "\tspeed: 0.1970s/iter; left time: 3661.4713s\n",
      "\titers: 700, epoch: 3 | loss: 0.2268675\n",
      "\tspeed: 0.1900s/iter; left time: 3513.1634s\n",
      "\titers: 800, epoch: 3 | loss: 0.2496476\n",
      "\tspeed: 0.1861s/iter; left time: 3422.3971s\n",
      "\titers: 900, epoch: 3 | loss: 0.2740805\n",
      "\tspeed: 0.2139s/iter; left time: 3911.9145s\n",
      "\titers: 1000, epoch: 3 | loss: 0.2455821\n",
      "\tspeed: 0.2400s/iter; left time: 4365.8852s\n",
      "Epoch: 3 cost time: 223.2009675502777\n",
      "Epoch: 3, Steps: 1066 | Train Loss: 0.2705445 Vali Loss: 0.5087384 Test Loss: 0.3762128\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Updating learning rate to 0.0026005895091706643\n",
      "\titers: 100, epoch: 4 | loss: 0.2751341\n",
      "\tspeed: 2.0110s/iter; left time: 36243.4057s\n",
      "\titers: 200, epoch: 4 | loss: 0.2715366\n",
      "\tspeed: 0.1990s/iter; left time: 3567.4259s\n",
      "\titers: 300, epoch: 4 | loss: 0.2776595\n",
      "\tspeed: 0.2169s/iter; left time: 3866.4461s\n",
      "\titers: 400, epoch: 4 | loss: 0.2676955\n",
      "\tspeed: 0.2421s/iter; left time: 4290.9212s\n",
      "\titers: 500, epoch: 4 | loss: 0.2401305\n",
      "\tspeed: 0.2199s/iter; left time: 3875.2891s\n",
      "\titers: 600, epoch: 4 | loss: 0.2857484\n",
      "\tspeed: 0.2130s/iter; left time: 3732.6350s\n",
      "\titers: 700, epoch: 4 | loss: 0.3015832\n",
      "\tspeed: 0.2021s/iter; left time: 3521.7810s\n",
      "\titers: 800, epoch: 4 | loss: 0.3039461\n",
      "\tspeed: 0.2231s/iter; left time: 3864.4011s\n",
      "\titers: 900, epoch: 4 | loss: 0.3033838\n",
      "\tspeed: 0.1978s/iter; left time: 3407.2774s\n",
      "\titers: 1000, epoch: 4 | loss: 0.2851889\n",
      "\tspeed: 0.1940s/iter; left time: 3321.9536s\n",
      "Epoch: 4 cost time: 224.00054574012756\n",
      "Epoch: 4, Steps: 1066 | Train Loss: 0.2900400 Vali Loss: 0.4289766 Test Loss: 0.3401493\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Updating learning rate to 0.0038006806421956685\n",
      "\titers: 100, epoch: 5 | loss: 0.3054318\n",
      "\tspeed: 2.1462s/iter; left time: 36392.3775s\n",
      "\titers: 200, epoch: 5 | loss: 0.2916096\n",
      "\tspeed: 0.2218s/iter; left time: 3738.3912s\n",
      "\titers: 300, epoch: 5 | loss: 0.3279371\n",
      "\tspeed: 0.1852s/iter; left time: 3103.0582s\n",
      "\titers: 400, epoch: 5 | loss: 0.2821882\n",
      "\tspeed: 0.2159s/iter; left time: 3595.7150s\n",
      "\titers: 500, epoch: 5 | loss: 0.3131365\n",
      "\tspeed: 0.2080s/iter; left time: 3444.1601s\n",
      "\titers: 600, epoch: 5 | loss: 0.3102808\n",
      "\tspeed: 0.2140s/iter; left time: 3521.5924s\n",
      "\titers: 700, epoch: 5 | loss: 0.3017407\n",
      "\tspeed: 0.2350s/iter; left time: 3843.8683s\n",
      "\titers: 800, epoch: 5 | loss: 0.3021612\n",
      "\tspeed: 0.2190s/iter; left time: 3559.5509s\n",
      "\titers: 900, epoch: 5 | loss: 0.3907889\n",
      "\tspeed: 0.2251s/iter; left time: 3636.2660s\n",
      "\titers: 1000, epoch: 5 | loss: 0.2583530\n",
      "\tspeed: 0.2159s/iter; left time: 3467.1929s\n",
      "Epoch: 5 cost time: 230.10338735580444\n",
      "Epoch: 5, Steps: 1066 | Train Loss: 0.3008715 Vali Loss: 0.5015612 Test Loss: 0.3873927\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Updating learning rate to 0.004678952052548226\n",
      "\titers: 100, epoch: 6 | loss: 0.3159580\n",
      "\tspeed: 2.0650s/iter; left time: 32815.5911s\n",
      "\titers: 200, epoch: 6 | loss: 0.2971943\n",
      "\tspeed: 0.2040s/iter; left time: 3221.6593s\n",
      "\titers: 300, epoch: 6 | loss: 0.3383486\n",
      "\tspeed: 0.2110s/iter; left time: 3310.8140s\n",
      "\titers: 400, epoch: 6 | loss: 0.4522581\n",
      "\tspeed: 0.1909s/iter; left time: 2977.0243s\n",
      "\titers: 500, epoch: 6 | loss: 0.3445234\n",
      "\tspeed: 0.2120s/iter; left time: 3284.0303s\n",
      "\titers: 600, epoch: 6 | loss: 0.2946137\n",
      "\tspeed: 0.2041s/iter; left time: 3141.7787s\n",
      "\titers: 700, epoch: 6 | loss: 0.3033440\n",
      "\tspeed: 0.1940s/iter; left time: 2966.0908s\n",
      "\titers: 800, epoch: 6 | loss: 0.3283331\n",
      "\tspeed: 0.2290s/iter; left time: 3479.4048s\n",
      "\titers: 900, epoch: 6 | loss: 0.3591681\n",
      "\tspeed: 0.2230s/iter; left time: 3365.0337s\n",
      "\titers: 1000, epoch: 6 | loss: 0.3857500\n",
      "\tspeed: 0.2089s/iter; left time: 3131.0765s\n",
      "Epoch: 6 cost time: 221.99269080162048\n",
      "Epoch: 6, Steps: 1066 | Train Loss: 0.3136538 Vali Loss: 0.5096110 Test Loss: 0.4111784\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Updating learning rate to 0.004999999944609213\n",
      "\titers: 100, epoch: 7 | loss: 0.3359827\n",
      "\tspeed: 1.9731s/iter; left time: 29250.8891s\n",
      "\titers: 200, epoch: 7 | loss: 0.3560729\n",
      "\tspeed: 0.2209s/iter; left time: 3253.0570s\n",
      "\titers: 300, epoch: 7 | loss: 0.2291275\n",
      "\tspeed: 0.1920s/iter; left time: 2808.6864s\n",
      "\titers: 400, epoch: 7 | loss: 0.3295738\n",
      "\tspeed: 0.2030s/iter; left time: 2948.6417s\n",
      "\titers: 500, epoch: 7 | loss: 0.3059781\n",
      "\tspeed: 0.2101s/iter; left time: 3030.4196s\n",
      "\titers: 600, epoch: 7 | loss: 0.2722683\n",
      "\tspeed: 0.1959s/iter; left time: 2805.8464s\n",
      "\titers: 700, epoch: 7 | loss: 0.3145621\n",
      "\tspeed: 0.1970s/iter; left time: 2802.5629s\n",
      "\titers: 800, epoch: 7 | loss: 0.3645648\n",
      "\tspeed: 0.1980s/iter; left time: 2796.8221s\n",
      "\titers: 900, epoch: 7 | loss: 0.3077975\n",
      "\tspeed: 0.2030s/iter; left time: 2846.8149s\n",
      "\titers: 1000, epoch: 7 | loss: 0.3203718\n",
      "\tspeed: 0.2070s/iter; left time: 2882.3933s\n",
      "Epoch: 7 cost time: 217.8039472103119\n",
      "Epoch: 7, Steps: 1066 | Train Loss: 0.3103071 Vali Loss: 0.4899839 Test Loss: 0.3712360\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Updating learning rate to 0.004937202872622028\n",
      "\titers: 100, epoch: 8 | loss: 0.3036494\n",
      "\tspeed: 2.0861s/iter; left time: 28702.0090s\n",
      "\titers: 200, epoch: 8 | loss: 0.2669308\n",
      "\tspeed: 0.2261s/iter; left time: 3088.1560s\n",
      "\titers: 300, epoch: 8 | loss: 0.2690538\n",
      "\tspeed: 0.2349s/iter; left time: 3184.4718s\n",
      "\titers: 400, epoch: 8 | loss: 0.3264558\n",
      "\tspeed: 0.2310s/iter; left time: 3109.6987s\n",
      "\titers: 500, epoch: 8 | loss: 0.3001035\n",
      "\tspeed: 0.2209s/iter; left time: 2951.6347s\n",
      "\titers: 600, epoch: 8 | loss: 0.3242957\n",
      "\tspeed: 0.2100s/iter; left time: 2784.3875s\n",
      "\titers: 700, epoch: 8 | loss: 0.3458994\n",
      "\tspeed: 0.1900s/iter; left time: 2500.6084s\n",
      "\titers: 800, epoch: 8 | loss: 0.3393930\n",
      "\tspeed: 0.2031s/iter; left time: 2652.4634s\n",
      "\titers: 900, epoch: 8 | loss: 0.3070399\n",
      "\tspeed: 0.2119s/iter; left time: 2745.9261s\n",
      "\titers: 1000, epoch: 8 | loss: 0.3290171\n",
      "\tspeed: 0.2180s/iter; left time: 2803.5649s\n",
      "Epoch: 8 cost time: 231.00286960601807\n",
      "Epoch: 8, Steps: 1066 | Train Loss: 0.3142733 Vali Loss: 0.5156478 Test Loss: 0.3975283\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Updating learning rate to 0.004752194773170418\n",
      "\titers: 100, epoch: 9 | loss: 0.3088832\n",
      "\tspeed: 2.1169s/iter; left time: 26870.3226s\n",
      "\titers: 200, epoch: 9 | loss: 0.2791042\n",
      "\tspeed: 0.2151s/iter; left time: 2708.9433s\n",
      "\titers: 300, epoch: 9 | loss: 0.2963978\n",
      "\tspeed: 0.2130s/iter; left time: 2660.5941s\n",
      "\titers: 400, epoch: 9 | loss: 0.3221558\n",
      "\tspeed: 0.1810s/iter; left time: 2242.7558s\n",
      "\titers: 500, epoch: 9 | loss: 0.2939225\n",
      "\tspeed: 0.2260s/iter; left time: 2778.2969s\n",
      "\titers: 600, epoch: 9 | loss: 0.3543939\n",
      "\tspeed: 0.2410s/iter; left time: 2938.8212s\n",
      "\titers: 700, epoch: 9 | loss: 0.2893316\n",
      "\tspeed: 0.2530s/iter; left time: 3059.1626s\n",
      "\titers: 800, epoch: 9 | loss: 0.2983713\n",
      "\tspeed: 0.2349s/iter; left time: 2817.6921s\n",
      "\titers: 900, epoch: 9 | loss: 0.2808933\n",
      "\tspeed: 0.2380s/iter; left time: 2830.6016s\n",
      "\titers: 1000, epoch: 9 | loss: 0.2832290\n",
      "\tspeed: 0.2110s/iter; left time: 2488.2906s\n",
      "Epoch: 9 cost time: 236.80615258216858\n",
      "Epoch: 9, Steps: 1066 | Train Loss: 0.3043860 Vali Loss: 0.4863562 Test Loss: 0.3802918\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Updating learning rate to 0.00445425272488743\n",
      "\titers: 100, epoch: 10 | loss: 0.2376926\n",
      "\tspeed: 2.1821s/iter; left time: 25371.1265s\n",
      "\titers: 200, epoch: 10 | loss: 0.2970727\n",
      "\tspeed: 0.2231s/iter; left time: 2571.1267s\n",
      "\titers: 300, epoch: 10 | loss: 0.3384405\n",
      "\tspeed: 0.2419s/iter; left time: 2763.6882s\n",
      "\titers: 400, epoch: 10 | loss: 0.3060039\n",
      "\tspeed: 0.2151s/iter; left time: 2436.0566s\n",
      "\titers: 500, epoch: 10 | loss: 0.2754360\n",
      "\tspeed: 0.2201s/iter; left time: 2471.1839s\n",
      "\titers: 600, epoch: 10 | loss: 0.2939005\n",
      "\tspeed: 0.2268s/iter; left time: 2524.0571s\n",
      "\titers: 700, epoch: 10 | loss: 0.3290821\n",
      "\tspeed: 0.2320s/iter; left time: 2558.1936s\n",
      "\titers: 800, epoch: 10 | loss: 0.3175697\n",
      "\tspeed: 0.2401s/iter; left time: 2623.5912s\n",
      "\titers: 900, epoch: 10 | loss: 0.2715886\n",
      "\tspeed: 0.2389s/iter; left time: 2586.9720s\n",
      "\titers: 1000, epoch: 10 | loss: 0.3318566\n",
      "\tspeed: 0.2450s/iter; left time: 2627.8829s\n",
      "Epoch: 10 cost time: 247.4947669506073\n",
      "Epoch: 10, Steps: 1066 | Train Loss: 0.3086352 Vali Loss: 0.4946047 Test Loss: 0.3694962\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Updating learning rate to 0.004058316786171623\n",
      "\titers: 100, epoch: 11 | loss: 0.3104575\n",
      "\tspeed: 2.1881s/iter; left time: 23108.3375s\n",
      "\titers: 200, epoch: 11 | loss: 0.2938634\n",
      "\tspeed: 0.2419s/iter; left time: 2530.7456s\n",
      "\titers: 300, epoch: 11 | loss: 0.3463811\n",
      "\tspeed: 0.2500s/iter; left time: 2590.4891s\n",
      "\titers: 400, epoch: 11 | loss: 0.3056556\n",
      "\tspeed: 0.2370s/iter; left time: 2431.5533s\n",
      "\titers: 500, epoch: 11 | loss: 0.4087439\n",
      "\tspeed: 0.2310s/iter; left time: 2347.3709s\n",
      "\titers: 600, epoch: 11 | loss: 0.2950611\n",
      "\tspeed: 0.2430s/iter; left time: 2444.6021s\n",
      "\titers: 700, epoch: 11 | loss: 0.3648129\n",
      "\tspeed: 0.1970s/iter; left time: 1962.3392s\n",
      "\titers: 800, epoch: 11 | loss: 0.3378738\n",
      "\tspeed: 0.2370s/iter; left time: 2337.1366s\n",
      "\titers: 900, epoch: 11 | loss: 0.3103725\n",
      "\tspeed: 0.2301s/iter; left time: 2245.5363s\n",
      "\titers: 1000, epoch: 11 | loss: 0.2524386\n",
      "\tspeed: 0.2410s/iter; left time: 2327.9413s\n",
      "Epoch: 11 cost time: 250.39236068725586\n",
      "Epoch: 11, Steps: 1066 | Train Loss: 0.3105870 Vali Loss: 0.4693412 Test Loss: 0.3622843\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Updating learning rate to 0.003584240838274705\n",
      "\titers: 100, epoch: 12 | loss: 0.2682878\n",
      "\tspeed: 2.1720s/iter; left time: 20623.2349s\n",
      "\titers: 200, epoch: 12 | loss: 0.3535172\n",
      "\tspeed: 0.2190s/iter; left time: 2057.2272s\n",
      "\titers: 300, epoch: 12 | loss: 0.3326555\n",
      "\tspeed: 0.1750s/iter; left time: 1626.4810s\n",
      "\titers: 400, epoch: 12 | loss: 0.2585241\n",
      "\tspeed: 0.2440s/iter; left time: 2243.7610s\n",
      "\titers: 500, epoch: 12 | loss: 0.3480316\n",
      "\tspeed: 0.2260s/iter; left time: 2055.3567s\n",
      "\titers: 600, epoch: 12 | loss: 0.2682044\n",
      "\tspeed: 0.2570s/iter; left time: 2312.1279s\n",
      "\titers: 700, epoch: 12 | loss: 0.3314502\n",
      "\tspeed: 0.2480s/iter; left time: 2205.7613s\n",
      "\titers: 800, epoch: 12 | loss: 0.3474393\n",
      "\tspeed: 0.2520s/iter; left time: 2216.4615s\n",
      "\titers: 900, epoch: 12 | loss: 0.3236839\n",
      "\tspeed: 0.2169s/iter; left time: 1886.2862s\n",
      "\titers: 1000, epoch: 12 | loss: 0.3087365\n",
      "\tspeed: 0.2010s/iter; left time: 1728.0216s\n",
      "Epoch: 12 cost time: 243.11099433898926\n",
      "Epoch: 12, Steps: 1066 | Train Loss: 0.3034332 Vali Loss: 0.4777956 Test Loss: 0.3773479\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Updating learning rate to 0.0030557970287929913\n",
      "\titers: 100, epoch: 13 | loss: 0.2671912\n",
      "\tspeed: 2.2330s/iter; left time: 18821.7398s\n",
      "\titers: 200, epoch: 13 | loss: 0.3105684\n",
      "\tspeed: 0.2280s/iter; left time: 1898.8606s\n",
      "\titers: 300, epoch: 13 | loss: 0.3421765\n",
      "\tspeed: 0.2490s/iter; left time: 2049.1442s\n",
      "\titers: 400, epoch: 13 | loss: 0.3347946\n",
      "\tspeed: 0.2250s/iter; left time: 1828.8061s\n",
      "\titers: 500, epoch: 13 | loss: 0.3712304\n",
      "\tspeed: 0.2100s/iter; left time: 1686.4862s\n",
      "\titers: 600, epoch: 13 | loss: 0.3310980\n",
      "\tspeed: 0.2351s/iter; left time: 1863.8541s\n",
      "\titers: 700, epoch: 13 | loss: 0.2881189\n",
      "\tspeed: 0.2389s/iter; left time: 1870.6832s\n",
      "\titers: 800, epoch: 13 | loss: 0.2799405\n",
      "\tspeed: 0.2441s/iter; left time: 1886.6272s\n",
      "\titers: 900, epoch: 13 | loss: 0.2309173\n",
      "\tspeed: 0.2401s/iter; left time: 1831.8081s\n",
      "\titers: 1000, epoch: 13 | loss: 0.3236510\n",
      "\tspeed: 0.2420s/iter; left time: 1821.9262s\n",
      "Epoch: 13 cost time: 250.0027105808258\n",
      "Epoch: 13, Steps: 1066 | Train Loss: 0.2954668 Vali Loss: 0.4807599 Test Loss: 0.3643355\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Updating learning rate to 0.002499483736923074\n",
      "\titers: 100, epoch: 14 | loss: 0.2838587\n",
      "\tspeed: 2.2099s/iter; left time: 16271.7098s\n",
      "\titers: 200, epoch: 14 | loss: 0.2571622\n",
      "\tspeed: 0.2440s/iter; left time: 1772.4337s\n",
      "\titers: 300, epoch: 14 | loss: 0.3461806\n",
      "\tspeed: 0.2310s/iter; left time: 1654.3129s\n",
      "\titers: 400, epoch: 14 | loss: 0.2938927\n",
      "\tspeed: 0.2470s/iter; left time: 1744.6777s\n",
      "\titers: 500, epoch: 14 | loss: 0.3444382\n",
      "\tspeed: 0.2341s/iter; left time: 1630.3458s\n",
      "\titers: 600, epoch: 14 | loss: 0.2462124\n",
      "\tspeed: 0.2088s/iter; left time: 1433.1743s\n",
      "\titers: 700, epoch: 14 | loss: 0.3119216\n",
      "\tspeed: 0.2492s/iter; left time: 1685.3016s\n",
      "\titers: 800, epoch: 14 | loss: 0.2630194\n",
      "\tspeed: 0.2419s/iter; left time: 1611.4501s\n",
      "\titers: 900, epoch: 14 | loss: 0.2713610\n",
      "\tspeed: 0.2431s/iter; left time: 1595.1575s\n",
      "\titers: 1000, epoch: 14 | loss: 0.2494114\n",
      "\tspeed: 0.2479s/iter; left time: 1602.1764s\n",
      "Epoch: 14 cost time: 254.70535945892334\n",
      "Epoch: 14, Steps: 1066 | Train Loss: 0.2914157 Vali Loss: 0.4763526 Test Loss: 0.3679706\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Updating learning rate to 0.0019431968340813185\n",
      "\titers: 100, epoch: 15 | loss: 0.2963225\n",
      "\tspeed: 2.2041s/iter; left time: 13879.1097s\n",
      "\titers: 200, epoch: 15 | loss: 0.3040628\n",
      "\tspeed: 0.2430s/iter; left time: 1505.9478s\n",
      "\titers: 300, epoch: 15 | loss: 0.2717607\n",
      "\tspeed: 0.2330s/iter; left time: 1420.5967s\n",
      "\titers: 400, epoch: 15 | loss: 0.2674853\n",
      "\tspeed: 0.2180s/iter; left time: 1307.3046s\n",
      "\titers: 500, epoch: 15 | loss: 0.2716066\n",
      "\tspeed: 0.2339s/iter; left time: 1379.3969s\n",
      "\titers: 600, epoch: 15 | loss: 0.3868525\n",
      "\tspeed: 0.2370s/iter; left time: 1373.8751s\n",
      "\titers: 700, epoch: 15 | loss: 0.2540365\n",
      "\tspeed: 0.2382s/iter; left time: 1357.1785s\n",
      "\titers: 800, epoch: 15 | loss: 0.2802218\n",
      "\tspeed: 0.2058s/iter; left time: 1151.9653s\n",
      "\titers: 900, epoch: 15 | loss: 0.3237149\n",
      "\tspeed: 0.2500s/iter; left time: 1374.4602s\n",
      "\titers: 1000, epoch: 15 | loss: 0.2919720\n",
      "\tspeed: 0.2471s/iter; left time: 1333.4638s\n",
      "Epoch: 15 cost time: 247.59913730621338\n",
      "Epoch: 15, Steps: 1066 | Train Loss: 0.2884772 Vali Loss: 0.4912077 Test Loss: 0.3698853\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Updating learning rate to 0.001414830868428021\n",
      "\titers: 100, epoch: 16 | loss: 0.2679077\n",
      "\tspeed: 2.2180s/iter; left time: 11602.1861s\n",
      "\titers: 200, epoch: 16 | loss: 0.2804462\n",
      "\tspeed: 0.2439s/iter; left time: 1251.4435s\n",
      "\titers: 300, epoch: 16 | loss: 0.2654314\n",
      "\tspeed: 0.2410s/iter; left time: 1212.6172s\n",
      "\titers: 400, epoch: 16 | loss: 0.3565859\n",
      "\tspeed: 0.2360s/iter; left time: 1163.5806s\n",
      "\titers: 500, epoch: 16 | loss: 0.3526590\n",
      "\tspeed: 0.2411s/iter; left time: 1164.7061s\n",
      "\titers: 600, epoch: 16 | loss: 0.2892540\n",
      "\tspeed: 0.2339s/iter; left time: 1106.5363s\n",
      "\titers: 700, epoch: 16 | loss: 0.3200344\n",
      "\tspeed: 0.2410s/iter; left time: 1116.2114s\n",
      "\titers: 800, epoch: 16 | loss: 0.2785325\n",
      "\tspeed: 0.2520s/iter; left time: 1141.6494s\n",
      "\titers: 900, epoch: 16 | loss: 0.2714866\n",
      "\tspeed: 0.1930s/iter; left time: 855.1864s\n",
      "\titers: 1000, epoch: 16 | loss: 0.3196246\n",
      "\tspeed: 0.1871s/iter; left time: 810.5260s\n",
      "Epoch: 16 cost time: 244.00073838233948\n",
      "Epoch: 16, Steps: 1066 | Train Loss: 0.2810613 Vali Loss: 0.5062452 Test Loss: 0.3723489\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Updating learning rate to 0.0009408803157451751\n",
      "\titers: 100, epoch: 17 | loss: 0.2646129\n",
      "\tspeed: 2.2499s/iter; left time: 9370.8831s\n",
      "\titers: 200, epoch: 17 | loss: 0.2868425\n",
      "\tspeed: 0.2300s/iter; left time: 935.0238s\n",
      "\titers: 300, epoch: 17 | loss: 0.3018965\n",
      "\tspeed: 0.2150s/iter; left time: 852.5022s\n",
      "\titers: 400, epoch: 17 | loss: 0.2855314\n",
      "\tspeed: 0.2540s/iter; left time: 981.6914s\n",
      "\titers: 500, epoch: 17 | loss: 0.3067055\n",
      "\tspeed: 0.2661s/iter; left time: 1001.8274s\n",
      "\titers: 600, epoch: 17 | loss: 0.2816727\n",
      "\tspeed: 0.2560s/iter; left time: 938.2470s\n",
      "\titers: 700, epoch: 17 | loss: 0.2707953\n",
      "\tspeed: 0.2550s/iter; left time: 908.9662s\n",
      "\titers: 800, epoch: 17 | loss: 0.2605777\n",
      "\tspeed: 0.2319s/iter; left time: 803.4038s\n",
      "\titers: 900, epoch: 17 | loss: 0.2699721\n",
      "\tspeed: 0.2310s/iter; left time: 777.4531s\n",
      "\titers: 1000, epoch: 17 | loss: 0.2826258\n",
      "\tspeed: 0.2450s/iter; left time: 799.9469s\n",
      "Epoch: 17 cost time: 256.41284823417664\n",
      "Epoch: 17, Steps: 1066 | Train Loss: 0.2770256 Vali Loss: 0.4915188 Test Loss: 0.3692251\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Updating learning rate to 0.0005451110357894541\n",
      "\titers: 100, epoch: 18 | loss: 0.2883897\n",
      "\tspeed: 2.2070s/iter; left time: 6839.5039s\n",
      "\titers: 200, epoch: 18 | loss: 0.2855675\n",
      "\tspeed: 0.2480s/iter; left time: 743.7763s\n",
      "\titers: 300, epoch: 18 | loss: 0.2651449\n",
      "\tspeed: 0.2241s/iter; left time: 649.5361s\n",
      "\titers: 400, epoch: 18 | loss: 0.2470195\n",
      "\tspeed: 0.2349s/iter; left time: 657.4811s\n",
      "\titers: 500, epoch: 18 | loss: 0.2472479\n",
      "\tspeed: 0.1830s/iter; left time: 493.8789s\n",
      "\titers: 600, epoch: 18 | loss: 0.2652398\n",
      "\tspeed: 0.2341s/iter; left time: 608.3244s\n",
      "\titers: 700, epoch: 18 | loss: 0.2646227\n",
      "\tspeed: 0.2201s/iter; left time: 549.9182s\n",
      "\titers: 800, epoch: 18 | loss: 0.2653856\n",
      "\tspeed: 0.2179s/iter; left time: 522.6828s\n",
      "\titers: 900, epoch: 18 | loss: 0.2338603\n",
      "\tspeed: 0.2332s/iter; left time: 536.1011s\n",
      "\titers: 1000, epoch: 18 | loss: 0.2394356\n",
      "\tspeed: 0.2219s/iter; left time: 487.9953s\n",
      "Epoch: 18 cost time: 241.39652514457703\n",
      "Epoch: 18, Steps: 1066 | Train Loss: 0.2728269 Vali Loss: 0.4947801 Test Loss: 0.3640780\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Updating learning rate to 0.00024736855284643\n",
      "\titers: 100, epoch: 19 | loss: 0.2167196\n",
      "\tspeed: 2.0840s/iter; left time: 4236.8580s\n",
      "\titers: 200, epoch: 19 | loss: 0.2924813\n",
      "\tspeed: 0.2269s/iter; left time: 438.5118s\n",
      "\titers: 300, epoch: 19 | loss: 0.2687509\n",
      "\tspeed: 0.2360s/iter; left time: 432.5771s\n",
      "\titers: 400, epoch: 19 | loss: 0.2330209\n",
      "\tspeed: 0.2141s/iter; left time: 370.9615s\n",
      "\titers: 500, epoch: 19 | loss: 0.2461385\n",
      "\tspeed: 0.2110s/iter; left time: 344.5557s\n",
      "\titers: 600, epoch: 19 | loss: 0.3040788\n",
      "\tspeed: 0.2250s/iter; left time: 344.9156s\n",
      "\titers: 700, epoch: 19 | loss: 0.3851055\n",
      "\tspeed: 0.2380s/iter; left time: 340.9901s\n",
      "\titers: 800, epoch: 19 | loss: 0.2825858\n",
      "\tspeed: 0.2291s/iter; left time: 305.3475s\n",
      "\titers: 900, epoch: 19 | loss: 0.2481631\n",
      "\tspeed: 0.2329s/iter; left time: 287.1983s\n",
      "\titers: 1000, epoch: 19 | loss: 0.2952844\n",
      "\tspeed: 0.2261s/iter; left time: 256.2175s\n",
      "Epoch: 19 cost time: 241.50197386741638\n",
      "Epoch: 19, Steps: 1066 | Train Loss: 0.2712828 Vali Loss: 0.5020989 Test Loss: 0.3649084\n",
      "EarlyStopping counter: 18 out of 20\n",
      "Updating learning rate to 6.25829182752033e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.2630900\n",
      "\tspeed: 2.0710s/iter; left time: 2002.7030s\n",
      "\titers: 200, epoch: 20 | loss: 0.3631445\n",
      "\tspeed: 0.2278s/iter; left time: 197.5170s\n",
      "\titers: 300, epoch: 20 | loss: 0.2652210\n",
      "\tspeed: 0.2270s/iter; left time: 174.1130s\n",
      "\titers: 400, epoch: 20 | loss: 0.2303094\n",
      "\tspeed: 0.2311s/iter; left time: 154.1697s\n",
      "\titers: 500, epoch: 20 | loss: 0.2688562\n",
      "\tspeed: 0.1969s/iter; left time: 111.6461s\n",
      "\titers: 600, epoch: 20 | loss: 0.3094188\n",
      "\tspeed: 0.2170s/iter; left time: 101.3259s\n",
      "\titers: 700, epoch: 20 | loss: 0.3210035\n",
      "\tspeed: 0.2340s/iter; left time: 85.8945s\n",
      "\titers: 800, epoch: 20 | loss: 0.2382380\n",
      "\tspeed: 0.2270s/iter; left time: 60.6101s\n",
      "\titers: 900, epoch: 20 | loss: 0.2654021\n",
      "\tspeed: 0.2250s/iter; left time: 37.5764s\n",
      "\titers: 1000, epoch: 20 | loss: 0.3388176\n",
      "\tspeed: 0.2171s/iter; left time: 14.5483s\n",
      "Epoch: 20 cost time: 236.49956488609314\n",
      "Epoch: 20, Steps: 1066 | Train Loss: 0.2698487 Vali Loss: 0.4986086 Test Loss: 0.3657099\n",
      "EarlyStopping counter: 19 out of 20\n",
      "Updating learning rate to 2.0055390787437723e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): PatchTST_backbone(\n",
       "    (backbone): TSTiEncoder(\n",
       "      (W_P): Linear(in_features=60, out_features=128, bias=True)\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (encoder): TSTEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-2): 3 x TSTEncoderLayer(\n",
       "            (self_attn): _MultiheadAttention(\n",
       "              (W_Q): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (W_K): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (W_V): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sdp_attn): _ScaledDotProductAttention(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (1): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout_attn): Dropout(p=0.2, inplace=False)\n",
       "            (norm_attn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.2, inplace=False)\n",
       "              (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "            )\n",
       "            (dropout_ffn): Dropout(p=0.2, inplace=False)\n",
       "            (norm_ffn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Flatten_Head(\n",
       "      (flatten): Flatten(start_dim=-2, end_dim=-1)\n",
       "      (linear): Linear(in_features=4480, out_features=96, bias=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44c1a96",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c1b1dcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 11425\n",
      "mse:0.34566619992256165, mae:0.4012136459350586, rse:0.5594445466995239\n"
     ]
    }
   ],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf10b6be",
   "metadata": {},
   "source": [
    "---\n",
    "## Trail 2: PatchTST/64, Dataset:ETTm1 , Metric: 96\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2e519568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ETTm1, Prediction Length: 96, Number of Patches: 64\n"
     ]
    }
   ],
   "source": [
    "args.pred_len = 96 # prediction sequence length\n",
    "args.num_patch = 64  # The number of input patches\n",
    "print(f\"Dataset: {args.data}, Prediction Length: {args.pred_len}, Number of Patches: {args.num_patch}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101eba5a",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f7e26d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "train 34129\n",
      "val 11425\n",
      "test 11425\n",
      "\titers: 100, epoch: 1 | loss: 0.3284838\n",
      "\tspeed: 0.2034s/iter; left time: 4316.0460s\n",
      "\titers: 200, epoch: 1 | loss: 0.3892616\n",
      "\tspeed: 0.2299s/iter; left time: 4855.1031s\n",
      "\titers: 300, epoch: 1 | loss: 0.3343551\n",
      "\tspeed: 0.2181s/iter; left time: 4585.2195s\n",
      "\titers: 400, epoch: 1 | loss: 0.3546901\n",
      "\tspeed: 0.2370s/iter; left time: 4957.5456s\n",
      "\titers: 500, epoch: 1 | loss: 0.2436483\n",
      "\tspeed: 0.2189s/iter; left time: 4557.9345s\n",
      "\titers: 600, epoch: 1 | loss: 0.2919290\n",
      "\tspeed: 0.2121s/iter; left time: 4395.6333s\n",
      "\titers: 700, epoch: 1 | loss: 0.2979367\n",
      "\tspeed: 0.2149s/iter; left time: 4431.9062s\n",
      "\titers: 800, epoch: 1 | loss: 0.2730506\n",
      "\tspeed: 0.2341s/iter; left time: 4804.2584s\n",
      "\titers: 900, epoch: 1 | loss: 0.3008355\n",
      "\tspeed: 0.2319s/iter; left time: 4734.7312s\n",
      "\titers: 1000, epoch: 1 | loss: 0.3204103\n",
      "\tspeed: 0.2390s/iter; left time: 4857.3499s\n",
      "Epoch: 1 cost time: 239.6319296360016\n",
      "Epoch: 1, Steps: 1066 | Train Loss: 0.3245013 Vali Loss: 0.4370958 Test Loss: 0.3415921\n",
      "Validation loss decreased (inf --> 0.437096).  Saving model ...\n",
      "Updating learning rate to 0.000521637289413384\n",
      "\titers: 100, epoch: 2 | loss: 0.3180556\n",
      "\tspeed: 2.1750s/iter; left time: 43837.0675s\n",
      "\titers: 200, epoch: 2 | loss: 0.3812967\n",
      "\tspeed: 0.2420s/iter; left time: 4853.2116s\n",
      "\titers: 300, epoch: 2 | loss: 0.2680837\n",
      "\tspeed: 0.1971s/iter; left time: 3933.6861s\n",
      "\titers: 400, epoch: 2 | loss: 0.3442388\n",
      "\tspeed: 0.2299s/iter; left time: 4563.8570s\n",
      "\titers: 500, epoch: 2 | loss: 0.2762150\n",
      "\tspeed: 0.2060s/iter; left time: 4069.7609s\n",
      "\titers: 600, epoch: 2 | loss: 0.2832123\n",
      "\tspeed: 0.2200s/iter; left time: 4325.0308s\n",
      "\titers: 700, epoch: 2 | loss: 0.2609709\n",
      "\tspeed: 0.2050s/iter; left time: 4007.8796s\n",
      "\titers: 800, epoch: 2 | loss: 0.2990474\n",
      "\tspeed: 0.2360s/iter; left time: 4591.5701s\n",
      "\titers: 900, epoch: 2 | loss: 0.3149389\n",
      "\tspeed: 0.2240s/iter; left time: 4335.0689s\n",
      "\titers: 1000, epoch: 2 | loss: 0.3190518\n",
      "\tspeed: 0.2200s/iter; left time: 4235.6769s\n",
      "Epoch: 2 cost time: 237.2975673675537\n",
      "Epoch: 2, Steps: 1066 | Train Loss: 0.2888744 Vali Loss: 0.4572895 Test Loss: 0.3642993\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Updating learning rate to 0.0014003403693692118\n",
      "\titers: 100, epoch: 3 | loss: 0.2730078\n",
      "\tspeed: 2.1210s/iter; left time: 40487.3994s\n",
      "\titers: 200, epoch: 3 | loss: 0.2833009\n",
      "\tspeed: 0.2370s/iter; left time: 4501.0128s\n",
      "\titers: 300, epoch: 3 | loss: 0.2389544\n",
      "\tspeed: 0.2131s/iter; left time: 4025.2420s\n",
      "\titers: 400, epoch: 3 | loss: 0.2513844\n",
      "\tspeed: 0.2359s/iter; left time: 4432.6296s\n",
      "\titers: 500, epoch: 3 | loss: 0.2781471\n",
      "\tspeed: 0.2151s/iter; left time: 4020.0068s\n",
      "\titers: 600, epoch: 3 | loss: 0.2917385\n",
      "\tspeed: 0.2310s/iter; left time: 4293.9280s\n",
      "\titers: 700, epoch: 3 | loss: 0.2701128\n",
      "\tspeed: 0.2279s/iter; left time: 4213.9702s\n",
      "\titers: 800, epoch: 3 | loss: 0.2174722\n",
      "\tspeed: 0.2340s/iter; left time: 4303.6949s\n",
      "\titers: 900, epoch: 3 | loss: 0.2884752\n",
      "\tspeed: 0.2211s/iter; left time: 4042.9246s\n",
      "\titers: 1000, epoch: 3 | loss: 0.3145734\n",
      "\tspeed: 0.2539s/iter; left time: 4618.3171s\n",
      "Epoch: 3 cost time: 241.1090235710144\n",
      "Epoch: 3, Steps: 1066 | Train Loss: 0.2658130 Vali Loss: 0.3970587 Test Loss: 0.3271179\n",
      "Validation loss decreased (0.437096 --> 0.397059).  Saving model ...\n",
      "Updating learning rate to 0.0026005895091706643\n",
      "\titers: 100, epoch: 4 | loss: 0.2968210\n",
      "\tspeed: 2.0560s/iter; left time: 37055.3434s\n",
      "\titers: 200, epoch: 4 | loss: 0.3037373\n",
      "\tspeed: 0.2220s/iter; left time: 3978.5208s\n",
      "\titers: 300, epoch: 4 | loss: 0.4837794\n",
      "\tspeed: 0.2290s/iter; left time: 4081.7405s\n",
      "\titers: 400, epoch: 4 | loss: 0.2453674\n",
      "\tspeed: 0.2310s/iter; left time: 4093.8924s\n",
      "\titers: 500, epoch: 4 | loss: 0.2931569\n",
      "\tspeed: 0.2110s/iter; left time: 3719.2766s\n",
      "\titers: 600, epoch: 4 | loss: 0.2937636\n",
      "\tspeed: 0.2100s/iter; left time: 3679.0903s\n",
      "\titers: 700, epoch: 4 | loss: 0.3194177\n",
      "\tspeed: 0.2461s/iter; left time: 4288.3057s\n",
      "\titers: 800, epoch: 4 | loss: 0.3486413\n",
      "\tspeed: 0.2368s/iter; left time: 4102.2961s\n",
      "\titers: 900, epoch: 4 | loss: 0.3103444\n",
      "\tspeed: 0.2211s/iter; left time: 3808.8338s\n",
      "\titers: 1000, epoch: 4 | loss: 0.2117048\n",
      "\tspeed: 0.2309s/iter; left time: 3954.2558s\n",
      "Epoch: 4 cost time: 240.495361328125\n",
      "Epoch: 4, Steps: 1066 | Train Loss: 0.2883525 Vali Loss: 0.4462393 Test Loss: 0.3573360\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Updating learning rate to 0.0038006806421956685\n",
      "\titers: 100, epoch: 5 | loss: 0.2227904\n",
      "\tspeed: 2.0980s/iter; left time: 35575.3776s\n",
      "\titers: 200, epoch: 5 | loss: 0.2748767\n",
      "\tspeed: 0.2339s/iter; left time: 3943.6656s\n",
      "\titers: 300, epoch: 5 | loss: 0.2822181\n",
      "\tspeed: 0.2241s/iter; left time: 3754.8435s\n",
      "\titers: 400, epoch: 5 | loss: 0.2624627\n",
      "\tspeed: 0.2290s/iter; left time: 3814.3162s\n",
      "\titers: 500, epoch: 5 | loss: 0.2838930\n",
      "\tspeed: 0.2130s/iter; left time: 3527.0186s\n",
      "\titers: 600, epoch: 5 | loss: 0.3956335\n",
      "\tspeed: 0.1969s/iter; left time: 3240.6081s\n",
      "\titers: 700, epoch: 5 | loss: 0.2568835\n",
      "\tspeed: 0.2371s/iter; left time: 3878.6367s\n",
      "\titers: 800, epoch: 5 | loss: 0.3082246\n",
      "\tspeed: 0.2329s/iter; left time: 3786.5804s\n",
      "\titers: 900, epoch: 5 | loss: 0.2912508\n",
      "\tspeed: 0.2430s/iter; left time: 3926.1463s\n",
      "\titers: 1000, epoch: 5 | loss: 0.3125530\n",
      "\tspeed: 0.2410s/iter; left time: 3868.9592s\n",
      "Epoch: 5 cost time: 244.70743894577026\n",
      "Epoch: 5, Steps: 1066 | Train Loss: 0.3098990 Vali Loss: 0.4665969 Test Loss: 0.3634210\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Updating learning rate to 0.004678952052548226\n",
      "\titers: 100, epoch: 6 | loss: 0.3319426\n",
      "\tspeed: 2.0240s/iter; left time: 32163.7084s\n",
      "\titers: 200, epoch: 6 | loss: 0.3121018\n",
      "\tspeed: 0.1900s/iter; left time: 3000.8125s\n",
      "\titers: 300, epoch: 6 | loss: 0.4024402\n",
      "\tspeed: 0.1901s/iter; left time: 2983.1222s\n",
      "\titers: 400, epoch: 6 | loss: 0.3290400\n",
      "\tspeed: 0.2098s/iter; left time: 3271.4685s\n",
      "\titers: 500, epoch: 6 | loss: 0.2978219\n",
      "\tspeed: 0.2110s/iter; left time: 3269.3353s\n",
      "\titers: 600, epoch: 6 | loss: 0.3127100\n",
      "\tspeed: 0.1980s/iter; left time: 3046.8376s\n",
      "\titers: 700, epoch: 6 | loss: 0.2805758\n",
      "\tspeed: 0.2020s/iter; left time: 3088.8249s\n",
      "\titers: 800, epoch: 6 | loss: 0.3214164\n",
      "\tspeed: 0.2030s/iter; left time: 3083.4962s\n",
      "\titers: 900, epoch: 6 | loss: 0.3489761\n",
      "\tspeed: 0.2011s/iter; left time: 3035.4646s\n",
      "\titers: 1000, epoch: 6 | loss: 0.3349140\n",
      "\tspeed: 0.2090s/iter; left time: 3133.2721s\n",
      "Epoch: 6 cost time: 216.40766596794128\n",
      "Epoch: 6, Steps: 1066 | Train Loss: 0.3183605 Vali Loss: 0.5175679 Test Loss: 0.3942514\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Updating learning rate to 0.004999999944609213\n",
      "\titers: 100, epoch: 7 | loss: 0.3384669\n",
      "\tspeed: 2.1579s/iter; left time: 31990.1496s\n",
      "\titers: 200, epoch: 7 | loss: 0.3071374\n",
      "\tspeed: 0.2320s/iter; left time: 3416.3329s\n",
      "\titers: 300, epoch: 7 | loss: 0.3273486\n",
      "\tspeed: 0.2422s/iter; left time: 3542.3093s\n",
      "\titers: 400, epoch: 7 | loss: 0.2693081\n",
      "\tspeed: 0.2478s/iter; left time: 3599.7956s\n",
      "\titers: 500, epoch: 7 | loss: 0.3033170\n",
      "\tspeed: 0.2411s/iter; left time: 3477.5889s\n",
      "\titers: 600, epoch: 7 | loss: 0.3070108\n",
      "\tspeed: 0.2249s/iter; left time: 3221.6517s\n",
      "\titers: 700, epoch: 7 | loss: 0.3255158\n",
      "\tspeed: 0.2251s/iter; left time: 3201.7553s\n",
      "\titers: 800, epoch: 7 | loss: 0.3927919\n",
      "\tspeed: 0.2349s/iter; left time: 3318.5663s\n",
      "\titers: 900, epoch: 7 | loss: 0.3587339\n",
      "\tspeed: 0.2260s/iter; left time: 3169.5727s\n",
      "\titers: 1000, epoch: 7 | loss: 0.3512644\n",
      "\tspeed: 0.2241s/iter; left time: 3120.1381s\n",
      "Epoch: 7 cost time: 248.8918354511261\n",
      "Epoch: 7, Steps: 1066 | Train Loss: 0.3306375 Vali Loss: 0.4721581 Test Loss: 0.3797661\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Updating learning rate to 0.004937202872622028\n",
      "\titers: 100, epoch: 8 | loss: 0.3669337\n",
      "\tspeed: 2.1760s/iter; left time: 29939.9094s\n",
      "\titers: 200, epoch: 8 | loss: 0.2875212\n",
      "\tspeed: 0.2339s/iter; left time: 3194.7817s\n",
      "\titers: 300, epoch: 8 | loss: 0.3120394\n",
      "\tspeed: 0.2461s/iter; left time: 3336.6405s\n",
      "\titers: 400, epoch: 8 | loss: 0.3273082\n",
      "\tspeed: 0.2369s/iter; left time: 3188.9679s\n",
      "\titers: 500, epoch: 8 | loss: 0.3067497\n",
      "\tspeed: 0.2460s/iter; left time: 3285.8968s\n",
      "\titers: 600, epoch: 8 | loss: 0.4197537\n",
      "\tspeed: 0.2270s/iter; left time: 3009.3631s\n",
      "\titers: 700, epoch: 8 | loss: 0.3420227\n",
      "\tspeed: 0.2000s/iter; left time: 2632.3052s\n",
      "\titers: 800, epoch: 8 | loss: 0.3382995\n",
      "\tspeed: 0.2390s/iter; left time: 3120.6784s\n",
      "\titers: 900, epoch: 8 | loss: 0.3638290\n",
      "\tspeed: 0.2320s/iter; left time: 3007.0295s\n",
      "\titers: 1000, epoch: 8 | loss: 0.3696015\n",
      "\tspeed: 0.2340s/iter; left time: 3009.2677s\n",
      "Epoch: 8 cost time: 249.30885004997253\n",
      "Epoch: 8, Steps: 1066 | Train Loss: 0.3239258 Vali Loss: 0.4970765 Test Loss: 0.3952902\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Updating learning rate to 0.004752194773170418\n",
      "\titers: 100, epoch: 9 | loss: 0.2725642\n",
      "\tspeed: 2.1540s/iter; left time: 27340.9642s\n",
      "\titers: 200, epoch: 9 | loss: 0.3168779\n",
      "\tspeed: 0.2480s/iter; left time: 3122.8676s\n",
      "\titers: 300, epoch: 9 | loss: 0.3353391\n",
      "\tspeed: 0.2380s/iter; left time: 2972.8102s\n",
      "\titers: 400, epoch: 9 | loss: 0.3317634\n",
      "\tspeed: 0.2381s/iter; left time: 2950.3642s\n",
      "\titers: 500, epoch: 9 | loss: 0.3346618\n",
      "\tspeed: 0.2311s/iter; left time: 2840.7616s\n",
      "\titers: 600, epoch: 9 | loss: 0.2586750\n",
      "\tspeed: 0.2438s/iter; left time: 2972.7852s\n",
      "\titers: 700, epoch: 9 | loss: 0.2707933\n",
      "\tspeed: 0.2620s/iter; left time: 3168.9056s\n",
      "\titers: 800, epoch: 9 | loss: 0.3005543\n",
      "\tspeed: 0.1981s/iter; left time: 2375.3018s\n",
      "\titers: 900, epoch: 9 | loss: 0.3115899\n",
      "\tspeed: 0.2420s/iter; left time: 2877.8263s\n",
      "\titers: 1000, epoch: 9 | loss: 0.3596305\n",
      "\tspeed: 0.2259s/iter; left time: 2664.2847s\n",
      "Epoch: 9 cost time: 250.08922171592712\n",
      "Epoch: 9, Steps: 1066 | Train Loss: 0.3181540 Vali Loss: 0.5048389 Test Loss: 0.3875533\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Updating learning rate to 0.00445425272488743\n",
      "\titers: 100, epoch: 10 | loss: 0.3592379\n",
      "\tspeed: 2.1721s/iter; left time: 25255.5788s\n",
      "\titers: 200, epoch: 10 | loss: 0.2257831\n",
      "\tspeed: 0.2499s/iter; left time: 2880.3491s\n",
      "\titers: 300, epoch: 10 | loss: 0.2842496\n",
      "\tspeed: 0.2080s/iter; left time: 2377.0032s\n",
      "\titers: 400, epoch: 10 | loss: 0.3146243\n",
      "\tspeed: 0.2290s/iter; left time: 2593.6913s\n",
      "\titers: 500, epoch: 10 | loss: 0.3260279\n",
      "\tspeed: 0.2360s/iter; left time: 2649.8417s\n",
      "\titers: 600, epoch: 10 | loss: 0.3076666\n",
      "\tspeed: 0.2301s/iter; left time: 2560.0658s\n",
      "\titers: 700, epoch: 10 | loss: 0.3247235\n",
      "\tspeed: 0.2370s/iter; left time: 2613.3147s\n",
      "\titers: 800, epoch: 10 | loss: 0.2879558\n",
      "\tspeed: 0.2440s/iter; left time: 2665.9132s\n",
      "\titers: 900, epoch: 10 | loss: 0.2837608\n",
      "\tspeed: 0.2351s/iter; left time: 2545.3406s\n",
      "\titers: 1000, epoch: 10 | loss: 0.2940673\n",
      "\tspeed: 0.2089s/iter; left time: 2240.7721s\n",
      "Epoch: 10 cost time: 245.79488921165466\n",
      "Epoch: 10, Steps: 1066 | Train Loss: 0.3130875 Vali Loss: 0.4980497 Test Loss: 0.3893125\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Updating learning rate to 0.004058316786171623\n",
      "\titers: 100, epoch: 11 | loss: 0.2726318\n",
      "\tspeed: 2.2230s/iter; left time: 23476.7813s\n",
      "\titers: 200, epoch: 11 | loss: 0.2631840\n",
      "\tspeed: 0.2460s/iter; left time: 2573.0354s\n",
      "\titers: 300, epoch: 11 | loss: 0.3146327\n",
      "\tspeed: 0.2460s/iter; left time: 2548.7016s\n",
      "\titers: 400, epoch: 11 | loss: 0.2931761\n",
      "\tspeed: 0.2151s/iter; left time: 2206.6412s\n",
      "\titers: 500, epoch: 11 | loss: 0.3435571\n",
      "\tspeed: 0.2410s/iter; left time: 2449.0786s\n",
      "\titers: 600, epoch: 11 | loss: 0.2565869\n",
      "\tspeed: 0.2430s/iter; left time: 2444.6718s\n",
      "\titers: 700, epoch: 11 | loss: 0.2461441\n",
      "\tspeed: 0.2320s/iter; left time: 2311.1190s\n",
      "\titers: 800, epoch: 11 | loss: 0.3281962\n",
      "\tspeed: 0.2359s/iter; left time: 2326.5608s\n",
      "\titers: 900, epoch: 11 | loss: 0.2666638\n",
      "\tspeed: 0.2410s/iter; left time: 2352.3531s\n",
      "\titers: 1000, epoch: 11 | loss: 0.3468761\n",
      "\tspeed: 0.2380s/iter; left time: 2299.3587s\n",
      "Epoch: 11 cost time: 252.50037717819214\n",
      "Epoch: 11, Steps: 1066 | Train Loss: 0.3070017 Vali Loss: 0.4910306 Test Loss: 0.3843827\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Updating learning rate to 0.003584240838274705\n",
      "\titers: 100, epoch: 12 | loss: 0.3258141\n",
      "\tspeed: 2.1660s/iter; left time: 20566.1860s\n",
      "\titers: 200, epoch: 12 | loss: 0.2691712\n",
      "\tspeed: 0.2280s/iter; left time: 2142.0178s\n",
      "\titers: 300, epoch: 12 | loss: 0.2489127\n",
      "\tspeed: 0.2360s/iter; left time: 2193.8031s\n",
      "\titers: 400, epoch: 12 | loss: 0.2904308\n",
      "\tspeed: 0.2410s/iter; left time: 2215.9366s\n",
      "\titers: 500, epoch: 12 | loss: 0.2788420\n",
      "\tspeed: 0.2180s/iter; left time: 1982.5816s\n",
      "\titers: 600, epoch: 12 | loss: 0.2677372\n",
      "\tspeed: 0.2240s/iter; left time: 2015.1752s\n",
      "\titers: 700, epoch: 12 | loss: 0.3235720\n",
      "\tspeed: 0.2350s/iter; left time: 2090.3962s\n",
      "\titers: 800, epoch: 12 | loss: 0.2094187\n",
      "\tspeed: 0.2460s/iter; left time: 2163.5202s\n",
      "\titers: 900, epoch: 12 | loss: 0.2728820\n",
      "\tspeed: 0.2449s/iter; left time: 2129.8215s\n",
      "\titers: 1000, epoch: 12 | loss: 0.2813557\n",
      "\tspeed: 0.2210s/iter; left time: 1899.7762s\n",
      "Epoch: 12 cost time: 248.1959216594696\n",
      "Epoch: 12, Steps: 1066 | Train Loss: 0.3043991 Vali Loss: 0.5291283 Test Loss: 0.4063411\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Updating learning rate to 0.0030557970287929913\n",
      "\titers: 100, epoch: 13 | loss: 0.3071057\n",
      "\tspeed: 2.1800s/iter; left time: 18374.9498s\n",
      "\titers: 200, epoch: 13 | loss: 0.3236203\n",
      "\tspeed: 0.2452s/iter; left time: 2042.3763s\n",
      "\titers: 300, epoch: 13 | loss: 0.3001985\n",
      "\tspeed: 0.2258s/iter; left time: 1858.1490s\n",
      "\titers: 400, epoch: 13 | loss: 0.2809617\n",
      "\tspeed: 0.2460s/iter; left time: 1999.5613s\n",
      "\titers: 500, epoch: 13 | loss: 0.3020389\n",
      "\tspeed: 0.2402s/iter; left time: 1928.3005s\n",
      "\titers: 600, epoch: 13 | loss: 0.2720407\n",
      "\tspeed: 0.2340s/iter; left time: 1855.5032s\n",
      "\titers: 700, epoch: 13 | loss: 0.3068303\n",
      "\tspeed: 0.2250s/iter; left time: 1761.2526s\n",
      "\titers: 800, epoch: 13 | loss: 0.4074813\n",
      "\tspeed: 0.2349s/iter; left time: 1815.6285s\n",
      "\titers: 900, epoch: 13 | loss: 0.3697517\n",
      "\tspeed: 0.2400s/iter; left time: 1831.3060s\n",
      "\titers: 1000, epoch: 13 | loss: 0.3161417\n",
      "\tspeed: 0.2490s/iter; left time: 1874.3459s\n",
      "Epoch: 13 cost time: 254.30501341819763\n",
      "Epoch: 13, Steps: 1066 | Train Loss: 0.3003159 Vali Loss: 0.5001261 Test Loss: 0.3899169\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Updating learning rate to 0.002499483736923074\n",
      "\titers: 100, epoch: 14 | loss: 0.2922381\n",
      "\tspeed: 2.2500s/iter; left time: 16566.5699s\n",
      "\titers: 200, epoch: 14 | loss: 0.2894678\n",
      "\tspeed: 0.2010s/iter; left time: 1460.1634s\n",
      "\titers: 300, epoch: 14 | loss: 0.3302211\n",
      "\tspeed: 0.1760s/iter; left time: 1260.6287s\n",
      "\titers: 400, epoch: 14 | loss: 0.3005593\n",
      "\tspeed: 0.1899s/iter; left time: 1341.5879s\n",
      "\titers: 500, epoch: 14 | loss: 0.2738260\n",
      "\tspeed: 0.1762s/iter; left time: 1226.7438s\n",
      "\titers: 600, epoch: 14 | loss: 0.3310556\n",
      "\tspeed: 0.1909s/iter; left time: 1310.2003s\n",
      "\titers: 700, epoch: 14 | loss: 0.3191149\n",
      "\tspeed: 0.2149s/iter; left time: 1453.4470s\n",
      "\titers: 800, epoch: 14 | loss: 0.2720521\n",
      "\tspeed: 0.2490s/iter; left time: 1659.3570s\n",
      "\titers: 900, epoch: 14 | loss: 0.2604404\n",
      "\tspeed: 0.2450s/iter; left time: 1608.0021s\n",
      "\titers: 1000, epoch: 14 | loss: 0.2542923\n",
      "\tspeed: 0.2211s/iter; left time: 1428.9103s\n",
      "Epoch: 14 cost time: 222.69900059700012\n",
      "Epoch: 14, Steps: 1066 | Train Loss: 0.2926795 Vali Loss: 0.5154560 Test Loss: 0.3866810\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Updating learning rate to 0.0019431968340813185\n",
      "\titers: 100, epoch: 15 | loss: 0.2942128\n",
      "\tspeed: 2.1880s/iter; left time: 13777.5299s\n",
      "\titers: 200, epoch: 15 | loss: 0.2533699\n",
      "\tspeed: 0.1979s/iter; left time: 1226.6040s\n",
      "\titers: 300, epoch: 15 | loss: 0.2301399\n",
      "\tspeed: 0.2350s/iter; left time: 1432.7258s\n",
      "\titers: 400, epoch: 15 | loss: 0.3236956\n",
      "\tspeed: 0.2430s/iter; left time: 1457.3743s\n",
      "\titers: 500, epoch: 15 | loss: 0.2387688\n",
      "\tspeed: 0.2310s/iter; left time: 1361.9465s\n",
      "\titers: 600, epoch: 15 | loss: 0.3207030\n",
      "\tspeed: 0.2320s/iter; left time: 1345.1288s\n",
      "\titers: 700, epoch: 15 | loss: 0.2647373\n",
      "\tspeed: 0.2440s/iter; left time: 1390.3390s\n",
      "\titers: 800, epoch: 15 | loss: 0.2669805\n",
      "\tspeed: 0.2281s/iter; left time: 1276.4334s\n",
      "\titers: 900, epoch: 15 | loss: 0.2748279\n",
      "\tspeed: 0.2249s/iter; left time: 1236.3214s\n",
      "\titers: 1000, epoch: 15 | loss: 0.2584307\n",
      "\tspeed: 0.2330s/iter; left time: 1257.4566s\n",
      "Epoch: 15 cost time: 243.6073613166809\n",
      "Epoch: 15, Steps: 1066 | Train Loss: 0.2871556 Vali Loss: 0.4978221 Test Loss: 0.3745981\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Updating learning rate to 0.001414830868428021\n",
      "\titers: 100, epoch: 16 | loss: 0.2495004\n",
      "\tspeed: 2.2131s/iter; left time: 11576.4780s\n",
      "\titers: 200, epoch: 16 | loss: 0.2695310\n",
      "\tspeed: 0.2400s/iter; left time: 1231.2775s\n",
      "\titers: 300, epoch: 16 | loss: 0.2954092\n",
      "\tspeed: 0.2330s/iter; left time: 1172.4635s\n",
      "\titers: 400, epoch: 16 | loss: 0.2602929\n",
      "\tspeed: 0.2249s/iter; left time: 1108.9581s\n",
      "\titers: 500, epoch: 16 | loss: 0.3174767\n",
      "\tspeed: 0.2361s/iter; left time: 1140.3668s\n",
      "\titers: 600, epoch: 16 | loss: 0.2257481\n",
      "\tspeed: 0.2380s/iter; left time: 1126.1886s\n",
      "\titers: 700, epoch: 16 | loss: 0.3782798\n",
      "\tspeed: 0.2270s/iter; left time: 1051.1302s\n",
      "\titers: 800, epoch: 16 | loss: 0.2744608\n",
      "\tspeed: 0.2259s/iter; left time: 1023.7585s\n",
      "\titers: 900, epoch: 16 | loss: 0.3951034\n",
      "\tspeed: 0.2410s/iter; left time: 1067.8862s\n",
      "\titers: 1000, epoch: 16 | loss: 0.2967038\n",
      "\tspeed: 0.1920s/iter; left time: 831.4856s\n",
      "Epoch: 16 cost time: 246.20706486701965\n",
      "Epoch: 16, Steps: 1066 | Train Loss: 0.2819514 Vali Loss: 0.5089924 Test Loss: 0.3855127\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Updating learning rate to 0.0009408803157451751\n",
      "\titers: 100, epoch: 17 | loss: 0.2593746\n",
      "\tspeed: 2.1900s/iter; left time: 9121.2961s\n",
      "\titers: 200, epoch: 17 | loss: 0.2713059\n",
      "\tspeed: 0.2290s/iter; left time: 931.0106s\n",
      "\titers: 300, epoch: 17 | loss: 0.2724253\n",
      "\tspeed: 0.2440s/iter; left time: 967.3329s\n",
      "\titers: 400, epoch: 17 | loss: 0.2378368\n",
      "\tspeed: 0.2130s/iter; left time: 823.2666s\n",
      "\titers: 500, epoch: 17 | loss: 0.2995411\n",
      "\tspeed: 0.2280s/iter; left time: 858.4143s\n",
      "\titers: 600, epoch: 17 | loss: 0.2440246\n",
      "\tspeed: 0.2331s/iter; left time: 854.4245s\n",
      "\titers: 700, epoch: 17 | loss: 0.3135412\n",
      "\tspeed: 0.2389s/iter; left time: 851.5918s\n",
      "\titers: 800, epoch: 17 | loss: 0.3167353\n",
      "\tspeed: 0.2321s/iter; left time: 804.2244s\n",
      "\titers: 900, epoch: 17 | loss: 0.2451959\n",
      "\tspeed: 0.2439s/iter; left time: 820.6808s\n",
      "\titers: 1000, epoch: 17 | loss: 0.2782887\n",
      "\tspeed: 0.2262s/iter; left time: 738.3848s\n",
      "Epoch: 17 cost time: 243.99365496635437\n",
      "Epoch: 17, Steps: 1066 | Train Loss: 0.2761372 Vali Loss: 0.5065069 Test Loss: 0.3778329\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Updating learning rate to 0.0005451110357894541\n",
      "\titers: 100, epoch: 18 | loss: 0.2578799\n",
      "\tspeed: 2.1710s/iter; left time: 6727.9270s\n",
      "\titers: 200, epoch: 18 | loss: 0.2676201\n",
      "\tspeed: 0.2378s/iter; left time: 713.2985s\n",
      "\titers: 300, epoch: 18 | loss: 0.2979323\n",
      "\tspeed: 0.2501s/iter; left time: 724.9159s\n",
      "\titers: 400, epoch: 18 | loss: 0.3237371\n",
      "\tspeed: 0.2370s/iter; left time: 663.3851s\n",
      "\titers: 500, epoch: 18 | loss: 0.2408527\n",
      "\tspeed: 0.2291s/iter; left time: 618.2619s\n",
      "\titers: 600, epoch: 18 | loss: 0.2695325\n",
      "\tspeed: 0.2309s/iter; left time: 600.2049s\n",
      "\titers: 700, epoch: 18 | loss: 0.3287108\n",
      "\tspeed: 0.2320s/iter; left time: 579.6884s\n",
      "\titers: 800, epoch: 18 | loss: 0.2984483\n",
      "\tspeed: 0.2340s/iter; left time: 561.2881s\n",
      "\titers: 900, epoch: 18 | loss: 0.2937847\n",
      "\tspeed: 0.2361s/iter; left time: 542.7557s\n",
      "\titers: 1000, epoch: 18 | loss: 0.2940240\n",
      "\tspeed: 0.2389s/iter; left time: 525.4327s\n",
      "Epoch: 18 cost time: 251.6141972541809\n",
      "Epoch: 18, Steps: 1066 | Train Loss: 0.2734593 Vali Loss: 0.5126603 Test Loss: 0.3765165\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Updating learning rate to 0.00024736855284643\n",
      "\titers: 100, epoch: 19 | loss: 0.2473061\n",
      "\tspeed: 2.2501s/iter; left time: 4574.4372s\n",
      "\titers: 200, epoch: 19 | loss: 0.2339845\n",
      "\tspeed: 0.2439s/iter; left time: 471.4874s\n",
      "\titers: 300, epoch: 19 | loss: 0.2511453\n",
      "\tspeed: 0.2571s/iter; left time: 471.2630s\n",
      "\titers: 400, epoch: 19 | loss: 0.2961770\n",
      "\tspeed: 0.2559s/iter; left time: 443.4592s\n",
      "\titers: 500, epoch: 19 | loss: 0.2775190\n",
      "\tspeed: 0.2462s/iter; left time: 402.0093s\n",
      "\titers: 600, epoch: 19 | loss: 0.2782498\n",
      "\tspeed: 0.2439s/iter; left time: 373.8265s\n",
      "\titers: 700, epoch: 19 | loss: 0.3149872\n",
      "\tspeed: 0.2371s/iter; left time: 339.7921s\n",
      "\titers: 800, epoch: 19 | loss: 0.2602175\n",
      "\tspeed: 0.2379s/iter; left time: 317.0920s\n",
      "\titers: 900, epoch: 19 | loss: 0.2662716\n",
      "\tspeed: 0.2561s/iter; left time: 315.7611s\n",
      "\titers: 1000, epoch: 19 | loss: 0.2794972\n",
      "\tspeed: 0.2499s/iter; left time: 283.1821s\n",
      "Epoch: 19 cost time: 263.7164182662964\n",
      "Epoch: 19, Steps: 1066 | Train Loss: 0.2701653 Vali Loss: 0.5201271 Test Loss: 0.3774112\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Updating learning rate to 6.25829182752033e-05\n",
      "\titers: 100, epoch: 20 | loss: 0.2639760\n",
      "\tspeed: 2.2849s/iter; left time: 2209.5240s\n",
      "\titers: 200, epoch: 20 | loss: 0.2775403\n",
      "\tspeed: 0.2050s/iter; left time: 177.7503s\n",
      "\titers: 300, epoch: 20 | loss: 0.2324835\n",
      "\tspeed: 0.2120s/iter; left time: 162.5784s\n",
      "\titers: 400, epoch: 20 | loss: 0.2679330\n",
      "\tspeed: 0.2380s/iter; left time: 158.7715s\n",
      "\titers: 500, epoch: 20 | loss: 0.2410249\n",
      "\tspeed: 0.2201s/iter; left time: 124.8237s\n",
      "\titers: 600, epoch: 20 | loss: 0.2349082\n",
      "\tspeed: 0.2219s/iter; left time: 103.6123s\n",
      "\titers: 700, epoch: 20 | loss: 0.2209024\n",
      "\tspeed: 0.2320s/iter; left time: 85.1257s\n",
      "\titers: 800, epoch: 20 | loss: 0.2828148\n",
      "\tspeed: 0.2352s/iter; left time: 62.8117s\n",
      "\titers: 900, epoch: 20 | loss: 0.2431716\n",
      "\tspeed: 0.2198s/iter; left time: 36.7032s\n",
      "\titers: 1000, epoch: 20 | loss: 0.2748412\n",
      "\tspeed: 0.2261s/iter; left time: 15.1513s\n",
      "Epoch: 20 cost time: 237.90736603736877\n",
      "Epoch: 20, Steps: 1066 | Train Loss: 0.2698091 Vali Loss: 0.5214543 Test Loss: 0.3802127\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Updating learning rate to 2.0055390787437723e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): PatchTST_backbone(\n",
       "    (backbone): TSTiEncoder(\n",
       "      (W_P): Linear(in_features=60, out_features=128, bias=True)\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (encoder): TSTEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-2): 3 x TSTEncoderLayer(\n",
       "            (self_attn): _MultiheadAttention(\n",
       "              (W_Q): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (W_K): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (W_V): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sdp_attn): _ScaledDotProductAttention(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (1): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout_attn): Dropout(p=0.2, inplace=False)\n",
       "            (norm_attn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.2, inplace=False)\n",
       "              (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "            )\n",
       "            (dropout_ffn): Dropout(p=0.2, inplace=False)\n",
       "            (norm_ffn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Flatten_Head(\n",
       "      (flatten): Flatten(start_dim=-2, end_dim=-1)\n",
       "      (linear): Linear(in_features=4480, out_features=96, bias=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5ca166",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "979cf59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 11425\n",
      "mse:0.3271180987358093, mae:0.373868852853775, rse:0.5442279577255249\n"
     ]
    }
   ],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf8a719",
   "metadata": {},
   "source": [
    "---\n",
    "## Trail 3: PatchTST/42, Dataset:ETTm1,  Metric: 192\n",
    "\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "64ba5bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ETTm1, Prediction Length: 192, Number of Patches: 42\n"
     ]
    }
   ],
   "source": [
    "args.pred_len = 192 # prediction sequence length\n",
    "args.num_patch = 42  # The number of input patches\n",
    "print(f\"Dataset: {args.data}, Prediction Length: {args.pred_len}, Number of Patches: {args.num_patch}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85d7a34",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a572d9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "train 34033\n",
      "val 11329\n",
      "test 11329\n",
      "\titers: 100, epoch: 1 | loss: 0.3963512\n",
      "\tspeed: 0.3154s/iter; left time: 6673.4125s\n",
      "\titers: 200, epoch: 1 | loss: 0.3961364\n",
      "\tspeed: 0.3210s/iter; left time: 6760.0896s\n",
      "\titers: 300, epoch: 1 | loss: 0.3892810\n",
      "\tspeed: 0.3260s/iter; left time: 6833.6930s\n",
      "\titers: 400, epoch: 1 | loss: 0.3056287\n",
      "\tspeed: 0.3400s/iter; left time: 7092.5647s\n",
      "\titers: 500, epoch: 1 | loss: 0.4561167\n",
      "\tspeed: 0.3150s/iter; left time: 6539.7569s\n",
      "\titers: 600, epoch: 1 | loss: 0.4037109\n",
      "\tspeed: 0.3370s/iter; left time: 6963.0674s\n",
      "\titers: 700, epoch: 1 | loss: 0.2903167\n",
      "\tspeed: 0.3200s/iter; left time: 6578.7339s\n",
      "\titers: 800, epoch: 1 | loss: 0.3212291\n",
      "\tspeed: 0.3251s/iter; left time: 6651.5133s\n",
      "\titers: 900, epoch: 1 | loss: 0.3361028\n",
      "\tspeed: 0.2879s/iter; left time: 5862.6059s\n",
      "\titers: 1000, epoch: 1 | loss: 0.3125722\n",
      "\tspeed: 0.3060s/iter; left time: 6200.1842s\n",
      "Epoch: 1 cost time: 341.04079818725586\n",
      "Epoch: 1, Steps: 1063 | Train Loss: 0.3533901 Vali Loss: 0.5473794 Test Loss: 0.3818463\n",
      "Validation loss decreased (inf --> 0.547379).  Saving model ...\n",
      "Updating learning rate to 0.0005216375667818456\n",
      "\titers: 100, epoch: 2 | loss: 0.3956089\n",
      "\tspeed: 8.0880s/iter; left time: 162552.4090s\n",
      "\titers: 200, epoch: 2 | loss: 0.3322218\n",
      "\tspeed: 0.3320s/iter; left time: 6639.8166s\n",
      "\titers: 300, epoch: 2 | loss: 0.3537815\n",
      "\tspeed: 0.3330s/iter; left time: 6625.7073s\n",
      "\titers: 400, epoch: 2 | loss: 0.3292276\n",
      "\tspeed: 0.3090s/iter; left time: 6117.2114s\n",
      "\titers: 500, epoch: 2 | loss: 0.4182433\n",
      "\tspeed: 0.3242s/iter; left time: 6386.8781s\n",
      "\titers: 600, epoch: 2 | loss: 0.2700469\n",
      "\tspeed: 0.3228s/iter; left time: 6325.9156s\n",
      "\titers: 700, epoch: 2 | loss: 0.3259510\n",
      "\tspeed: 0.3290s/iter; left time: 6414.4838s\n",
      "\titers: 800, epoch: 2 | loss: 0.2851601\n",
      "\tspeed: 0.3311s/iter; left time: 6422.0341s\n",
      "\titers: 900, epoch: 2 | loss: 0.3077056\n",
      "\tspeed: 0.3120s/iter; left time: 6020.9836s\n",
      "\titers: 1000, epoch: 2 | loss: 0.3193937\n",
      "\tspeed: 0.3280s/iter; left time: 6296.4799s\n",
      "Epoch: 2 cost time: 348.3842968940735\n",
      "Epoch: 2, Steps: 1063 | Train Loss: 0.3303335 Vali Loss: 0.6173386 Test Loss: 0.4345407\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0014003413301562607\n",
      "\titers: 100, epoch: 3 | loss: 0.2880771\n",
      "\tspeed: 8.0152s/iter; left time: 152568.4036s\n",
      "\titers: 200, epoch: 3 | loss: 0.2808566\n",
      "\tspeed: 0.3178s/iter; left time: 6018.1441s\n",
      "\titers: 300, epoch: 3 | loss: 0.2938342\n",
      "\tspeed: 0.3209s/iter; left time: 6044.7515s\n",
      "\titers: 400, epoch: 3 | loss: 0.2509321\n",
      "\tspeed: 0.3381s/iter; left time: 6333.5619s\n",
      "\titers: 500, epoch: 3 | loss: 0.2985090\n",
      "\tspeed: 0.3310s/iter; left time: 6168.7748s\n",
      "\titers: 600, epoch: 3 | loss: 0.3264555\n",
      "\tspeed: 0.3151s/iter; left time: 5839.5496s\n",
      "\titers: 700, epoch: 3 | loss: 0.2881773\n",
      "\tspeed: 0.3199s/iter; left time: 5897.5731s\n",
      "\titers: 800, epoch: 3 | loss: 0.2383036\n",
      "\tspeed: 0.3120s/iter; left time: 5720.4964s\n",
      "\titers: 900, epoch: 3 | loss: 0.3451992\n",
      "\tspeed: 0.3190s/iter; left time: 5817.4431s\n",
      "\titers: 1000, epoch: 3 | loss: 0.3054610\n",
      "\tspeed: 0.3322s/iter; left time: 6023.6193s\n",
      "Epoch: 3 cost time: 342.6160807609558\n",
      "Epoch: 3, Steps: 1063 | Train Loss: 0.2941295 Vali Loss: 0.5204019 Test Loss: 0.4130231\n",
      "Validation loss decreased (0.547379 --> 0.520402).  Saving model ...\n",
      "Updating learning rate to 0.0026005911731450814\n",
      "\titers: 100, epoch: 4 | loss: 0.2986266\n",
      "\tspeed: 8.0308s/iter; left time: 144329.8129s\n",
      "\titers: 200, epoch: 4 | loss: 0.2711958\n",
      "\tspeed: 0.3141s/iter; left time: 5613.7354s\n",
      "\titers: 300, epoch: 4 | loss: 0.3281887\n",
      "\tspeed: 0.2878s/iter; left time: 5115.5604s\n",
      "\titers: 400, epoch: 4 | loss: 0.3336326\n",
      "\tspeed: 0.3071s/iter; left time: 5426.2079s\n",
      "\titers: 500, epoch: 4 | loss: 0.2893320\n",
      "\tspeed: 0.3190s/iter; left time: 5606.2060s\n",
      "\titers: 600, epoch: 4 | loss: 0.2768189\n",
      "\tspeed: 0.3069s/iter; left time: 5362.4824s\n",
      "\titers: 700, epoch: 4 | loss: 0.3181492\n",
      "\tspeed: 0.3210s/iter; left time: 5576.6098s\n",
      "\titers: 800, epoch: 4 | loss: 0.3063841\n",
      "\tspeed: 0.3101s/iter; left time: 5355.6074s\n",
      "\titers: 900, epoch: 4 | loss: 0.3451204\n",
      "\tspeed: 0.3182s/iter; left time: 5463.6474s\n",
      "\titers: 1000, epoch: 4 | loss: 0.3877253\n",
      "\tspeed: 0.2968s/iter; left time: 5066.3203s\n",
      "Epoch: 4 cost time: 331.67146134376526\n",
      "Epoch: 4, Steps: 1063 | Train Loss: 0.2969011 Vali Loss: 0.5351025 Test Loss: 0.4094927\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0038006825632239593\n",
      "\titers: 100, epoch: 5 | loss: 0.4133559\n",
      "\tspeed: 7.2650s/iter; left time: 122843.2295s\n",
      "\titers: 200, epoch: 5 | loss: 0.3632080\n",
      "\tspeed: 0.2881s/iter; left time: 4842.0775s\n",
      "\titers: 300, epoch: 5 | loss: 0.3923839\n",
      "\tspeed: 0.2921s/iter; left time: 4880.1948s\n",
      "\titers: 400, epoch: 5 | loss: 0.3422902\n",
      "\tspeed: 0.2811s/iter; left time: 4668.6344s\n",
      "\titers: 500, epoch: 5 | loss: 0.3055559\n",
      "\tspeed: 0.2818s/iter; left time: 4651.8696s\n",
      "\titers: 600, epoch: 5 | loss: 0.3377950\n",
      "\tspeed: 0.2890s/iter; left time: 4742.9322s\n",
      "\titers: 700, epoch: 5 | loss: 0.3662440\n",
      "\tspeed: 0.2890s/iter; left time: 4713.1860s\n",
      "\titers: 800, epoch: 5 | loss: 0.4403391\n",
      "\tspeed: 0.3080s/iter; left time: 4992.9708s\n",
      "\titers: 900, epoch: 5 | loss: 0.3745748\n",
      "\tspeed: 0.2811s/iter; left time: 4527.8340s\n",
      "\titers: 1000, epoch: 5 | loss: 0.3320703\n",
      "\tspeed: 0.2848s/iter; left time: 4560.0724s\n",
      "Epoch: 5 cost time: 307.5093195438385\n",
      "Epoch: 5, Steps: 1063 | Train Loss: 0.3557300 Vali Loss: 0.5962618 Test Loss: 0.4312054\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.004678953438208884\n",
      "\titers: 100, epoch: 6 | loss: 0.3893048\n",
      "\tspeed: 7.0661s/iter; left time: 111970.1614s\n",
      "\titers: 200, epoch: 6 | loss: 0.3803819\n",
      "\tspeed: 0.3169s/iter; left time: 4989.2148s\n",
      "\titers: 300, epoch: 6 | loss: 0.4217762\n",
      "\tspeed: 0.2590s/iter; left time: 4053.0233s\n",
      "\titers: 400, epoch: 6 | loss: 0.3005695\n",
      "\tspeed: 0.2841s/iter; left time: 4417.1563s\n",
      "\titers: 500, epoch: 6 | loss: 0.3676342\n",
      "\tspeed: 0.2940s/iter; left time: 4541.4356s\n",
      "\titers: 600, epoch: 6 | loss: 0.3540850\n",
      "\tspeed: 0.2720s/iter; left time: 4174.2077s\n",
      "\titers: 700, epoch: 6 | loss: 0.4151505\n",
      "\tspeed: 0.2980s/iter; left time: 4543.1742s\n",
      "\titers: 800, epoch: 6 | loss: 0.3422015\n",
      "\tspeed: 0.3058s/iter; left time: 4631.7671s\n",
      "\titers: 900, epoch: 6 | loss: 0.3691925\n",
      "\tspeed: 0.3031s/iter; left time: 4559.7470s\n",
      "\titers: 1000, epoch: 6 | loss: 0.4325194\n",
      "\tspeed: 0.2961s/iter; left time: 4425.5709s\n",
      "Epoch: 6 cost time: 309.5018663406372\n",
      "Epoch: 6, Steps: 1063 | Train Loss: 0.3533864 Vali Loss: 0.6027290 Test Loss: 0.3748811\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 0.004999999944296124\n",
      "\titers: 100, epoch: 7 | loss: 0.3772670\n",
      "\tspeed: 7.5099s/iter; left time: 111018.8238s\n",
      "\titers: 200, epoch: 7 | loss: 0.3819058\n",
      "\tspeed: 0.3441s/iter; left time: 5052.9965s\n",
      "\titers: 300, epoch: 7 | loss: 0.3669047\n",
      "\tspeed: 0.3089s/iter; left time: 4504.0521s\n",
      "\titers: 400, epoch: 7 | loss: 0.3414760\n",
      "\tspeed: 0.3340s/iter; left time: 4837.9117s\n",
      "\titers: 500, epoch: 7 | loss: 0.3103914\n",
      "\tspeed: 0.3230s/iter; left time: 4645.6767s\n",
      "\titers: 600, epoch: 7 | loss: 0.3844618\n",
      "\tspeed: 0.3159s/iter; left time: 4512.4437s\n",
      "\titers: 700, epoch: 7 | loss: 0.4430332\n",
      "\tspeed: 0.3092s/iter; left time: 4385.4787s\n",
      "\titers: 800, epoch: 7 | loss: 0.3209312\n",
      "\tspeed: 0.3440s/iter; left time: 4844.3290s\n",
      "\titers: 900, epoch: 7 | loss: 0.3161517\n",
      "\tspeed: 0.3219s/iter; left time: 4500.8032s\n",
      "\titers: 1000, epoch: 7 | loss: 0.3486019\n",
      "\tspeed: 0.3270s/iter; left time: 4539.2936s\n",
      "Epoch: 7 cost time: 346.2011637687683\n",
      "Epoch: 7, Steps: 1063 | Train Loss: 0.3658654 Vali Loss: 0.5598319 Test Loss: 0.3911926\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 0.004937202541824174\n",
      "\titers: 100, epoch: 8 | loss: 0.3659373\n",
      "\tspeed: 7.9801s/iter; left time: 109487.3422s\n",
      "\titers: 200, epoch: 8 | loss: 0.3842545\n",
      "\tspeed: 0.3289s/iter; left time: 4479.7055s\n",
      "\titers: 300, epoch: 8 | loss: 0.3249388\n",
      "\tspeed: 0.3150s/iter; left time: 4258.1942s\n",
      "\titers: 400, epoch: 8 | loss: 0.3532070\n",
      "\tspeed: 0.3260s/iter; left time: 4374.8195s\n",
      "\titers: 500, epoch: 8 | loss: 0.4103616\n",
      "\tspeed: 0.3250s/iter; left time: 4329.3457s\n",
      "\titers: 600, epoch: 8 | loss: 0.4370359\n",
      "\tspeed: 0.3150s/iter; left time: 4163.6499s\n",
      "\titers: 700, epoch: 8 | loss: 0.4500147\n",
      "\tspeed: 0.3212s/iter; left time: 4214.3271s\n",
      "\titers: 800, epoch: 8 | loss: 0.3083836\n",
      "\tspeed: 0.3230s/iter; left time: 4205.4904s\n",
      "\titers: 900, epoch: 8 | loss: 0.3973259\n",
      "\tspeed: 0.3209s/iter; left time: 4146.1890s\n",
      "\titers: 1000, epoch: 8 | loss: 0.3390141\n",
      "\tspeed: 0.3329s/iter; left time: 4267.2925s\n",
      "Epoch: 8 cost time: 344.408154964447\n",
      "Epoch: 8, Steps: 1063 | Train Loss: 0.3552502 Vali Loss: 0.6291083 Test Loss: 0.4233477\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 0.004752194128475385\n",
      "\titers: 100, epoch: 9 | loss: 0.3592913\n",
      "\tspeed: 8.0271s/iter; left time: 101599.1969s\n",
      "\titers: 200, epoch: 9 | loss: 0.3084839\n",
      "\tspeed: 0.3219s/iter; left time: 4042.0985s\n",
      "\titers: 300, epoch: 9 | loss: 0.3818018\n",
      "\tspeed: 0.3130s/iter; left time: 3898.6948s\n",
      "\titers: 400, epoch: 9 | loss: 0.3781725\n",
      "\tspeed: 0.3211s/iter; left time: 3967.6884s\n",
      "\titers: 500, epoch: 9 | loss: 0.2865401\n",
      "\tspeed: 0.3260s/iter; left time: 3995.2675s\n",
      "\titers: 600, epoch: 9 | loss: 0.2970202\n",
      "\tspeed: 0.3342s/iter; left time: 4062.7721s\n",
      "\titers: 700, epoch: 9 | loss: 0.3840101\n",
      "\tspeed: 0.3130s/iter; left time: 3773.8087s\n",
      "\titers: 800, epoch: 9 | loss: 0.3150095\n",
      "\tspeed: 0.3020s/iter; left time: 3610.4510s\n",
      "\titers: 900, epoch: 9 | loss: 0.3583897\n",
      "\tspeed: 0.3359s/iter; left time: 3982.7459s\n",
      "\titers: 1000, epoch: 9 | loss: 0.3748881\n",
      "\tspeed: 0.3380s/iter; left time: 3973.4012s\n",
      "Epoch: 9 cost time: 342.59356713294983\n",
      "Epoch: 9, Steps: 1063 | Train Loss: 0.3526771 Vali Loss: 0.5948576 Test Loss: 0.3924020\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 0.004454251798622921\n",
      "\titers: 100, epoch: 10 | loss: 0.3458828\n",
      "\tspeed: 8.0220s/iter; left time: 93006.6388s\n",
      "\titers: 200, epoch: 10 | loss: 0.3159563\n",
      "\tspeed: 0.3270s/iter; left time: 3758.9435s\n",
      "\titers: 300, epoch: 10 | loss: 0.3100654\n",
      "\tspeed: 0.3330s/iter; left time: 3794.7693s\n",
      "\titers: 400, epoch: 10 | loss: 0.4330860\n",
      "\tspeed: 0.3259s/iter; left time: 3681.0869s\n",
      "\titers: 500, epoch: 10 | loss: 0.3696443\n",
      "\tspeed: 0.3220s/iter; left time: 3604.2001s\n",
      "\titers: 600, epoch: 10 | loss: 0.3480978\n",
      "\tspeed: 0.3470s/iter; left time: 3850.0675s\n",
      "\titers: 700, epoch: 10 | loss: 0.3990502\n",
      "\tspeed: 0.3210s/iter; left time: 3529.2646s\n",
      "\titers: 800, epoch: 10 | loss: 0.3344728\n",
      "\tspeed: 0.3350s/iter; left time: 3649.4971s\n",
      "\titers: 900, epoch: 10 | loss: 0.2976494\n",
      "\tspeed: 0.3209s/iter; left time: 3464.2586s\n",
      "\titers: 1000, epoch: 10 | loss: 0.3352448\n",
      "\tspeed: 0.3471s/iter; left time: 3711.6985s\n",
      "Epoch: 10 cost time: 352.51186990737915\n",
      "Epoch: 10, Steps: 1063 | Train Loss: 0.3462491 Vali Loss: 0.5630178 Test Loss: 0.3891786\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 0.0040583156247844055\n",
      "\titers: 100, epoch: 11 | loss: 0.3876426\n",
      "\tspeed: 8.0270s/iter; left time: 84532.2653s\n",
      "\titers: 200, epoch: 11 | loss: 0.3667076\n",
      "\tspeed: 0.3160s/iter; left time: 3295.9925s\n",
      "\titers: 300, epoch: 11 | loss: 0.2936554\n",
      "\tspeed: 0.3430s/iter; left time: 3543.4711s\n",
      "\titers: 400, epoch: 11 | loss: 0.3267487\n",
      "\tspeed: 0.3230s/iter; left time: 3304.6149s\n",
      "\titers: 500, epoch: 11 | loss: 0.3389584\n",
      "\tspeed: 0.3180s/iter; left time: 3221.8066s\n",
      "\titers: 600, epoch: 11 | loss: 0.3800408\n",
      "\tspeed: 0.3229s/iter; left time: 3239.3524s\n",
      "\titers: 700, epoch: 11 | loss: 0.3251600\n",
      "\tspeed: 0.3030s/iter; left time: 3009.2596s\n",
      "\titers: 800, epoch: 11 | loss: 0.3198416\n",
      "\tspeed: 0.3132s/iter; left time: 3079.1638s\n",
      "\titers: 900, epoch: 11 | loss: 0.3099320\n",
      "\tspeed: 0.3410s/iter; left time: 3318.2949s\n",
      "\titers: 1000, epoch: 11 | loss: 0.3465792\n",
      "\tspeed: 0.3468s/iter; left time: 3340.0174s\n",
      "Epoch: 11 cost time: 346.5112462043762\n",
      "Epoch: 11, Steps: 1063 | Train Loss: 0.3406800 Vali Loss: 0.5580065 Test Loss: 0.3872813\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 0.0035842395000015848\n",
      "\titers: 100, epoch: 12 | loss: 0.3868378\n",
      "\tspeed: 8.0012s/iter; left time: 75755.2030s\n",
      "\titers: 200, epoch: 12 | loss: 0.3051461\n",
      "\tspeed: 0.3138s/iter; left time: 2939.5791s\n",
      "\titers: 300, epoch: 12 | loss: 0.2929367\n",
      "\tspeed: 0.3061s/iter; left time: 2836.8618s\n",
      "\titers: 400, epoch: 12 | loss: 0.2731199\n",
      "\tspeed: 0.3520s/iter; left time: 3226.8147s\n",
      "\titers: 500, epoch: 12 | loss: 0.3564174\n",
      "\tspeed: 0.3200s/iter; left time: 2901.6337s\n",
      "\titers: 600, epoch: 12 | loss: 0.2916277\n",
      "\tspeed: 0.3310s/iter; left time: 2968.4616s\n",
      "\titers: 700, epoch: 12 | loss: 0.3597770\n",
      "\tspeed: 0.3302s/iter; left time: 2927.9002s\n",
      "\titers: 800, epoch: 12 | loss: 0.2984121\n",
      "\tspeed: 0.3568s/iter; left time: 3128.7724s\n",
      "\titers: 900, epoch: 12 | loss: 0.2871146\n",
      "\tspeed: 0.3180s/iter; left time: 2756.7138s\n",
      "\titers: 1000, epoch: 12 | loss: 0.3005918\n",
      "\tspeed: 0.3310s/iter; left time: 2835.7929s\n",
      "Epoch: 12 cost time: 348.7888693809509\n",
      "Epoch: 12, Steps: 1063 | Train Loss: 0.3342898 Vali Loss: 0.5942886 Test Loss: 0.3690114\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 0.0030557955807405703\n",
      "\titers: 100, epoch: 13 | loss: 0.3278311\n",
      "\tspeed: 7.9319s/iter; left time: 66668.0290s\n",
      "\titers: 200, epoch: 13 | loss: 0.3250177\n",
      "\tspeed: 0.3330s/iter; left time: 2765.3943s\n",
      "\titers: 300, epoch: 13 | loss: 0.3871066\n",
      "\tspeed: 0.3070s/iter; left time: 2518.9719s\n",
      "\titers: 400, epoch: 13 | loss: 0.3094060\n",
      "\tspeed: 0.3341s/iter; left time: 2708.0865s\n",
      "\titers: 500, epoch: 13 | loss: 0.3044350\n",
      "\tspeed: 0.3390s/iter; left time: 2713.9796s\n",
      "\titers: 600, epoch: 13 | loss: 0.2854370\n",
      "\tspeed: 0.3079s/iter; left time: 2434.1246s\n",
      "\titers: 700, epoch: 13 | loss: 0.3330233\n",
      "\tspeed: 0.3070s/iter; left time: 2395.8365s\n",
      "\titers: 800, epoch: 13 | loss: 0.3746689\n",
      "\tspeed: 0.3422s/iter; left time: 2636.4824s\n",
      "\titers: 900, epoch: 13 | loss: 0.3227815\n",
      "\tspeed: 0.3138s/iter; left time: 2386.3671s\n",
      "\titers: 1000, epoch: 13 | loss: 0.4165869\n",
      "\tspeed: 0.3102s/iter; left time: 2328.1842s\n",
      "Epoch: 13 cost time: 341.50210762023926\n",
      "Epoch: 13, Steps: 1063 | Train Loss: 0.3253607 Vali Loss: 0.5963860 Test Loss: 0.3726271\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): PatchTST_backbone(\n",
       "    (backbone): TSTiEncoder(\n",
       "      (W_P): Linear(in_features=24, out_features=128, bias=True)\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (encoder): TSTEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-2): 3 x TSTEncoderLayer(\n",
       "            (self_attn): _MultiheadAttention(\n",
       "              (W_Q): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (W_K): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (W_V): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (sdp_attn): _ScaledDotProductAttention(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (1): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (dropout_attn): Dropout(p=0.2, inplace=False)\n",
       "            (norm_attn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.2, inplace=False)\n",
       "              (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "            )\n",
       "            (dropout_ffn): Dropout(p=0.2, inplace=False)\n",
       "            (norm_ffn): Sequential(\n",
       "              (0): Transpose()\n",
       "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): Transpose()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Flatten_Head(\n",
       "      (flatten): Flatten(start_dim=-2, end_dim=-1)\n",
       "      (linear): Linear(in_features=5120, out_features=192, bias=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ffecc9",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc831e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 11329\n",
      "mse:0.4130229949951172, mae:0.438975065946579, rse:0.6117640733718872\n"
     ]
    }
   ],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5067b97",
   "metadata": {},
   "source": [
    "---\n",
    "## Trail 4: PatchTST/64, Dataset:ETTm1,  Metric: 192\n",
    "\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6dedcc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ETTm1, Prediction Length: 192, Number of Patches: 64\n"
     ]
    }
   ],
   "source": [
    "args.pred_len = 192 # prediction sequence length\n",
    "args.num_patch = 64  # The number of input patches\n",
    "print(f\"Dataset: {args.data}, Prediction Length: {args.pred_len}, Number of Patches: {args.num_patch}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38148493",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b883fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "train 34033\n",
      "val 11329\n",
      "test 11329\n",
      "\titers: 100, epoch: 1 | loss: 0.4179065\n",
      "\tspeed: 0.3510s/iter; left time: 7427.1056s\n",
      "\titers: 200, epoch: 1 | loss: 0.4193902\n",
      "\tspeed: 0.3421s/iter; left time: 7204.8105s\n",
      "\titers: 300, epoch: 1 | loss: 0.3345259\n",
      "\tspeed: 0.3180s/iter; left time: 6665.0604s\n",
      "\titers: 400, epoch: 1 | loss: 0.4020755\n",
      "\tspeed: 0.3139s/iter; left time: 6548.3559s\n",
      "\titers: 500, epoch: 1 | loss: 0.3505193\n",
      "\tspeed: 0.3491s/iter; left time: 7247.9694s\n",
      "\titers: 600, epoch: 1 | loss: 0.2714881\n",
      "\tspeed: 0.3179s/iter; left time: 6568.8266s\n",
      "\titers: 700, epoch: 1 | loss: 0.3446555\n",
      "\tspeed: 0.3112s/iter; left time: 6397.9991s\n",
      "\titers: 800, epoch: 1 | loss: 0.3572136\n",
      "\tspeed: 0.3208s/iter; left time: 6563.3261s\n",
      "\titers: 900, epoch: 1 | loss: 0.3371011\n",
      "\tspeed: 0.3340s/iter; left time: 6801.1653s\n",
      "\titers: 1000, epoch: 1 | loss: 0.3129145\n",
      "\tspeed: 0.3190s/iter; left time: 6463.6882s\n",
      "Epoch: 1 cost time: 349.4956429004669\n",
      "Epoch: 1, Steps: 1063 | Train Loss: 0.3566981 Vali Loss: 0.5289810 Test Loss: 0.3740084\n",
      "Validation loss decreased (inf --> 0.528981).  Saving model ...\n",
      "Updating learning rate to 0.0005216375667818456\n",
      "\titers: 100, epoch: 2 | loss: 0.3403362\n",
      "\tspeed: 8.0700s/iter; left time: 162191.3787s\n",
      "\titers: 200, epoch: 2 | loss: 0.4522078\n",
      "\tspeed: 0.3301s/iter; left time: 6601.5468s\n",
      "\titers: 300, epoch: 2 | loss: 0.3114782\n",
      "\tspeed: 0.3309s/iter; left time: 6584.9994s\n",
      "\titers: 400, epoch: 2 | loss: 0.3614300\n",
      "\tspeed: 0.3369s/iter; left time: 6669.6616s\n",
      "\titers: 500, epoch: 2 | loss: 0.3286117\n",
      "\tspeed: 0.3280s/iter; left time: 6460.6836s\n",
      "\titers: 600, epoch: 2 | loss: 0.2806916\n",
      "\tspeed: 0.3181s/iter; left time: 6234.8462s\n",
      "\titers: 700, epoch: 2 | loss: 0.3451965\n",
      "\tspeed: 0.3199s/iter; left time: 6237.8445s\n",
      "\titers: 800, epoch: 2 | loss: 0.3511057\n",
      "\tspeed: 0.3390s/iter; left time: 6576.8038s\n",
      "\titers: 900, epoch: 2 | loss: 0.4574474\n",
      "\tspeed: 0.3369s/iter; left time: 6502.2654s\n",
      "\titers: 1000, epoch: 2 | loss: 0.3109913\n",
      "\tspeed: 0.3670s/iter; left time: 7045.7543s\n",
      "Epoch: 2 cost time: 352.6714689731598\n",
      "Epoch: 2, Steps: 1063 | Train Loss: 0.3295101 Vali Loss: 0.5179107 Test Loss: 0.3992817\n",
      "Validation loss decreased (0.528981 --> 0.517911).  Saving model ...\n",
      "Updating learning rate to 0.0014003413301562607\n",
      "\titers: 100, epoch: 3 | loss: 0.3044737\n",
      "\tspeed: 7.9960s/iter; left time: 152203.2407s\n",
      "\titers: 200, epoch: 3 | loss: 0.2780176\n",
      "\tspeed: 0.3231s/iter; left time: 6117.6872s\n",
      "\titers: 300, epoch: 3 | loss: 0.3179520\n",
      "\tspeed: 0.3350s/iter; left time: 6308.9016s\n",
      "\titers: 400, epoch: 3 | loss: 0.2892277\n",
      "\tspeed: 0.3140s/iter; left time: 5882.0791s\n",
      "\titers: 500, epoch: 3 | loss: 0.3174324\n",
      "\tspeed: 0.3020s/iter; left time: 5628.2238s\n",
      "\titers: 600, epoch: 3 | loss: 0.2538651\n",
      "\tspeed: 0.3141s/iter; left time: 5821.2136s\n",
      "\titers: 700, epoch: 3 | loss: 0.3192883\n",
      "\tspeed: 0.2960s/iter; left time: 5456.6083s\n",
      "\titers: 800, epoch: 3 | loss: 0.3151757\n",
      "\tspeed: 0.2949s/iter; left time: 5407.7509s\n",
      "\titers: 900, epoch: 3 | loss: 0.3086067\n",
      "\tspeed: 0.2641s/iter; left time: 4814.9969s\n",
      "\titers: 1000, epoch: 3 | loss: 0.2673055\n",
      "\tspeed: 0.3091s/iter; left time: 5605.5596s\n",
      "Epoch: 3 cost time: 327.7994260787964\n",
      "Epoch: 3, Steps: 1063 | Train Loss: 0.2946761 Vali Loss: 0.5508270 Test Loss: 0.3796095\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0026005911731450814\n",
      "\titers: 100, epoch: 4 | loss: 0.3191010\n",
      "\tspeed: 6.3248s/iter; left time: 113669.3994s\n",
      "\titers: 200, epoch: 4 | loss: 0.2569815\n",
      "\tspeed: 0.3220s/iter; left time: 5755.6422s\n",
      "\titers: 300, epoch: 4 | loss: 0.2625699\n",
      "\tspeed: 0.3271s/iter; left time: 5813.0688s\n",
      "\titers: 400, epoch: 4 | loss: 0.3403909\n",
      "\tspeed: 0.3089s/iter; left time: 5459.3844s\n",
      "\titers: 500, epoch: 4 | loss: 0.2895393\n",
      "\tspeed: 0.3250s/iter; left time: 5710.9705s\n",
      "\titers: 600, epoch: 4 | loss: 0.3146322\n",
      "\tspeed: 0.3029s/iter; left time: 5292.9978s\n",
      "\titers: 700, epoch: 4 | loss: 0.2680637\n",
      "\tspeed: 0.3200s/iter; left time: 5559.7382s\n",
      "\titers: 800, epoch: 4 | loss: 0.3510581\n",
      "\tspeed: 0.3040s/iter; left time: 5250.5821s\n",
      "\titers: 900, epoch: 4 | loss: 0.3013207\n",
      "\tspeed: 0.3050s/iter; left time: 5236.6897s\n",
      "\titers: 1000, epoch: 4 | loss: 0.3216736\n",
      "\tspeed: 0.3230s/iter; left time: 5515.0858s\n",
      "Epoch: 4 cost time: 336.5975239276886\n",
      "Epoch: 4, Steps: 1063 | Train Loss: 0.3000206 Vali Loss: 0.6062011 Test Loss: 0.4337018\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.0038006825632239593\n",
      "\titers: 100, epoch: 5 | loss: 0.4142238\n",
      "\tspeed: 7.6261s/iter; left time: 128948.9556s\n",
      "\titers: 200, epoch: 5 | loss: 0.3572605\n",
      "\tspeed: 0.3150s/iter; left time: 5294.6466s\n",
      "\titers: 300, epoch: 5 | loss: 0.3693417\n",
      "\tspeed: 0.3159s/iter; left time: 5278.3932s\n",
      "\titers: 400, epoch: 5 | loss: 0.4028408\n",
      "\tspeed: 0.3030s/iter; left time: 5032.5028s\n",
      "\titers: 500, epoch: 5 | loss: 0.3096469\n",
      "\tspeed: 0.3171s/iter; left time: 5234.3675s\n",
      "\titers: 600, epoch: 5 | loss: 0.3963543\n",
      "\tspeed: 0.3280s/iter; left time: 5382.4495s\n",
      "\titers: 700, epoch: 5 | loss: 0.3202924\n",
      "\tspeed: 0.3010s/iter; left time: 4908.8041s\n",
      "\titers: 800, epoch: 5 | loss: 0.4202910\n",
      "\tspeed: 0.3029s/iter; left time: 4910.2004s\n",
      "\titers: 900, epoch: 5 | loss: 0.3898428\n",
      "\tspeed: 0.3370s/iter; left time: 5429.4591s\n",
      "\titers: 1000, epoch: 5 | loss: 0.3843437\n",
      "\tspeed: 0.3080s/iter; left time: 4930.5765s\n",
      "Epoch: 5 cost time: 335.70220160484314\n",
      "Epoch: 5, Steps: 1063 | Train Loss: 0.3568867 Vali Loss: 0.6171964 Test Loss: 0.4179490\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 0.004678953438208884\n",
      "\titers: 100, epoch: 6 | loss: 0.3266828\n",
      "\tspeed: 7.6380s/iter; left time: 121031.1940s\n",
      "\titers: 200, epoch: 6 | loss: 0.2896382\n",
      "\tspeed: 0.2981s/iter; left time: 4693.1858s\n",
      "\titers: 300, epoch: 6 | loss: 0.3338185\n",
      "\tspeed: 0.2840s/iter; left time: 4443.8207s\n",
      "\titers: 400, epoch: 6 | loss: 0.2993533\n",
      "\tspeed: 0.3209s/iter; left time: 4989.0198s\n",
      "\titers: 500, epoch: 6 | loss: 0.3706346\n",
      "\tspeed: 0.3060s/iter; left time: 4726.5930s\n",
      "\titers: 600, epoch: 6 | loss: 0.2874359\n",
      "\tspeed: 0.2970s/iter; left time: 4558.2941s\n",
      "\titers: 700, epoch: 6 | loss: 0.3453195\n",
      "\tspeed: 0.2960s/iter; left time: 4513.4091s\n",
      "\titers: 800, epoch: 6 | loss: 0.2897613\n",
      "\tspeed: 0.3320s/iter; left time: 5027.8706s\n",
      "\titers: 900, epoch: 6 | loss: 0.3341952\n",
      "\tspeed: 0.2950s/iter; left time: 4438.9579s\n",
      "\titers: 1000, epoch: 6 | loss: 0.3185751\n",
      "\tspeed: 0.3150s/iter; left time: 4707.6500s\n",
      "Epoch: 6 cost time: 326.6080148220062\n",
      "Epoch: 6, Steps: 1063 | Train Loss: 0.3508531 Vali Loss: 0.5911328 Test Loss: 0.3932810\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 0.004999999944296124\n",
      "\titers: 100, epoch: 7 | loss: 0.3174972\n",
      "\tspeed: 7.6690s/iter; left time: 113370.2429s\n",
      "\titers: 200, epoch: 7 | loss: 0.3885055\n",
      "\tspeed: 0.2870s/iter; left time: 4214.1610s\n",
      "\titers: 300, epoch: 7 | loss: 0.3375808\n",
      "\tspeed: 0.3340s/iter; left time: 4871.4224s\n",
      "\titers: 400, epoch: 7 | loss: 0.3692495\n",
      "\tspeed: 0.3101s/iter; left time: 4490.5895s\n",
      "\titers: 500, epoch: 7 | loss: 0.3512037\n",
      "\tspeed: 0.3489s/iter; left time: 5018.6751s\n",
      "\titers: 600, epoch: 7 | loss: 0.3083505\n",
      "\tspeed: 0.3009s/iter; left time: 4298.4535s\n",
      "\titers: 700, epoch: 7 | loss: 0.3070047\n",
      "\tspeed: 0.3100s/iter; left time: 4396.7488s\n",
      "\titers: 800, epoch: 7 | loss: 0.3322286\n",
      "\tspeed: 0.3210s/iter; left time: 4520.7444s\n",
      "\titers: 900, epoch: 7 | loss: 0.3713633\n",
      "\tspeed: 0.3270s/iter; left time: 4572.9382s\n",
      "\titers: 1000, epoch: 7 | loss: 0.4062921\n",
      "\tspeed: 0.2860s/iter; left time: 3969.8717s\n",
      "Epoch: 7 cost time: 333.1028242111206\n",
      "Epoch: 7, Steps: 1063 | Train Loss: 0.3548042 Vali Loss: 0.6564420 Test Loss: 0.4453136\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 0.004937202541824174\n",
      "\titers: 100, epoch: 8 | loss: 0.3757055\n",
      "\tspeed: 7.7040s/iter; left time: 105699.5120s\n",
      "\titers: 200, epoch: 8 | loss: 0.3422600\n",
      "\tspeed: 0.3080s/iter; left time: 4195.2358s\n",
      "\titers: 300, epoch: 8 | loss: 0.4095349\n",
      "\tspeed: 0.3140s/iter; left time: 4245.1936s\n",
      "\titers: 400, epoch: 8 | loss: 0.3694459\n",
      "\tspeed: 0.3389s/iter; left time: 4548.6519s\n",
      "\titers: 500, epoch: 8 | loss: 0.3940437\n",
      "\tspeed: 0.3110s/iter; left time: 4142.4478s\n",
      "\titers: 600, epoch: 8 | loss: 0.3447484\n",
      "\tspeed: 0.3280s/iter; left time: 4336.7187s\n",
      "\titers: 700, epoch: 8 | loss: 0.2708187\n",
      "\tspeed: 0.3070s/iter; left time: 4027.7606s\n",
      "\titers: 800, epoch: 8 | loss: 0.3178230\n",
      "\tspeed: 0.3380s/iter; left time: 4401.0248s\n",
      "\titers: 900, epoch: 8 | loss: 0.3159982\n",
      "\tspeed: 0.3081s/iter; left time: 3981.0349s\n",
      "\titers: 1000, epoch: 8 | loss: 0.3785062\n",
      "\tspeed: 0.3428s/iter; left time: 4394.9205s\n",
      "Epoch: 8 cost time: 340.70095467567444\n",
      "Epoch: 8, Steps: 1063 | Train Loss: 0.3550600 Vali Loss: 0.6760303 Test Loss: 0.4133295\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 0.004752194128475385\n",
      "\titers: 100, epoch: 9 | loss: 0.3553716\n",
      "\tspeed: 7.7470s/iter; left time: 98054.2323s\n",
      "\titers: 200, epoch: 9 | loss: 0.3073311\n",
      "\tspeed: 0.3200s/iter; left time: 4017.8180s\n",
      "\titers: 300, epoch: 9 | loss: 0.2994146\n",
      "\tspeed: 0.3241s/iter; left time: 4036.8189s\n",
      "\titers: 400, epoch: 9 | loss: 0.4146659\n",
      "\tspeed: 0.3119s/iter; left time: 3854.6626s\n",
      "\titers: 500, epoch: 9 | loss: 0.3678674\n",
      "\tspeed: 0.3160s/iter; left time: 3873.3164s\n",
      "\titers: 600, epoch: 9 | loss: 0.3218613\n",
      "\tspeed: 0.3150s/iter; left time: 3829.2333s\n",
      "\titers: 700, epoch: 9 | loss: 0.2825035\n",
      "\tspeed: 0.3122s/iter; left time: 3763.6881s\n",
      "\titers: 800, epoch: 9 | loss: 0.3307456\n",
      "\tspeed: 0.3119s/iter; left time: 3729.6894s\n"
     ]
    }
   ],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea133f8",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872416d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8153cd38",
   "metadata": {},
   "source": [
    "---\n",
    "## Trail 5: PatchTST/42, Dataset:ETTm1,  Metric: 336\n",
    "\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ea7b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.pred_len = 336 # prediction sequence length\n",
    "args.num_patch = 42  # The number of input patches\n",
    "print(f\"Dataset: {args.data}, Prediction Length: {args.pred_len}, Number of Patches: {args.num_patch}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6083c130",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea3dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1970a3b7",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a86d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1eafbf",
   "metadata": {},
   "source": [
    "---\n",
    "## Trail 6: PatchTST/64, Dataset:ETTm1,  Metric: 336\n",
    "\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d6142e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.pred_len = 336 # prediction sequence length\n",
    "args.num_patch = 64  # The number of input patches\n",
    "print(f\"Dataset: {args.data}, Prediction Length: {args.pred_len}, Number of Patches: {args.num_patch}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ff9af",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1ba313",
   "metadata": {},
   "outputs": [],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35b5869",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5407ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1030e00a",
   "metadata": {},
   "source": [
    "---\n",
    "## Trail 7: PatchTST/42, Dataset:ETTm1,  Metric: 720\n",
    "\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7715c800",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.pred_len = 720 # prediction sequence length\n",
    "args.num_patch = 42  # The number of input patches\n",
    "print(f\"Dataset: {args.data}, Prediction Length: {args.pred_len}, Number of Patches: {args.num_patch}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bbfcda",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ee5d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cdd1e6",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04c70f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a192e040",
   "metadata": {},
   "source": [
    "---\n",
    "## Trail 8: PatchTST/64, Dataset:ETTm1,  Metric: 720\n",
    "\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef6af03",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.pred_len = 720 # prediction sequence length\n",
    "args.num_patch = 64  # The number of input patches\n",
    "print(f\"Dataset: {args.data}, Prediction Length: {args.pred_len}, Number of Patches: {args.num_patch}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a83ba03",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7183a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efd5e85",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd650b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b3412c",
   "metadata": {},
   "source": [
    "### Compare our results with paper results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cf4ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=r\"./Images/ETT1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d4b245",
   "metadata": {},
   "source": [
    "#### **Experiment Results**:\n",
    "Comaparing my results with the paper resulted highlited in the image above.\n",
    "\n",
    "| PatchTST/42 | Dataset | Seq_len | MSE | MAE |\n",
    "|---|---|---|---|---|\n",
    "|  | ETTm1 | 96 |  0.37019482254981995| 0.3915349543094635  |\n",
    "|  | ETTm1 | 192 |  0.40499380230903625 | 0.4138428270816803 |\n",
    "|  | ETTm1 | 336 | 0.4337225556373596 | 0.434622198343277  |\n",
    "|  | ETTm1 | 720 | 0.470274418592453| 0.4867483675479889 |\n",
    "\n",
    "\n",
    "| PatchTST/64 | Dataset | Seq_len | MSE | MAE |\n",
    "|---|---|---|---|---|\n",
    "|  | ETTm1 | 96 |  0.37019482254981995| 0.3915349543094635  |\n",
    "|  | ETTm1 | 192 |  0.40499380230903625 | 0.4138428270816803 |\n",
    "|  | ETTm1 | 336 | 0.4337225556373596 | 0.434622198343277  |\n",
    "|  | ETTm1 | 720 | 0.470274418592453| 0.4867483675479889 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117b68ac",
   "metadata": {},
   "source": [
    "---\n",
    "# Working on ETTm2 Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d3218a",
   "metadata": {},
   "source": [
    "## Trail 1: PatchTST/42, Dataset:ETTm2,  Metric: 96\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bbab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.data_path = 'ETTm2.csv' # data file\n",
    "args.data = 'ETTm2'  # data\n",
    "args.pred_len = 96 # prediction sequence length\n",
    "args.num_patch = 42  # The number of input patches\n",
    "print(f\"Dataset: {args.data}, Prediction Length: {args.pred_len}, Number of Patches: {args.num_patch}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef40de5",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392b2b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cd730b",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62669203",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffbcd38",
   "metadata": {},
   "source": [
    "---\n",
    "## Trail 2: PatchTST/64, Dataset:ETTm2,  Metric: 96\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fec8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.pred_len = 96 # prediction sequence length\n",
    "args.num_patch = 64  # The number of input patches\n",
    "print(f\"Dataset: {args.data}, Prediction Length: {args.pred_len}, Number of Patches: {args.num_patch}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320d6200",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78cceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eb2b00",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c499f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7688af4",
   "metadata": {},
   "source": [
    "---\n",
    "## Trail 3: PatchTST/42, Dataset:ETTm2,  Metric: 192\n",
    "\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398cbf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.pred_len = 192 # prediction sequence length\n",
    "args.num_patch = 42  # The number of input patches\n",
    "print(f\"Dataset: {args.data}, Prediction Length: {args.pred_len}, Number of Patches: {args.num_patch}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ae76f4",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0201f523",
   "metadata": {},
   "outputs": [],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154dbae4",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff72633",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf199b38",
   "metadata": {},
   "source": [
    "---\n",
    "## Trail 4: PatchTST/64, Dataset:ETTm2,  Metric: 192\n",
    "\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c1c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.pred_len = 192 # prediction sequence length\n",
    "args.num_patch = 64  # The number of input patches\n",
    "print(f\"Dataset: {args.data}, Prediction Length: {args.pred_len}, Number of Patches: {args.num_patch}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fda646",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d16c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0df22d",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfead5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92b5630",
   "metadata": {},
   "source": [
    "---\n",
    "## Trail 5: PatchTST/42, Dataset:ETTm2,  Metric: 336\n",
    "\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3718a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.pred_len = 336 # prediction sequence length\n",
    "args.num_patch = 42  # The number of input patches\n",
    "print(f\"Dataset: {args.data}, Prediction Length: {args.pred_len}, Number of Patches: {args.num_patch}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1c4eda",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaa48bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fce1515",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49e1152",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c3a5fb",
   "metadata": {},
   "source": [
    "---\n",
    "## Trail 6: PatchTST/64, Dataset:ETTm2,  Metric: 336\n",
    "\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13adcae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.pred_len = 336 # prediction sequence length\n",
    "args.num_patch = 64  # The number of input patches\n",
    "print(f\"Dataset: {args.data}, Prediction Length: {args.pred_len}, Number of Patches: {args.num_patch}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff89934d",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0364def7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85333a3",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8717d4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6779394",
   "metadata": {},
   "source": [
    "---\n",
    "## Trail 7: PatchTST/42, Dataset:ETTm2,  Metric: 720\n",
    "\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4346cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.pred_len = 720 # prediction sequence length\n",
    "args.num_patch = 42  # The number of input patches\n",
    "print(f\"Dataset: {args.data}, Prediction Length: {args.pred_len}, Number of Patches: {args.num_patch}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ffaa5c",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af79a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80669a4b",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6228af",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dc69be",
   "metadata": {},
   "source": [
    "---\n",
    "## Trail 8: PatchTST/64, Dataset:ETTm2,  Metric: 720\n",
    "\n",
    "### Set hyperparameters\n",
    "Set some parameters (Args) for the our experiment like dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763e0327",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.pred_len = 720 # prediction sequence length\n",
    "args.num_patch = 42  # The number of input patches\n",
    "print(f\"Dataset: {args.data}, Prediction Length: {args.pred_len}, Number of Patches: {args.num_patch}\") \n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e9914d",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce06a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "Exp = Exp_Main\n",
    "setting=f'PatchTST_train_on_{args.data}_{args.pred_len}_{args.num_patch}'\n",
    "# set experiments\n",
    "exp = Exp(args)\n",
    "exp.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cae994",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e0c3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.test(setting)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7988edcb",
   "metadata": {},
   "source": [
    "### Compare our results with paper results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8056a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=r\"./Images/ETT1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d6d317",
   "metadata": {},
   "source": [
    "#### **Experiment Results**:\n",
    "Comaparing my results with the paper resulted highlited in the image above.\n",
    "\n",
    "| PatchTST/42 | Dataset | Seq_len | MSE | MAE |\n",
    "|---|---|---|---|---|\n",
    "|  | ETTm2 | 96 |  0.37019482254981995| 0.3915349543094635  |\n",
    "|  | ETTm2 | 192 |  0.40499380230903625 | 0.4138428270816803 |\n",
    "|  | ETTm2 | 336 | 0.4337225556373596 | 0.434622198343277  |\n",
    "|  | ETTm2 | 720 | 0.470274418592453| 0.4867483675479889 |\n",
    "\n",
    "\n",
    "| PatchTST/64 | Dataset | Seq_len | MSE | MAE |\n",
    "|---|---|---|---|---|\n",
    "|  | ETTm2 | 96 |  0.37019482254981995| 0.3915349543094635  |\n",
    "|  | ETTm2 | 192 |  0.40499380230903625 | 0.4138428270816803 |\n",
    "|  | ETTm2 | 336 | 0.4337225556373596 | 0.434622198343277  |\n",
    "|  | ETTm2 | 720 | 0.470274418592453| 0.4867483675479889 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6c6226",
   "metadata": {},
   "source": [
    "---\n",
    "### Conclusion\n",
    "---\n",
    "\n",
    "The training process is progressing well and the `PatchTST` model is being optimized in an effective manner. \n",
    "- The loss is steadily decreasing over the epochs, which indicates the model is learning and improving. \n",
    "- The validation loss is also decreasing at each epoch, showing the model is generalizing well.\n",
    "\n",
    "**The key positive signs I see are:**\n",
    "\n",
    "- Decreasing loss and validation loss\n",
    "- Learning rate decay schedule\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7d535e",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
